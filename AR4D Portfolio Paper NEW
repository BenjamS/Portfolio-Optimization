---
title: "Risk-adjusted optimization of agricultural research for development portfolios"
author:
  - name: Benjamin Schiek
    email: b.schiek@cgiar.org
    affiliation: CIAT
address:
  - code: CIAT
    address: Decision and Policy Analysis Unit, Alliance Bioversity-CIAT, Km 17 Recta Cali-Palmira, Valle del Cauca, Colombia
abstract: |
 Conventional agricultural research for development (AR4D) priority setting exercises generate valuable insight into the strengths and weaknesses of individual agricultural research proposals, but they stop short of providing tools that can translate this insight into optimal resource allocation shares across a portfolio containing several such proposals. They also lack tools for risk accounting. Here I explore the possibility of redressing both methodological lacunae in one stroke by adapting Mean-Variance (MV) Analysis, a risk-adjusted portfolio optimization technique developed in financial contexts, to the AR4D portfolio optimization problem. Along the way, I present new approaches to two old problems in MV Analysis: negative budget shares and noise-induced inaccuracy. These innovations compare favorably to the baseline Mean-Variance model in a backtest using financial data. I then apply the modified MV model to the AR4D context using FAO farmgate price data for Sub-Saharan Africa.
keywords: Risk-adjusted portfolio optmization, Mean-Variance Analysis, AR4D, ex-ante impact assessment, foresight
journal: Alliance Bioversity-CIAT Working Paper
bibliography: AR4D Portfolio Optimization.bib
header-includes:
  - \usepackage{float}
  - \floatplacement{figure}{H}
#linenumbers: true
numbersections: true
output:
  rticles::elsevier_article:
    latex_engine: xelatex
mainfont: Garamond
---
<!-- https://tug.org/FontCatalogue/ -->
<!-- Source Serif Pro Light -->
<!-- rmarkdown::draft("Test.Rmd", template = "elsevier_article", package = "rticles") -->
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE,
                      message = FALSE,
                      warning = FALSE)
#devtools::install_github("thomasp85/patchwork")
library(plyr)
library(tidyverse)
library(zoo)
library(tidyquant)
library(patchwork)
```

# Introduction

## Mills' missing fifth step

Long ago, Bradford Mills articulated the four steps of priority setting in agricultural research for development (AR4D) as follows [-@Mills1998]:

>Step one is the development of an information base. Step two is the establishment of research alternatives. Step three is the evaluation of the potential impact of research alternatives. Step four, often considered the final output of a priority-setting exercise, is the ranking of alternatives into research priorities.

However, he went on to observe that these steps were insufficient. An as of yet non-existent

>fifth step, the development of guidelines for translating priorities into planning and resource allocation decisions, is necessary to establish direct links with planning and resource allocation activities [@Mills1998].

Mills identified this lacuna at a time when AR4D centers were just beginning to come under pressure from public and private donors to "do a lot more with a lot less" [@Alston1995]. Methodological work in response to this pressure focused on Steps 1-4. The emergence and refinement of numerous ex-ante impact assessment models for the evaluation of individual research alternatives, in particular, is remarkable (see, for example, @Alston1995; @Antle1999; @Antle2015; Mills, 1998; and @nelson2014modeling). However, work has still not begun on Mills' missing fifth step.

![Mill's missing fifth step in the resource allocation workflow. Adapted from Mills [-@Mills1998]](Mills_missing_step5.png)

In hindsight, Mills' concern was well founded. Budgetary pressure on AR4D centers to "prove their relevance" [@Braunschweig2000], "show value for money" [@yet2016bayesian], and otherwise demonstrate "more efficient spending of resources" [@petsakos2018comparing] has only increased since the 1990s. Between Step 4 and the final allocation of funds, stakeholder politics, institutional inertia, ad hoc procedures and criteria, and other subjective forces have repeatedly undercut any careful, objective rigor occurring in Steps 1-4 [@Birner2016; @mccalla2014cgiar]. This  inconsistency, opacity, and subjectivity in resource allocation decisions, has, in turn, aggravated scientists' already longstanding distrust of budget allocation mechanisms. Relations between donors and the research programs they fund have, as a result, reached an historic level of toxicity [@Birner2016; @leeuwis2018reforming; @mccalla2014cgiar].

A rigorous tool _at the portfolio level_, along the lines suggested by Mills over twenty years ago, could go a long ways in guaranteeing the consistency, transparency, and objectivity that is, in large part, responsible for this toxicity. [has been lacking from the resource allocation process.] It would also be nice if such a method took explicit account of the risk associated with each research alternative, and were quick and inexpensive to implement relative to consensus building mechanisms such as the Analytical Hierarchy Process [@Braunschweig2000]. In this paper, I explore the possibility of building such a tool based on risk-adjusted portfolio optimization techniques developed in the financial context.

## AR4D mean-variance analysis

In particular, I explore the possibility of adapting Mean-Variance (MV) Analysis, first conceptualized by Markowitz [@markowitz1952portfolio] and later formalized and refined by Merton [@merton1972analytic], to the AR4D context. In stock market investment contexts, MV Analysis is used to optimize the investor's return on investment in a portfolio of $n$ risky assets, given the investor's level of risk tolerance. On input, MV Analysis takes the expected returns and risk of each asset in the portfolio, and outputs the precise resource allocation that must be invested in each of the portfolio assets in order to achieve the risk adjusted maximum return. The optimal solution changes with the investor's risk tolerance. The locus of all solutions across all levels of risk tolerance is called the "efficient frontier". In the conventional approach, the math works out such that this frontier takes the shape of a hyperbola (Figure \ref{fig:basic_illust}). Hyperbolae give two solutions for each x-axis input. The investor is of course interested only in the greater of these, given by the upper branch of the hyperbola. The lower branch of the frontier is therefore omitted.

```{r, fig.show = 'hold', fig.width=4, fig.height=3, fig.align='center', fig.cap="\\label{fig:basic_illust}The efficient frontier. Each point on the frontier indicates the highest expected return that can be obtained for a given risk tolerance.", echo=FALSE}
y <- seq(0, 1, length.out = 50)
a <- 1 / 2
b <- 1 / 3
x <- a * sqrt(y^2 / b^2 + 1)
gg <- ggplot(data.frame(x, y), aes(x = x, y = y))
gg <- gg + geom_point()
gg <- gg + labs(x = "Portfolio risk (variance)", y = "Portfolio expected return")
gg <- gg + theme(axis.text = element_blank(),
                 axis.ticks = element_blank())
gg

```

There is an important sense in which this reflects AR4D decisionmaking contexts. Around the time that Merton was writing his seminal paper, for example, it was common practice at the International Center for Tropical Agriculture to guide funding decisions by schematically plotting research proposals in a risk reward space such as that in Figure \ref{fig:basic_illust} [@JCock]. This paper is primarily concerned with the possibility of resuscitating and formalizing this practice.

On the other hand, the AR4D context differs from the financial context in many important respects. Three of these are particularly important to address when adapting MV Analysis to the research portfolio context: scaleability, negative resource allocation shares, and [lack of risk assessment].

-Scaleability: In the financial context, returns scale linearly with investment. In the AR4D context, returns tend to scale sub-linearly with investment. That is, marginal returns are typically diminishing in investment. In the workflow articulated by Mills, the relation between investment and returns would typically be determined during Step 3. If not, then one could, at Step 5, introduce an assumption of functional homogeneity, and then assign degrees of homogeneity through a consensus process. Note that if a program is not scaleable, i.e., if returns to a research program are not a continuous function of investment, then it cannot be part of an optimal portfolio (because the portfolio optimization problem is ill-posed, in that case.)

-Negative resource allocations: A unique optimal resource allocation corresponds to each point along the efficient frontier. In the conventional approach, optimal resource allocation shares can be, and frequently are, negative. A negative sign on a budget share indicates that the investor should invest in the inverse of the corresponding portfolio item. In the financial context, this is possible through short selling, or through investment in financial products that track the inverse of the given asset price.^[However, even in the financial context, negative weights are viewed by many as a methodological nuissance. See, for example, Boyle [-@boyle2014positive]. One problem that arises is that a portfolio with both negative and positive weights implies that the investor must borrow beyond their budget.] In the AR4D context, there is no analogue to either one of these conventions. To force positive budget shares, I replace the linear returns and cost functions with log-linear returns utility and budget utility functions, respectively. In section ... I derive the utility function from the time-honored assumption of marginally diminishing returns to the utility of increments in the quantity of any particular good. This modification violates the budget constraint, but I show that the budget constraint can then be arbitrarily restored by virtue of the fact that utility functions are defined only up to an affine transformation. 

-Risk assessment: Risk-adjusted portfolio optimization presupposes some means of assessing the variances and covariances of portfolio items in the first place. In the data-rich financial context, variances and covariances are easy to measure empircally. Assessment of the risks involved an AR4D program is far less straightforward. There is no time series on which   This would occur in Step 3, together with the assessment of impact and scalability. Alston and Norton acknowledged in 1995 that the treatment of risk in impact assessment models was "rudimentary and in need of further refinement" [-@Alston1995]. This remains true today. In section ... I introduce a way of "reverse engineering" a covariance matrix based on stakeholder consensus.

key points of the argument:
-The covariance matrix, or a close approx to it, can be expressed L'L
-The Signals in L tend to be thematically organized (as shown in the financial example)
-In AR4D context we do not have data (which the fin context has), but we do have a good understanding of the dimensionality of the portfolio space, and of how the different portfolio items map onto it (which is lacking in the fin context).
-I am essentially arguing that the latter can stand in for the former.
-May not be exactly consistent with the true matrix, but it is consistent with our knowledge of the space.
[-There is a real sense in which we do not want the reconstructed correlation matrix to equal the empirical correlation matrix. The reconstructed matrix purges noise. But it is not invertible.]

-The covariance matrix is defined up to an orthogonal rotation of its eigenvectors

-The eigenvalues of the Loadings corelation matrix are the same as the (purged) data correlation matrix

-In a workshop, stakeholders define dimensions...
-...
-One would maybe want to use the purged version of the data correlation matrix (LL') in the MV analysis, but this is not possible since it is not invertible.
-Instead, we use the correlation matrix of the elicited loadings (which will have the same eigenvalues of the data correlation matrix).




As mentioned in the introduction, in practice the effectiveness of MV Analysis is severely limited by noise-induced inaccuracy in the efficient frontier. The empirical covariance matrix $\Sigma$ is typically ill conditioned, such that it is impossible to invert. A simple fix for this problem is to use the correlation matrix $K=D_{\sigma}^{-1}\Sigma D_{\sigma}^{-1}$ instead of the correlation matrix in the equations above. Even so, real portfolio returns tend to be lower, and risk higher, than the MV frontier would suggest. In the examples below, I propose a new approach to this problem. I conduct MV Analysis over a portfolio containing only the signals (Eq. \ref{eq:signalEq}). This may be characterized as a dimensional reduction of the original problem from several assets to just a few key cross-cutting trends.

# Brief introduction to risk adjusted portfolio optimization

The risk-adjusted optimization problem for a portfolio of $n$ risky assets is formulated as follows. 

\begin{equation}
\max_{\mathbf{w}}\:R \:\:\:\:s.t. \:\:\: C=\bar{C} \:, \:\:\: \sigma^2 = \bar{\sigma}^2
\end{equation}

where $\mathbf{w}$ is the vector of budget shares, $R$ is the expected portfolio return, defined $R = \mathbf{w} \cdot \boldsymbol{\mu}$, where $\boldsymbol{\mu}$ is the vector of expected returns of each asset; $C$ is the cost, defined $C = \mathbf{w} \cdot \mathbf{1}$, constrained to sum to the budget $\bar{C}$; and $\sigma^2$ is the portfolio variance (a measure of the portfolio risk), defined $\sigma^2 = \mathbf{w} \cdot \Sigma \cdot \mathbf{w}$, where $\Sigma$ is the asset covariance matrix; and $\sigma^2$ is constrained to equal a tolerance $\bar{\sigma}^2$ set by the investor.

(In the financial context, the problem is usually formulated as a risk minimization problem subject to a budget constraint and return target. Both approaches yield the same outputs. In the financial context, the problem includes the option to invest in one risk free asset. There is no risk free investment in the AR4D context, so the set up here focuses on optimization of the risky portfolio only.)

The Lagrangian is then

\begin{equation}
\mathcal{L} = R - \lambda_C(C - \bar{C}) + \lambda_{\sigma}(\sigma^2 - \bar{\sigma}^2)
\end{equation}

with first order conditions

\begin{equation}
\nabla \mathcal{L} = \boldsymbol{\mu} - \lambda_C \mathbf{1} + 2 \lambda_{\sigma} \Sigma \cdot \mathbf{w} = \mathbf{0}
\end{equation}

and second order conditions

\begin{equation}
\mathbf{w} \cdot \nabla^2 \mathcal{L} \cdot \mathbf{w} = 2 \lambda_{\sigma} \sigma^2 < 0
\end{equation}

(where $\nabla^2 \mathcal{L}$ is the Hessian matrix of $\mathcal{L}$.) Since $\Sigma$ is symmetric and the variances of random variables are, by definition, positive, then $\Sigma$ is positive semi-definite. Hence, a maximum or break-even point is guaranteed so long as $\lambda_{\sigma}$ is negative.

Dotting the first order conditions through by $\mathbf{w}$ gives the equation for the efficient frontier.

\begin{equation}
R^* = \lambda_C \bar{C} + 2 \lambda_{\sigma} \bar{\sigma}^2
\label{eq:rStar}
\end{equation}

where the asterisk on $R$ indicates that this is the maximum portfolio return given budget constraint $\bar{C}$ and risk tolerance $\bar{\sigma}^2$.

Note, in passing, that equation \ref{eq:rStar} implies that the risk shadow price is proportional to the expected reward to risk ratio.

\begin{equation}
\lambda_{\sigma} = \frac{\psi}{2\bar{\sigma}^2} \: ; \:\:\: \psi = R^* - \lambda_C \bar{C}
\end{equation}

where $\psi = R^* - \lambda_C \bar{C}$ is the expected reward, i.e., the expected net revenue, adjusted by the budget shadow price $\lambda_C$.

Now, dotting the first order conditions through by $\Sigma^{-1}$ and rearranging gives an equation for the optimal budget shares.

\begin{equation}
\mathbf{w}^* = -\frac{1}{2 \lambda_{\sigma}} \Sigma^{-1} \cdot \nabla \psi \: ; \:\:\: \nabla \psi = \boldsymbol{\mu} - \lambda_C \mathbf{1}
\label{eq:budgetShares}
\end{equation}

Note, in passing, that dotting this through by $\nabla \psi$ gives another instructive equation for $\lambda_{\sigma}$.

\begin{equation}
\lambda_V = \frac{d_m^2}{2 \psi} \: ; \:\:\:\:d_m^2 = \nabla \psi \cdot \Sigma^{-1} \cdot \nabla \psi
\end{equation}

This says that the risk shadow price is inversely proportional to the ratio of the squared Mahalanobis distance ($d_m^2$) of the portfolio net reward gradient to the net reward. In this setting, the Mahalanobis distance reflects how improbable a given portfolio is. (Note that $d_m^2 = 0$ only when $\nabla \psi = \mathbf{0}$, which occurs only when every component in the vector of asset returns ($\mu$) equals the budget shadow price ($\lambda_C$)] Moreover, combining this with the previous expression for $\lambda_{\sigma}$ gives an instructive expression.

\begin{equation}
\psi = d_m \bar{\sigma}
\end{equation}

This says that the investor's expected portfolio return is thus equal to their risk tolerance scaled by the improbability of the portfolio. Alternatively, this can be rearranged to give another instructive expression

\begin{equation}
p  = \bar{\sigma} \over \psi \: ; \:\: p = 1 \over d_m
\end{equation}

which says that the probability of the optimal portfolio ($p$) is proportional to the ratio of the portfolio risk to portfolio expected return.

Returning to the task at hand, in order to evaluate the optimal budget shares or the frontier in equations \ref{eq:rStar} and \ref{eq:budgetShares}, we must first solve for the cost and risk shadow prices $\lambda_C$ and $\lambda_{\sigma}$.

To do this, first note that the budget shares equation can be rewritten as follows:

\begin{equation}
\mathbf{w}^* = \frac{1}{2 \lambda_{\sigma}} \Sigma^{-1} \cdot [\boldsymbol{\mu}, \: \mathbf{1}] \left[\begin{matrix} 1 \\ -\lambda_C \\ \end{matrix} \right]
\end{equation}

Now, dotting through by $[\boldsymbol{\mu}, \mathbf{1}]$ gives

\begin{equation}
\left[\begin{matrix}
R \\
\bar{C} \\
\end{matrix} \right]  = \frac{1}{2 \lambda_{\sigma}} M\left[\begin{matrix}
1 \\
-\lambda_C \\
\end{matrix} \right]
\end{equation}

where $M$ has been introduced to stand for the matrix

\begin{equation}
M = [\boldsymbol{\mu}, \: \mathbf{1}]' \cdot \Sigma^{-1} \cdot [\boldsymbol{\mu}, \: \mathbf{1}]
\end{equation}

Let $M$ be called the "Merton matrix", after the author in whose footsteps I am now following [@merton1972analytic]. Pre-multiplying both sides of the previous equation by the inverse Merton matrix and rearranging gives the following expression: 

\begin{equation}
2 M^{-1}\left[\begin{matrix}
R \\
\bar{C} \\
\end{matrix} \right]  = \left[\begin{matrix}
1/\lambda_V \\
-\lambda_C / \lambda_{\sigma} \\
\end{matrix} \right]
\end{equation}

For any given return target $R$ and budget $\bar{C}$, then, the cost and risk shadow prices are given by this equation. With values for $\lambda_C$ and $\lambda_{\sigma}$ in hand, the budget shares and risk associated with the chosen return target can be evaluated.

<!-- Before exploring the possibility of applying these tools to the AR4D context, it will be instructive to first apply them in the financial context where they have been developed. -->

An example application of MV Analysis is presented below using daily financial data for 46 securities over the period 20 June 2018 to 23 March 2019, downloaded from yahoo finance using the R tidyquant package. These assets are deliberately numerous and varied in order to demonstrate the reach and limits of MV Analysis. They are grouped into eight categories in order to facilitate interpretation.

Period returns for each asset in the dataset are presented in the bottom of Figure \ref{fig:hReturns}. The period return of the $i^{th}$ portfolio asset $\bar{r}_{i}$ with given price time series $\mathbf{p}$ is calculated as 

$$
\bar{r}_i = \frac{p_T - p_1}{p_1}
$$
where $T$ denotes the length of the time series. A box and whiskers plot of the daily returns is also given in Figure \ref{fig:hReturns}. Daily returns are calculated
$$
r_{it} = \frac{p_t - p_{t-1}}{p_{t-1}}
$$
for any given day $t$ in the series. As a practical matter, note that the period returns can also be calculated from the daily returns as follows.

$$
\bar{r}_i = \prod_{t = 2}^T (1 + r_{it}) - 1
$$
<!-- They are mostly exchange traded funds tracking broad categories of stocks, bonds, markets, and commodities, plus major currency pairs traded on the foreign exchange market. -->

<!-- [Table here detailing the ETF names, what they track, and my category name] -->


```{r, fig.width=14, fig.height=14, fig.align='center', echo = FALSE, include = FALSE}
#options(warn = -1); options(scipen = 999)
spy_sector_symbs <- c("XLF", "XLC", "XLY", "XLP", "XLV", "XLK", "RWR",
                      "XLU", "XLI", "XBI", "IYT") #"TTEK"
spy_sector_detail <- c("Financials", "Communications", "Luxury goods", "Staple goods",
                       "Healthcare", "Technology", "Real estate", "Utilities", "Industrial",
                       "Biotechnology", "Transportation") #"Gov. foreign aid"
minerals_symbs <- c("GLD", "SLV", "PPLT", "JJC", "JJM") #"XME"
minerals_detail <- c("Gold", "Silver", "Platinum", "Copper", "Industrial metals") #"US metals and mining"
agriculture_symbs <- c("JJG", "BAL", "SOYB", "SGG", "JO", "NIB", "DBA")
agriculture_detail <- c("Grains", "Cotton", "Soybean", "Sugar", "Coffee", "Cacao", "General agriculture")
energy_symbs <- c("WTI", "FUE", "WOOD", "ICLN", "KOL", "UNG")
energy_detail <- c("Oil (W&T Offshore Inc.)", "Biofuels", "Timber", "Clean energy", "Coal", "US natural gas")
#currency_symbs <- c("EMLC", "UUP", "FXE", "FXY", "FXF", "FXC", "FXB", "FXA")
#currency_detail <- c("Emerging mkt currencies", "USD", "EUR", "JPY", "CHF", "CND", "GBP", "AUD")
currency_symbs <- c("EURUSD=X", "JPY=X", "CHF=X", "CAD=X",
                    "GBPUSD=X", "AUDUSD=X", "INR=X")
currency_detail <- c("EUR/USD", "USD/JPY",
                     "USD/CHF", "USD/CAD", "GBP/USD", "AUD/USD", "USD/INR")
emerg_mkt_symbs <- c("EMLC", "ELD", "BKF", "VWOB")
emerg_mkt_detail <- c("Emerg mkts debt", "Emerg mkts gov. bonds", "BRIC countries", "Emerging mkt currencies")
crypto_symbs <- c("BLOK", "LEGR", "BCNA")
crypto_detail <- c("Blockchain tech.", "Blockchain companies", "Blockchain in China")
Tbond_symbs <- c("IEI", "IEF", "TLT")#, "BIL"
Tbond_detail <- c("T-bond 3-7 yrs", "T-bond 7-10 yrs", "T-bond 20+ yrs") #"T-bond 1-3 months"

ts_symb_vec <- c(spy_sector_symbs, minerals_symbs, agriculture_symbs, energy_symbs,
                 currency_symbs, emerg_mkt_symbs, crypto_symbs, Tbond_symbs)
ts_detail_vec <- c(spy_sector_detail, minerals_detail, agriculture_detail, energy_detail,
                   currency_detail, emerg_mkt_detail, crypto_detail, Tbond_detail)
df_symb_detail <- data.frame(symbol = ts_symb_vec, detail = ts_detail_vec)
#--
#length(ts_symb_vec)
#--
# list_groups <- list(spy_sector_symbs, minerals_symbs, agriculture_symbs, energy_symbs,
#                      currency_symbs, emerg_mkt_symbs, crypto_symbs, Tbond_symbs)
#-------------------------------------------
# Create grouping info for plots (required input into functions)
list_groups <- list(spy_sector_detail, minerals_detail, agriculture_detail, energy_detail,
                    currency_detail, emerg_mkt_detail, Tbond_detail) #crypto_detail,
group_names <- c("US Sectors", "Minerals", "Agriculture", "Energy", "Major Currency Pairs",
                 "Emerging Markets", "T-Bonds") #"Cryptocurrencies/\nBlockchain"
n_groups <- length(list_groups)
#group_colors <- RColorBrewer::brewer.pal(n = n_groups, "Dark2")
# "Darjeeling"
# group_colors <- wesanderson::wes_palette("Darjeeling1", n = n_groups, type = "continuous")
bag_of_colors <- randomcoloR::distinctColorPalette(k = 5 * n_groups)
group_colors <- sample(bag_of_colors, n_groups)
#--------------------------------------------------------------
# Define function to order data by group
group_fn <- function(group_info){
  list_groups <- group_info[[1]]
  group_names <- group_info[[2]]
  group_colors <- group_info[[3]]
  varNames_ordered <- do.call(c, list_groups)
  n_groups <- length(group_names)
  n_items <- length(varNames_ordered)
  if(is.na(group_colors)){
    bag_of_colors <- randomcoloR::distinctColorPalette(k = 5 * n_groups)
    group_colors <- sample(bag_of_colors, n_groups)
    #group_colors <- viridis::viridis_pal(option = "D")(length(group_names))
  }
  #if(reverse_order){group_colors <- rev(group_colors)}
  #varNames_ordered <- colnames(mat_pctDiff)
  group_vec <- rep(NA, n_items)
  group_color_vec <- rep(NA, n_items)
  for(i in 1:n_groups){
    this_group_vec <- list_groups[[i]]
    this_group_name <- group_names[i]
    this_group_color <- group_colors[i]
    group_vec[which(varNames_ordered %in% this_group_vec)] <- this_group_name
    group_color_vec[which(varNames_ordered %in% this_group_vec)] <- this_group_color
  }
  ind_ordered_cols <- order(factor(group_vec))
  cols_ordered_by_group <- as.character(varNames_ordered[ind_ordered_cols])
  group_color_vec <- group_color_vec[ind_ordered_cols]
  group_vec_ordered <- group_vec[ind_ordered_cols]
  out_list <- list(cols_ordered_by_group, group_color_vec, group_vec_ordered, ind_ordered_cols)
  return(out_list)
}
#--------------------------------------------------------------
group_info <- list(list_groups, group_names, group_colors)
  outlist <- group_fn(group_info)
  cols_ordered_by_group <- outlist[[1]]
  group_color_vec <- outlist[[2]]
  group_vec_ordered <- outlist[[3]]
  ind_ordered_cols <- outlist[[4]]
  df_match_group <- data.frame(Item = cols_ordered_by_group, Group = group_vec_ordered)
#--------------------------------------------------------------
#per_ema <- 13
#fromdate = Sys.Date() - 360
fromdate = "2018-06-20"
todate = "2019-08-09"
tbl_ohlcv <- tq_get(ts_symb_vec, get = "stock.prices", from = fromdate, to = todate)
df_ohlcv <- as.data.frame(tbl_ohlcv)
#--
# Crypto ts can have duplicate date entries sometimes (BTC-USD)
# Get rid of these duplicates, if any
# df_ohlcv <- as.data.frame(df_ohlcv %>% group_by(symbol) %>% mutate(dup = duplicated(date)))
# df_ohlcv <- subset(df_ohlcv, dup == F)
# df_ohlcv$dup <- NULL
#--

#df_ohlcv$p <- rowSums(df_ohlcv[, c(4:6)]) / 3
#--
#df <- df_ohlcv[, c("date", "symbol", "p")]
# df <- df_ohlcv %>% group_by(symbol) %>% tq_transmute(select = adjusted, 
#                      mutate_fun = periodReturn, 
#                      period     = "monthly")
#--
df <- df_ohlcv[, c("date", "symbol", "adjusted")]
df <- df %>% spread(symbol, adjusted)
ind_shift <- which(colnames(df) %in% currency_symbs)
df[, ind_shift] <- rbind(rep(NA, length(ind_shift)), df[-nrow(df), ind_shift])
df <- df[-c(1, nrow(df)), ]
date_vec <- df$date
mat_ts_dy <- na.approx(df[, -1])

#o <- apply(mat_ts_dy, 2, function(x) length(which(is.na(x))))
#table(o)
#which(o==1)
#xts_ts_dy <- xts(mat_ts_dy, date_vec)
#date_vec <- index(xts_ts_dy)
#-----
mat_pctDiff_dy <- diff(mat_ts_dy) / mat_ts_dy[-nrow(mat_ts_dy), ] * 100
#mat_pctDiff_dy <- diff(log(mat_ts_dy))
row.names(mat_pctDiff_dy) <- as.character(date_vec[-1])
ts_avg_dy <- rowMeans(mat_pctDiff_dy)
mu_ret_dy <- colMeans(mat_pctDiff_dy)
sd_ret_dy <- apply(mat_pctDiff_dy, 2, sd)

mat_ts_wk <- to.weekly(xts(mat_ts_dy, date_vec), OHLC = F)
mat_pctDiff_wk <- diff(mat_ts_wk) / mat_ts_wk[-nrow(mat_ts_wk), ] * 100
mat_pctDiff_wk <- as.matrix(mat_pctDiff_wk[-1, ])
ts_avg_wk <- rowMeans(mat_pctDiff_wk)
mu_ret_wk <- colMeans(mat_pctDiff_wk)
sd_ret_wk <- apply(mat_pctDiff_wk, 2, sd)

mat_ts_mo <- to.monthly(xts(mat_ts_dy, date_vec), OHLC = F)
mat_pctDiff_mo <- diff(mat_ts_mo) / mat_ts_mo[-nrow(mat_ts_mo), ] * 100
mat_pctDiff_mo <- as.matrix(mat_pctDiff_mo[-1, ])
ts_avg_mo <- rowMeans(mat_pctDiff_mo)
mu_ret_mo <- colMeans(mat_pctDiff_mo)
sd_ret_mo <- apply(mat_pctDiff_mo, 2, sd)
#----------------------------------------------
# Compare histograms of daily, weekly, and monthly returns
# df_plot <- data.frame(ts_id = names(mu_ret_dy), mu_ret_dy, mu_ret_wk, mu_ret_mo)
# colnames(df_plot)[-1] <- c("Mean Daily Return", "Mean Weekly Return", "Mean Monthly Return")
# gathercols <- colnames(df_plot)[-1]
# df_plot <- df_plot %>% gather_("Return Type", "Value", gathercols)
# #gg <- ggplot(df_plot, aes(Value, fill = `Return Type`))
# gg <- ggplot(df_plot, aes(Value))
# gg <- gg + geom_density(alpha = 0.4)
# gg <- gg + facet_wrap(~`Return Type`, nrow = 1, scales = "free")
# #gg <- gg + coord_cartesian(xlim = c(-0.02, 0.02))
# gg
#----------------------------------------------
#ind_rm_ema <- 1:(per_ema - 1)
# mat_pctDiff <- apply(mat_pctDiff, 2, function(x) x - EMA(x, per_ema))
# mat_pctDiff <- mat_pctDiff[-ind_rm_ema, ]
# date_vec <- df$date[-c(ind_rm_ema, ind_rm_na)]
#----------------------------------------------
mat_ts_in <- mat_ts_dy
ts_avg_in <- ts_avg_dy
# nab_pctRet_in <- mu_ret_dy
# sd_ret_in <- sd_ret_dy
#----------------------------------------------
# (Convert to basis points, aka pips? -> *10^2)
# (1 basis point = 1 percent of 1 percent)
mat_pctDiff <- mat_pctDiff_dy
#mat_pctDiff <- mat_pctDiff_wk
date_vec <- row.names(mat_pctDiff)
#length(date_vec)
row.names(mat_pctDiff) <- as.character(date_vec)
colnames(mat_pctDiff) <- as.character(df_symb_detail$detail[order(df_symb_detail$symbol)])
#----------------------------------------------
check_on_data <- F
if(check_on_data){
  df_plot <- as.data.frame(mat_pctDiff)
  gathercols <- colnames(df_plot)
  df_plot$Date <- date_vec
  df_plot <- df_plot %>% gather_("Security", "Weekly change (%)", gathercols)
  df_plot$Type <- NA
  df_plot$Type[which(df_plot$Security %in% spy_sector_detail)] <- "US Sectors"
  df_plot$Type[which(df_plot$Security %in% minerals_detail)] <- "Minerals"
  df_plot$Type[which(df_plot$Security %in% agriculture_detail)] <- "Agriculture"
  df_plot$Type[which(df_plot$Security %in% energy_detail)] <- "Energy"
  df_plot$Type[which(df_plot$Security %in% currency_detail)] <- "Major Currency Pairs"
  df_plot$Type[which(df_plot$Security %in% emerg_mkt_detail)] <- "Emerging Markets"
  df_plot$Type[which(df_plot$Security %in% crypto_detail)] <- "Cryptocurrencies/\nBlockchain"
  df_plot$Type[which(df_plot$Security %in% Tbond_detail)] <- "T-bonds"
  #------------------------------------------------------------
  # gg <- ggplot(df_plot, aes(x = Year, y = Value, group = Security, color = Security))
  # gg <- gg + geom_line()
  # gg <- gg + facet_wrap(~ Type, nrow = 3, scales = "free")
  # gg <- gg + theme(legend.position = "none")
  # gg
  #------------------------------------------------------------
  this_type <- "US Sectors"
  df_plot2 <- subset(df_plot, Type == this_type)
  gg <- ggplot(df_plot2, aes(x = Date, y = `Weekly change (%)`, group = Security, color = Security))
  gg <- gg + geom_line(lwd = 1.1)
  gg <- gg + geom_point(size = 2)
  gg <- gg + labs(title = this_type)
  gg
  this_type <- "Minerals"
  df_plot2 <- subset(df_plot, Type == this_type)
  gg <- ggplot(df_plot2, aes(x = Date, y = `Weekly change (%)`, group = Security, color = Security))
  gg <- gg + geom_line(lwd = 1.1)
  gg <- gg + geom_point(size = 2)
  gg <- gg + labs(title = this_type)
  gg
  this_type <- "Agriculture"
  df_plot2 <- subset(df_plot, Type == this_type)
  gg <- ggplot(df_plot2, aes(x = Date, y = `Weekly change (%)`, group = Security, color = Security))
  gg <- gg + geom_line(lwd = 1.1)
  gg <- gg + geom_point(size = 2)
  gg <- gg + labs(title = this_type)
  gg
  this_type <- "Energy"
  df_plot2 <- subset(df_plot, Type == this_type)
  gg <- ggplot(df_plot2, aes(x = Date, y = `Weekly change (%)`, group = Security, color = Security))
  gg <- gg + geom_line(lwd = 1.1)
  gg <- gg + geom_point(size = 2)
  gg <- gg + labs(title = this_type)
  gg
  this_type <- "Major Currency Pairs"
  df_plot2 <- subset(df_plot, Type == this_type)
  gg <- ggplot(df_plot2, aes(x = Date, y = `Weekly change (%)`, group = Security, color = Security))
  gg <- gg + geom_line(lwd = 1.1)
  gg <- gg + geom_point(size = 2)
  gg <- gg + labs(title = this_type)
  gg
  this_type <- "Emerging Markets"
  df_plot2 <- subset(df_plot, Type == this_type)
  gg <- ggplot(df_plot2, aes(x = Date, y = `Weekly change (%)`, group = Security, color = Security))
  gg <- gg + geom_line(lwd = 1.1)
  gg <- gg + geom_point(size = 2)
  gg <- gg + labs(title = this_type)
  gg
  this_type <- "Cryptocurrencies/Blockchain"
  df_plot2 <- subset(df_plot, Type == this_type)
  gg <- ggplot(df_plot2, aes(x = Date, y = `Weekly change (%)`, group = Security, color = Security))
  gg <- gg + geom_line(lwd = 1.1)
  gg <- gg + geom_point(size = 2)
  gg <- gg + labs(title = this_type)
  gg
  this_type <- "T-bonds"
  df_plot2 <- subset(df_plot, Type == this_type)
  gg <- ggplot(df_plot2, aes(x = Date, y = `Weekly change (%)`, group = Security, color = Security))
  gg <- gg + geom_line(lwd = 1.1)
  gg <- gg + geom_point(size = 2)
  gg <- gg + labs(title = this_type)
  gg
}

```


```{r, fig.show = 'hold', fig.cap="\\label{fig:hReturns}Historical (top) daily returns and (bottom) period return, train and test data.", fig.width=7, fig.height=7, fig.align='left', echo=FALSE}
#=======================================================================
# Define function to plot period returns as barchart with colors
plot_returns_barchart <- function(df_pctRet, group_colors, list_graph_options, graph_on = T){
  
  if(is.null(list_graph_options)){
    fig_title <- NULL
    legend_position <- "bottom"
    ylab <- NULL
  }else{
    fig_title <- list_graph_options[["fig_title"]]
    legend_position <- list_graph_options[["legend_position"]]
    ylab <- list_graph_options[["ylab"]]
  }
  
  colnames(df_pctRet)[1] <- "Period Return"
  df_plot <- df_pctRet
  gg <- ggplot(df_plot, aes(x = Item, y = `Period Return`, fill = Group))
  #gg <- gg + scale_color_brewer(palette = "Dark2")
  gg <- gg + scale_fill_manual(values = group_colors)
  gg <- gg + geom_bar(stat = "identity", color = "black", position = "dodge")
  if("Dataset" %in% colnames(df_plot)){
      gg <- gg + facet_wrap(~Dataset, ncol = 1)
  }
  if(!is.null(fig_title)){
      gg <- gg + labs(title = fig_title)
gg <- gg + theme(plot.title = element_text(face = "bold", size = 9))
  }
  if(!is.null(ylab)){
    gg <- gg + labs(y = ylab)
  }else{
    gg <- gg + theme(axis.title.y = element_blank())
  }
  gg <- gg + theme(axis.text.x = element_text(size = 9, angle = 60, hjust = 1),
                   #axis.text.x = element_blank(),
                   axis.title.x = element_blank(),
                   axis.text.y = element_text(size = 9),
                   #axis.title.y = element_text(size = 9),
                   legend.position = legend_position,
                   legend.title = element_blank(),
                   legend.text = element_text(size = 9))
  #  gg <- gg + coord_equal()
  #  gg <- gg + coord_flip()
  if(graph_on){print(gg)}
  return(gg)

}

#=======================================================================
# Define function to plot daily returns box and whiskers chart 
plot_boxchart <- function(df_pctDiff, group_colors, list_graph_options, graph_on = T){
  if(is.null(list_graph_options)){
    legend_position <- "right" #"none"
    axis_titles <- "on" #"off"
    axis_text <- "on" #"y only"
    fig_title <- NULL
  }else{
    legend_position <- list_graph_options[["legend_position"]]
    axis_titles <- list_graph_options[["axis_titles"]]
    axis_text <- list_graph_options[["axis_text"]]
    fig_title <- list_graph_options[["fig_title"]]
  }
  
  df_plot <- df_pctDiff
    gg <- ggplot(df_plot, aes(x = Item, y = `Daily Returns`, fill = Group))
  #gg <- gg + scale_color_brewer(palette = "Dark2")
  gg <- gg + scale_fill_manual(values = group_colors)
  #gg <- gg + geom_bar(stat = "identity", color = "black", position = "dodge")
  gg <- gg + geom_boxplot()
  if("Dataset" %in% colnames(df_plot)){
    gg <- gg + facet_wrap(~Dataset, ncol = 1)
  }
  if(!is.null(fig_title)){
    gg <- gg + labs(title = fig_title)
    gg <- gg + theme(plot.title = element_text(face = "bold", size = 9))
  }
  if(axis_titles == "off"){
    gg <- gg + theme(axis.title = element_blank())
  }
  if(axis_text == "y only"){
    gg <- gg + theme(axis.text.x = element_blank(),
                     axis.text.y = element_text(size = 9))
  }
  if(axis_text == "on"){
    gg <- gg + theme(axis.text.y = element_text(size = 9),
                     axis.text.x = element_text(size = 9, angle = 60, hjust = 1))
  }
    gg <- gg + theme(legend.position = legend_position)
    if(graph_on){print(gg)}
    return(gg)

}

#=======================================================================
# Define function to order data frames by group for period returns barchart and daily returns box and whiskers plots
get_hist_dfs_ready <- function(mat_pctDiff, group_info){
  #------------------------------------------------------------
  # Group the vars according to provided group info
  #varNames_ordered <- do.call(c, group_info[[1]])
  outlist <- group_fn(group_info)
  cols_ordered_by_group <- outlist[[1]]
  group_color_vec <- outlist[[2]]
  group_vec_ordered <- outlist[[3]]
  ind_ordered_cols <- outlist[[4]]
  df_match_group <- data.frame(Item = cols_ordered_by_group, Group = group_vec_ordered)
  mat_pctDiff_ordered <- mat_pctDiff[, cols_ordered_by_group]
  #------------------------------------------------------------
  # Get date info
  from_date <- row.names(mat_pctDiff_ordered)[1]
  to_date <- row.names(mat_pctDiff_ordered)[nrow(mat_pctDiff_ordered)]
  from_date <- gsub("-", "/", from_date)
  to_date <- gsub("-", "/", to_date)
  date_interval <- paste(from_date, to_date, sep = " - ")
  #------------------------------------------------------------
  # Get returns barchart df organized by group
  nab_pctRet <- apply(mat_pctDiff_ordered, 2, function(x) prod(1 + x)) - 1
  df_pctRet <- data.frame(Returns = nab_pctRet)
  df_pctRet$Item <- row.names(df_pctRet)
  df_pctRet$Group <- factor(group_vec_ordered)
  df_pctRet$Item <- factor(df_pctRet$Item, levels = cols_ordered_by_group)
  #------------------------------------------------------------
  # Get boxplot df organized by group
      df_pctDiff <- as.data.frame(mat_pctDiff_ordered)
    df_pctDiff <- df_pctDiff %>% gather_("Item", "Daily Returns", colnames(df_pctDiff))
    df_match_group <- data.frame(Item = cols_ordered_by_group, Group = group_vec_ordered)
    df_pctDiff <- merge(df_pctDiff, df_match_group, by = "Item")
    df_pctDiff$Item <- factor(df_pctDiff$Item, levels = cols_ordered_by_group)
    df_pctDiff$Group <- factor(df_pctDiff$Group)
  #------------------------------------------------------------
  out_list <- list(df_pctRet, df_pctDiff, date_interval)
  return(out_list)
}


#=======================================================================
# Define function to plot covariance/correlation matrices
plot_covmat <- function(covmat, graph_on = T){
  #covmat <- cov(mat_pctDiff_train_mv)
  covmat[upper.tri(covmat)] <- NA
  df_plot <- covmat %>% tbl_df()
  these_levels <- colnames(df_plot)
  df_plot$ItemX <- colnames(df_plot)
  gathercols <- colnames(df_plot)[-ncol(df_plot)]
  df_plot <- df_plot %>% gather_("ItemY", "Value", gathercols)
  df_plot$ItemX <- factor(df_plot$ItemX, levels = these_levels)
  df_plot$ItemY <- factor(df_plot$ItemY, levels = these_levels)

  #midpoint <- min(covmat) + (max(covmat) - min(covmat)) / 2
  midpoint <- 0
  gg <- ggplot(df_plot, aes((ItemX), (ItemY)))
  gg <- gg + geom_tile(aes(fill = Value))#, width = 4, height = 4)
  gg <- gg + geom_text(aes(label = round(Value, 2)), size = 3)
  gg <- gg + labs(title = "Covariance Matrix")
  gg <- gg + theme_bw()
  gg <- gg + theme(axis.text.x = element_text(angle = 60, hjust = 1),
                 axis.title = element_blank(),
                 legend.title = element_blank(),
                 plot.title = element_text(face = "bold", size = 9))
  gg <- gg + scale_fill_gradient2(low = "khaki", mid = "cyan", high = "magenta", midpoint, na.value = "white")
  if(graph_on){print(gg)}
  return(gg)

}
#=======================================================================
# End function definition
#=======================================================================
# Plot historical period returns, daily returns box and whiskers, for train and test data, using functions defined above
# ind_train <- 1:round(nrow(mat_pctDiff) * 1 / 2)
# ind_test <- setdiff(1:nrow(mat_pctDiff), ind_train)
length_Q <- 67
ind_train <- (nrow(mat_pctDiff) - 2 * length_Q + 1):(nrow(mat_pctDiff) -  length_Q)
ind_test <- (nrow(mat_pctDiff) - length_Q + 1):nrow(mat_pctDiff)

mat_pctDiff_train <- mat_pctDiff[ind_train, ]
mat_pctDiff_test <- mat_pctDiff[ind_test, ]

out_list <- get_hist_dfs_ready(mat_pctDiff_train * 10^-2, group_info)
df_pctRet_train <- out_list[[1]]
df_pctDiff_train <- out_list[[2]]
date_interval_train <- out_list[[3]]
df_pctRet_train$Returns <- df_pctRet_train$Returns * 100
df_pctDiff_train$`Daily Returns` <- df_pctDiff_train$`Daily Returns` * 100

out_list <- get_hist_dfs_ready(mat_pctDiff_test * 10^-2, group_info)
df_pctRet_test <- out_list[[1]]
df_pctDiff_test <- out_list[[2]]
date_interval_test <- out_list[[3]]
df_pctRet_test$Returns <- df_pctRet_test$Returns * 100
df_pctDiff_test$`Daily Returns` <- df_pctDiff_test$`Daily Returns` * 100

#----------------------------------------------------------
# Separate out a reduced matrix for the basic MV Analysis demonstration
# group_names
demonstration_group <- c("US Sectors")
df_pctRet_train_mv <- subset(df_pctRet_train, Group %in% demonstration_group)
df_pctRet_test_mv <- subset(df_pctRet_test, Group %in% demonstration_group)
ind_group <- which(group_names %in% demonstration_group)
list_groups <- group_info[[1]]
these_items <- do.call(c, list_groups[ind_group])
ind_keep <- which(colnames(mat_pctDiff_train) %in% these_items)
mat_pctDiff_train_mv <- mat_pctDiff_train[, ind_keep]
mat_pctDiff_test_mv <- mat_pctDiff_test[, ind_keep]
#covmat <- cov(mat_pctDiff_train_mv); kappa(covmat)
#----------------------------------------------------------
fig_title <- "Period Returns"
list_graph_options <- list()
list_graph_options[["fig_title"]] <- fig_title
list_graph_options[["legend_position"]] <- "none"
list_graph_options[["ylab"]] <- "Percent"
gg_barchart <- plot_returns_barchart(df_pctRet_train_mv, group_colors, list_graph_options, graph_on = F)
#----------------------------------------------------------
# fig_title <- paste("Daily Returns ", date_interval_train)
#     list_graph_options <- list()
#     list_graph_options[["legend_position"]] <- "none"
#     list_graph_options[["axis_titles"]] <- "off"
#     list_graph_options[["axis_text"]] <- "y only"
#     list_graph_options[["fig_title"]] <- fig_title
#   gg_boxplot <- plot_boxchart(df_pctDiff_train, group_colors, list_graph_options, graph_on = F)
#----------------------------------------------------------
# Covariance matrix plot
covmat <- cov(mat_pctDiff_train_mv)
#kappa(covmat)
gg_covmat <- plot_covmat(covmat, graph_on = F)
#----------------------------------------------------------
#gg_boxplot + gg_barchart + plot_layout(ncol = 1, heights = c(2, 1))
gg_barchart + gg_covmat + plot_layout(ncol = 1, heights = c(1, 2))
#----------------------------------------------------------
# If you're going to do train and test datasets together:
# df_pctRet_train$Dataset <- paste("Train Data ", date_interval_train)
# df_pctDiff_train$Dataset <- paste("Train Data ", date_interval_train)
# df_pctRet_test$Dataset <- paste("Test Data ", date_interval_test)
# df_pctDiff_test$Dataset <- paste("Test Data ", date_interval_test)
# df_pctRet <- rbind(df_pctRet_train, df_pctRet_test)
# df_pctDiff <- rbind(df_pctDiff_train, df_pctDiff_test)
# 
#   fig_title <- "Period Returns"
#   list_graph_options <- list()
#   list_graph_options[["fig_title"]] <- fig_title
#     
# gg_barcharts <- plot_returns_barchart(df_pctRet, group_colors, list_graph_options, graph_on = F)
# 
# 
# fig_title <- "Daily Returns"
#     list_graph_options <- list()
#     list_graph_options[["legend_position"]] <- "none"
#     list_graph_options[["axis_titles"]] <- "off"
#     list_graph_options[["axis_text"]] <- "y only"
#     list_graph_options[["fig_title"]] <- fig_title
#   gg_boxplots <- plot_boxchart(df_pctDiff, list_graph_options, graph_on = F)
# 
#   gg_boxplots + gg_barcharts + plot_layout(ncol = 1, heights = c(2, 1))


```

\pagebreak

As explained above, MV Analysis requires the expected returns of each portfolio item, and their corresponding covariance matrix, in order to calculate the efficient frontier and budget weights. The expected returns are just the period returns. The covariance matrix $\Sigma$ is calculated from the centered daily returns as follows.
$$
\Sigma=\frac{1}{1 - n} X'X \:\:; \:\:\:X_{it} = r_{it}-\frac{1}{T}\sum_{t=1}^Tr_{it}
$$

The optimal frontier and budget shares based on this data are given in Figure \ref{fig:mvConv}. For clarity, the budget shares are aggregated by group. 

```{r, fig.show='hold', fig.width=5, fig.height=4, fig.align='center', fig.cap="\\label{fig:mvConv}Optimal frontier and budget shares. The budget shares are aggregated by group for clarity.", echo = FALSE}
#=======================================================================
# Define portfolio optimization function
optimize_portfolio <- function(covmat, mat_nab, targ_vec,
                               backtest_info = NULL,
                               utility_interpretation = F){
  covmat_inv <- solve(covmat)
  M <- t(mat_nab) %*% covmat_inv %*% mat_nab
  M_inv <- solve(M)
  #print(M)
  x <- -2 * M_inv %*% targ_vec
  # Risk shadow price
  l_V <- 1 / x[1]
  #if(l_V > 0){l_V <- -l_V}
  #print(l_V)
  # Budget shadow price (l_C = lambdas[2], l_R normalized to = 1)
  lambdas <- l_V * x
  l_C <- lambdas[2]
  # Optimal budget shares
  wStar <- -1 / (2 * l_V) * covmat_inv %*% mat_nab %*% lambdas
  #print(sum(wStar))
  # Portfolio variance
  Vtarg <- t(wStar) %*% covmat %*% wStar
  # Rtarg <- targ_vec[1]
  Rtarg <- t(wStar) %*% mat_nab[, 1]
  #----------------------------------------------------
  # check <- t(lambdas) %*% targ_vec + 2*l_V * V
  # print(check)
  #----------------------------------------------------
  # Utility function interpretation of equations
  # (Makes all budget shares positive)
  if(utility_interpretation){
    Exp_wStar <- exp(wStar)
    K <- sum(Exp_wStar)
    wStar <- Exp_wStar / K
    Rtarg <- t(wStar) %*% mat_nab[, 1]
    Vtarg <- t(wStar) %*% covmat %*% wStar
  } 
  #if(l_V > 0){wStar <- rep(NA, length(wStar))}
  #----------------------------------------------------
  if(!is.null(backtest_info)){
    nab_pctRet_test <- backtest_info[["nab_pctRet_test"]]
    covmat_test <- backtest_info[["covmat_test"]]
    Rtest <- t(wStar) %*% nab_pctRet_test
    Vtest <- t(wStar) %*% covmat_test %*% wStar
  }else{
    Rtest <- NULL
    Vtest <- NULL
  }
  #----------------------------------------------------
  frontier_vec <- c(Rtarg, Vtarg, l_V, l_C, Rtest, Vtest)
  list_out <- list(wStar, frontier_vec)
  return(list_out)
  
}
#=======================================================================
# Define function for budget shares plot
plot_budgetShares <- function(df_wStar, group_small = NULL, color_vec = NULL, graph_on = T, list_graph_options = NULL){
    # Budget shares plot
  # Note df_w_in should have just the target risk column (i.e. don't also include the backtest risk column). At any rate, the budget risk column should be the same as the one used in the frontier plot.
  #df_plot <- df_wStar_prop
  #------------------------------------
  if(!is.null(list_graph_options)){
    legend_position <- list_graph_options[["legend_position"]]
    fig_title <- list_graph_options[["fig_title"]]
    axis_titles <- list_graph_options[["axis_titles"]]
    Xaxis_numbers_off <- list_graph_options[["Xaxis_numbers_off"]]
  }else{
      legend_position = "bottom"
      fig_title = NULL
      axis_titles = "on"
      Xaxis_numbers_off = F
  }
  
  #------------------------------------
  df_plot <- df_wStar
  gathercols <- colnames(df_plot)[-1]
  #------------------------------------
  if(!is.null(group_small)){
    mat_plot <- as.matrix(df_plot[, -1])
    mu_vec <- apply(mat_plot, 2, mean)
    ind_group_small <- which(mu_vec < 10^-2)
    other_col <- rowSums(mat_plot[, ind_group_small])
    
    mat_plot <- mat_plot[, -ind_group_small]
    df_plot <- as.data.frame(mat_plot)
    df_plot$Other <- other_col
    df_plot$`Risk (variance)` <- df_wStar$`Risk (variance)`
    gathercols <- colnames(df_plot)[-ncol(df_plot)]
  }
  #------------------------------------
  df_plot$portfolio_id <- 1:nrow(df_wStar)
  df_match_V <- df_plot[, c("portfolio_id", "Risk (variance)")]
  df_plot <- df_plot %>% gather_("Item", "Budget shares", gathercols)
  df_plot <- df_plot %>% group_by(Item) %>% 
    mutate(mu = mean(`Budget shares`)) %>% 
    as.data.frame()
  ind_order_mu <- order(df_plot$mu, df_plot$Item, decreasing = T)
  df_plot$Item <- factor(df_plot$Item,
                         levels = unique(df_plot$Item[ind_order_mu]),
                         ordered = T)
    #------------------------------------
if(is.null(color_vec)){
  # Randomly assign a color to each portfolio item if none assigned
  n_items <- ncol(df_wStar) - 1
    bag_of_colors <- randomcoloR::distinctColorPalette(k = 5 * n_items)
    color_vec <- sample(bag_of_colors, n_items)
}
    #------------------------------------
  gg <- ggplot(df_plot, aes(x = `Risk (variance)`, y = `Budget shares`, fill = Item))
  #legend_position <- "right"
  gg <- gg + geom_area(position = "stack")
  gg <- gg + scale_fill_manual(values = color_vec)

  gg <- gg + theme(legend.title = element_blank(),
                   legend.position = legend_position)
  if(axis_titles == "off"){
    gg <- gg + theme(axis.title = element_blank())
  }
  if(axis_titles == "x only"){
    gg <- gg + theme(axis.title.y = element_blank())
  }
  if(Xaxis_numbers_off){
    gg <- gg + theme(axis.text.x = element_blank())
  }
  if(!is.null(fig_title)){
    gg <- gg + labs(title = fig_title)
    gg <- gg + theme(plot.title = element_text(size = 8))
  }
  if(length(unique(df_plot$Item)) > 15){
    gg <- gg + theme(legend.position = "none")
  }
  gg_weights <- gg
  
  if(graph_on){print(gg)}
  
  return(gg_weights)
  
}
#=========================================================
plot_frontier <- function(df_frontier, graph_on = T){
    df_plot <- df_frontier
  if(ncol(df_plot) == 3){
    gathercols <- colnames(df_plot)[2:3]
    df_plot <- df_plot %>% gather_("Type", "Value", gathercols)
    gg <- ggplot(df_plot, aes(x = `Risk (variance)`, y = Value, group = Type, color = Type))
  }else{
    gg <- ggplot(df_plot, aes(x = `Risk (variance)`, y = `Return target`))
  }
  gg <- gg + theme(axis.title.x = element_blank(),
                   axis.text.x = element_blank()
                   #plot.title = element_text(size = 10)
  )
  gg <- gg + geom_point()
  if(graph_on){print(gg)}
  return(gg)
}
#=========================================================
plot_frontier_and_budget <- function(env_plot_frontier,
                                     env_plot_budgetShares,
                                     graph_on = T){
  #-------------------------------------------
  # Frontier plot
  #if(is.null(fig_title)){fig_title <- "Optimal Portfolio Frontier"}
  #df_plot <- df_frontier[, c("Risk (variance)", "Return target")]
  df_frontier <- env_plot_frontier[["df_frontier"]]
  gg_frontier <- plot_frontier(df_frontier, graph_on = F)
  #-------------------------------------------
  # Budget shares plot
  # Note df_w_in should have just the target risk column (i.e. don't also include the backtest risk column). At any rate, the budget risk column should be the same as the one used in the frontier plot.
  df_wStar <- env_plot_budgetShares[["df_wStar"]]
  color_vec <- env_plot_budgetShares[["color_vec"]]
  list_graph_options <- env_plot_budgetShares[["list_graph_options"]]
  group_small <- env_plot_budgetShares[["group_small"]]
  gg_budgetShares <- plot_budgetShares(df_wStar, group_small, color_vec, graph_on = F, list_graph_options)
  #-------------------------------------------
  gg_together <- gg_frontier + gg_budgetShares + plot_layout(ncol = 1)
  if(graph_on){print(gg_together)}
  outlist_gg <- list(gg_frontier, gg_weights, gg_together)
  return(outlist_gg)
}
#=======================================================================
get_optimal_frontier <- function(covmat, mat_nab,
                                 fun_env = NULL){
  #-------------------------------------------
  if(is.null(fun_env)){
    n_points_on_frontier = 50
    Rtarg_limits = c(0.001, 0.3)
    utility_interpretation = F
    backtest_info = NULL
    C_targ = 1
  }else{
    n_points_on_frontier = fun_env[["n_points_on_frontier"]]
    Rtarg_limits <- fun_env[["Rtarg_limits"]]
    utility_interpretation = fun_env[["utility_interpretation"]]
    backtest_info = fun_env[["backtest_info"]]
    C_targ = fun_env[["C_targ"]]
  }
  #-------------------------------------------
  Rtarg_vec <- seq(Rtarg_limits[1], Rtarg_limits[2], length.out = n_points_on_frontier)
  list_wStar <- list()
  list_frontier <- list()
  #-------------------------------------------
  for(i in 1:length(Rtarg_vec)){
    this_Rtarg <- Rtarg_vec[i]
    targ_vec <- c(this_Rtarg, C_targ)
    list_out <- optimize_portfolio(covmat, mat_nab, targ_vec,
                                   backtest_info,
                                   utility_interpretation)
    list_wStar[[i]] <- list_out[[1]]
    list_frontier[[i]] <- list_out[[2]]
  }
  #-------------------------------------------
  df_frontier <- data.frame(do.call(rbind, list_frontier))
  colnames(df_frontier) <- c("Return target",
                             "Risk (variance)",
                             "Risk shadow price",
                             "Budget shadow price",
                             "Return backtest",
                             "Risk backtest")
  df_wStar <- data.frame(df_frontier$`Risk (variance)`, t(do.call(cbind, list_wStar)))
  varNames_ordered <- row.names(mat_nab)
  colnames(df_wStar) <- c("Risk (variance)", varNames_ordered)
  #--------------------------------------
  list_out <- list(df_wStar, df_frontier)
  return(list_out)
}
#=======================================================================
#=======================================================================
# End function definitions
#=======================================================================
#=======================================================================
# Conventional risk-reward frontier
n_items <- ncol(mat_pctDiff_train_mv)
C_targ <- 1
nab_C <- rep(1, n_items)
utility_interpretation <- F
#--------------------------------------------------------------
# Covariance matrix
covmat_train <- cov(mat_pctDiff_train_mv)
covmat_test <- cov(mat_pctDiff_test_mv)
#--------------------------------------------------------------
# Expected returns vector
#nab_pctRet_train <- apply(mat_pctDiff_train[ind_equal_test, ], 2, function(x) prod(1 + x)) - 1
nab_pctRet_train <- apply(10^-2 * mat_pctDiff_train_mv, 2, function(x) prod(1 + x)) - 1
nab_pctRet_test <- apply(10^-2 * mat_pctDiff_test_mv, 2, function(x) prod(1 + x)) - 1
#--------------------------------------------------------------
mat_nab <- cbind(nab_pctRet_train, nab_C)
n_points_on_frontier <- 50
Rtarg_limits <- c(0.1, 0.3)
backtest_info <- list()
backtest_info[["nab_pctRet_test"]] <- nab_pctRet_test
backtest_info[["covmat_test"]] <- covmat_test
#--------------------------------------------------------------
fun_env_getOptFront <- list()
fun_env_getOptFront[["n_points_on_frontier"]] <- n_points_on_frontier
fun_env_getOptFront[["Rtarg_limits"]] <- Rtarg_limits
fun_env_getOptFront[["utility_interpretation"]] <- utility_interpretation
fun_env_getOptFront[["backtest_info"]] <- backtest_info
fun_env_getOptFront[["C_targ"]] <- C_targ
#--------------------------------------------------------------
list_out <- get_optimal_frontier(covmat_train, mat_nab,
                                 fun_env = fun_env_getOptFront)
df_wStar <- list_out[[1]]
df_frontier <- list_out[[2]]
#--------------------------------------------------------------
gg_frontier <- plot_frontier(df_frontier, graph_on = F)
#--------------------------------------------------------------
# df_wStar_agg <- df_wStar %>% gather_("Item", "Budget Share", colnames(df_wStar)[-1])
# df_wStar_agg <- merge(df_wStar_agg, df_match_group, by = "Item")
# df_wStar_agg <- df_wStar_agg %>% group_by(`Risk (variance)`, Group) %>% summarise(`Budget Share` = sum(`Budget Share`)) %>% spread(Group, `Budget Share`)
#gg_budget <- plot_budgetShares(df_wStar_agg, group_small = NULL, color_vec = group_colors, graph_on = F, list_graph_options = NULL)
n_items <- ncol(df_wStar) - 1
bag_of_colors <- randomcoloR::distinctColorPalette(k = 5 * n_items)
color_vec_mv_eg <- sample(bag_of_colors, n_items)
gg_budget <- plot_budgetShares(df_wStar, color_vec = color_vec_mv_eg, graph_on = F, list_graph_options = NULL)
#--------------------------------------------------------------
gg_frontier + gg_budget + plot_layout(ncol = 1)
#https://gotellilab.github.io/GotelliLabMeetingHacks/NickGotelli/ggplotPatchwork.html
#--------------------------------------------------------------
df_frontier_conv <- df_frontier
df_wStar_conv <- df_wStar

```


## Dealing with [SCALABILITY AND] negative budget shares

The optimal budget allocation in the bottom of Figure \ref{fig:mvConv} includes some shares that are negative. As discussed in the introduction, this is unacceptable in the AR4D context. In order to force optimal budget shares, I replace the revenue $R$ and $C$ functions with utility functions $U_R$ and $U_C$, where

\begin{equation}
U_R = \boldsymbol{\mu} \cdot \ln{\mathbf{w}}
\end{equation}

and 

\begin{equation}
U_C = \mathbf{1} \cdot \ln{\mathbf{w}}
\end{equation}

Moreover, the budget $\bar{C}$ is replaced with the utility budget $\bar{U}_C$.

This is tantamount to substituting $\ln \mathbf{w}$ wherever $\mathbf{w}$ appears in the equations of the previous section.

I derive the utility functions $U_R$ and $U_C$ from the observation, going back perhaps as far as Aristotle [@kauder1953genesis], and certainly as far back as Bernoulli [-@bernoulli1954exposition], that

>"in the absence of the unusual, the utility resulting from any small increase in wealth will be inversely proportionate to the quantity of goods previously possessed."

This time honored observation can be formalized as follows:

\begin{equation}
\frac{\partial U}{\partial w_i} \sim \frac{d \ln w_i}{dw_i}
\label{eq:uDefine}
\end{equation}

Note that multiplying through by $U / w_i$ gives an interesting alternative expression,

\begin{equation}
\frac{\partial \ln U}{\partial \ln w_i} \sim \frac{d \ln U}{dU}
\end{equation}

so that Bernoulli's statement is mathematically equivalent to saying that

>"in the absence of the unusual, the _percentage increase in_ utility resulting from _a one percent_ increase in wealth will be inversely proportionate to the _utility_ of goods previously possessed."

Equation \ref{eq:uDefine} can be used to arrive at an expression for $U$ as follows:

\begin{equation}
U(\mathbf{w}; \boldsymbol{\alpha}, k) = \int \nabla_{\mathbf{w}} U \cdot d\mathbf{w} = \boldsymbol{\alpha} \cdot \ln{\mathbf{w}} + k
\end{equation}

Since utility is not defined directly, but rather on the basis of the marginal relation in equation \ref{eq:uDefine}, then it is defined only up to the arbitrary constants $\boldsymbol{\alpha}$ and $k$. (In mathspeak, it is said to be defined up to an affine transformation.) However, arbitrary constants are often fixed by the specific context of the problem. In the present setting, the constants $\boldsymbol{\alpha}$ correspond to the vectors $\boldsymbol{\mu}$ and $\mathbf{1}$ in the $U_R$ and $U_C$ functions, respectively, leaving just $k$ unfixed. $U_R$ and $U_C$ are thereby defined up to translation. This means that 

\begin{equation}
U_R(\mathbf{w}; \boldsymbol{\mu}, k_1) = U_R(\mathbf{w}; \boldsymbol{\mu}, k_2)
\label{eq:uTrans}
\end{equation}

for two different arbitrary constants $k_1$ and $k_2$. This is very important because it allows for arbitrary enforcement of the budget constraint. The substitution $\ln \mathbf{w}$ for $\mathbf{w}$ generally causes the budget shares to no longer add up to the constraint $\bar{C}$. But equation \ref{eq:uTrans} says that the weights can be divided by the constant $\bar{C} / \mathbf{1} \cdot \mathbf{w}$ so as to restore the condition $\mathbf{1} \cdot \mathbf{w} = \bar{C}$, and that $U_R$ is invariant under this operation. It is tantamount to setting $k_1 = 0$ and $k_2 = \ln \bar{C} - \ln (\mathbf{1} \cdot \mathbf{w})$.

\begin{equation}
U_R(\mathbf{w}; \boldsymbol{\mu}) = U_R(\mathbf{w}; \boldsymbol{\mu}, \ln \bar{C} - \ln (\mathbf{1} \cdot \mathbf{w}))
\end{equation}

The optimal frontier and budget shares using the utility approach presented in the methods section are plotted on the left in Figure \ref{fig:mvUtility}. Note how the optimal budget allocations increase with risk tolerance for some asset groups while dereasing for others. Specifically, the optimal budget allocated to the U.S. Sectors, Energy, and Mineral assets increases with risk tolerance, while optimal budget allocated to the other asset groups decreases with risk tolerance.


\pagebreak

```{r, fig.show='hold', fig.width=5, fig.height=5, fig.align='center', fig.cap="\\label{fig:mvUtility}Optimal frontier and budget shares, replacing returns and cost with utility functions.", echo = FALSE}

# # Get backtest data
# fun_env = list(eigenvalue_density_plot = F,
#                pca_var_plot = F,
#                pca_ind_plot = F,
#                group_info,
#                quietly = T)
# list_out <- signals_from_noise(mat_pctDiff_test, fig_title_eigDens = NULL, fun_env)
# # list_out <- list(mat_loads_sig, mat_Lrot_sig, mat_loads, mat_Lrot, mat_inData_sig, eigvals_sig, mat_eigvecs_sig, eigvals, mat_eigvecs)
# mat_loads_sig_test <- list_out[[1]]
# mat_Lrot_sig_test <- list_out[[2]]
# mat_pctDiff_sig_test <- list_out[[5]]
#=======================================================================
# Utility risk-reward frontier
utility_interpretation <- T
#--------------------------------------------------------------
Rtarg_limits <- c(0.1, 0.8)#c(0.1, 5)
#--------------------------------------------------------------
fun_env_getOptFront[["Rtarg_limits"]] <- Rtarg_limits
fun_env_getOptFront[["utility_interpretation"]] <- utility_interpretation
#--------------------------------------------------------------
list_out <- get_optimal_frontier(covmat_train, mat_nab,
                                 fun_env = fun_env_getOptFront)
df_wStar <- list_out[[1]]
df_frontier <- list_out[[2]]
#--------------------------------------------------------------
gg_frontier <- plot_frontier(df_frontier, graph_on = F)
#--------------------------------------------------------------
# df_wStar_agg <- df_wStar %>% gather_("Item", "Budget Share", colnames(df_wStar)[-1])
# df_wStar_agg <- merge(df_wStar_agg, df_match_group, by = "Item")
# df_wStar_agg <- df_wStar_agg %>% group_by(`Risk (variance)`, Group) %>% summarise(`Budget Share` = sum(`Budget Share`)) %>% spread(Group, `Budget Share`)
# gg_budget <- plot_budgetShares(df_wStar_agg, group_small = NULL, color_vec = group_colors, graph_on = F, list_graph_options = NULL)
gg_budget <- plot_budgetShares(df_wStar, group_small = NULL, color_vec = color_vec_mv_eg, graph_on = F, list_graph_options = NULL)
#--------------------------------------------------------------
gg_frontier + gg_budget + plot_layout(ncol = 1)
#--------------------------------------------------------------
df_frontier_U <- df_frontier
df_wStar_U <- df_wStar


kappa(cov(mat_pctDiff_train[, c(colnames(mat_pctDiff_train) %in% spy_sector_detail)]))

```

<!-- \pagebreak -->
Compared to the conventional frontier in Figure \ref{fig:mvConv}, the frontier in Figure \ref{fig:mvUtility} looks inferior. However, the high returns to the conventional portfolio are mostly attributable to unrealistic leveraging of the portfolio. That is to say, a sum of the absolute values of the conventional budget shares displayed in Figure \ref{fig:mvConv} would indicate that the investor must be prepared to borrow sums in excess of 2 to 4 times their budget in order to take up the corresponding positions on the frontier. To be fair, then, a comparison between the conventional approach and the utility approach must be made on the basis of return on investment (equation \ref{eq:roi_def}), rather than return alone.

\begin{equation}
\frac{R-C}{C}
\label{eq:roi_def}
\end{equation}

This comparison is presented in Figure \ref{fig:mv_roiCompare}. When viewed in this way, the efficient frontier generated by the utility approach is far superior to the conventional frontier.

<!-- [in practice, real portfolio returns are often far lower, and far riskier, than the optimal frontier would suggest. This is evident in the portfolio backtests in Figure \ref{fig:...}.   This is especially true when the covariance or correlation matrix used in the analysis is ill conditioned.  frontiers often diverge from reality.   the frontier is only as accurate as the data it is based on. The backtests are shown in Figure \ref{fig:mvBacktest} ... Also to be expected that the conventional frontier backtest is higher than the utility frontier backtest simply because negative weights require that the total investment is greater than that of the portfolio with only positive weights. If we look not just at the return, but at the return on investment, then the utility weights approach performs better than the conventional approach. Keep in mind this is just one scenario. you'd have to look at many before concluding definitively one way or the other.] -->


```{r, fig.show='hold', fig.width=5, fig.height=3, fig.align='center', fig.cap="\\label{fig:mv_roiCompare}Backtest of optimal frontiers. \\textit{Left}The conventional approach, \\textit{Right}The utility approach.", echo = FALSE}

df_plot_conv <- df_frontier_conv
df_plot_conv$Tot_investment <- rowSums(abs(df_wStar_conv[, -1]))
df_plot_conv$`Return on investment` <- df_plot_conv$`Return target` / df_plot_conv$Tot_investment
df_plot_conv$`Budget shares` <- "Conventional\n(can be negative)"
df_plot_U <- df_frontier_U
df_plot_U$Tot_investment <- rowSums(abs(df_wStar_U[, -1]))
df_plot_U$`Return on investment` <- df_plot_U$`Return target` / df_plot_U$Tot_investment
df_plot_U$`Budget shares` <- "Utility\n(positive only)"
df_plot <- rbind(df_plot_conv, df_plot_U)
these_colors <- c("black", "blue")
gg <- ggplot(df_plot, aes(x = `Risk (variance)`, y = `Return on investment`, group = `Budget shares`, color = `Budget shares`))
gg <- gg + geom_point()
gg <- gg + scale_color_manual(values = these_colors)
gg

#--------------------------------------------------------

#=======================================================================
# plot_frontier_wBacktest <- function(df_frontier, list_graph_options, graph_on = T){
#   #-------------------------------------------
#   if(is.null(list_graph_options)){
#     legend_position <- list_graph_options[["legend_position"]]
#     fig_title <- list_graph_options[["fig_title"]]
#   }else{
#     legend_position <- "right"
#     fig_title <- NULL
#   }
#   n_points_on_frontier <- nrow(df_frontier)
#   color_vec <- c("#56B4E9", "black")
#   #-------------------------------------------
#   #if(is.null(fig_title)){fig_title <- "Optimal Portfolio Frontier"}
#   df_plot1 <- df_frontier[, c("Risk (variance)", "Return target")]
#   df_plot2 <- df_frontier[, c("Risk backtest", "Return backtest")]
#   df_plot1$Type <- "Optimal frontier"
#   df_plot2$Type <- "Backtest of optimal frontier"
#   colnames(df_plot1)[2] <- "Return"
#   colnames(df_plot2)[1:2] <- c("Risk (variance)", "Return")
#   df_plot <- as.data.frame(do.call(rbind, list(df_plot1, df_plot2)))
#   gg <- ggplot(df_plot, aes(x = `Risk (variance)`, y = `Return`, group = Type, color = Type))
#   gg <- gg + geom_point()
#   gg <- gg + scale_color_manual(values = color_vec)
#   if(!is.null(fig_title)){
#       gg <- gg + labs(title = fig_title)
#       gg <- gg + theme(plot.title = element_text(face = "bold", size = 9))
#   }
#   #gg <- gg + theme(plot.title = element_text(size = 10))
#   gg <- gg + theme(legend.title = element_blank(),
#                    legend.position = legend_position)
#   gg <- gg + coord_cartesian(xlim = c(0, max(df_plot$`Risk (variance)`)))
#   if(graph_on){print(gg)}
#   return(gg)
# }
# #=======================================================================
# df_plot_conv_mod <- df_plot_conv[, c("Risk (variance)", "Return target")]
# df_plot_conv_btest <- df_plot_conv[, c("Risk backtest", "Return backtest")]
# df_plot_conv_mod$`Return on investment` <- df_plot_conv_mod$`Return target` / df_plot_conv$Tot_investment
# df_plot_conv_btest$`Return on investment` <- df_plot_conv_btest$`Return backtest` / df_plot_conv$Tot_investment
# df_plot_conv_mod$Type <- "Optimal\nsolution"
# df_plot_conv_btest$Type <- "Backtest"
# colnames(df_plot_conv_mod) <- c("Risk", "Return", "Return on investment", "Type")
# colnames(df_plot_conv_btest) <- c("Risk", "Return", "Return on investment", "Type")
# #--------------------------------------------------------
# df_plot <- rbind(df_plot_conv_mod, df_plot_conv_btest)
# gg <- ggplot(df_plot, aes(x = Risk, y = Return, group = Type, color = Type))
# gg <- gg + geom_point()
# gg <- gg + labs(title = "Conventional budget shares\n(can be negative)")
# gg <- gg + theme(legend.position = "none",
#                  plot.title = element_text(face = "bold", size = 10))
# gg_conv_backtest <- gg + scale_color_manual(values = these_colors)
# # ROI basis
# gg <- ggplot(df_plot, aes(x = Risk, y = `Return on investment`, group = Type, color = Type))
# gg <- gg + geom_point()
# gg <- gg + labs(title = "Conventional budget shares\n(can be negative)")
# gg <- gg + theme(legend.position = "none",
#                  plot.title = element_text(face = "bold", size = 10))
# gg_conv_backtest_roi <- gg + scale_color_manual(values = these_colors)
# #--------------------------------------------------------
# df_plot_U_mod <- df_plot_U[, c("Risk (variance)", "Return target")]
# df_plot_U_btest <- df_plot_U[, c("Risk backtest", "Return backtest")]
# df_plot_U_mod$Type <- "Optimal\nsolution"
# df_plot_U_btest$Type <- "Backtest"
# colnames(df_plot_U_mod) <- c("Risk", "Return", "Type")
# colnames(df_plot_U_btest) <- c("Risk", "Return", "Type")
# #--------------------------------------------------------
# df_plot <- rbind(df_plot_U_mod, df_plot_U_btest)
# gg <- ggplot(df_plot, aes(x = Risk, y = Return, group = Type, color = Type))
# gg <- gg + geom_point()
# gg <- gg + labs(title = "Utility budget shares\n(positive only)")
# gg <- gg + theme(legend.title = element_blank(),
#                  axis.title.y = element_blank(),
#                  plot.title = element_text(face = "bold", size = 10))
# gg_U_backtest <- gg + scale_color_manual(values = these_colors)
# #--------------------------------------------------------
# gg_conv_backtest_roi + gg_U_backtest + plot_layout(ncol = 2)
# #--------------------------------------------------------




# gg <- ggplot()
# gg <- gg + geom_point(data = df_plot_conv, aes(x = `Risk backtest`, y = `Return on investment`))
# gg <- gg + geom_point(data = df_plot_U, aes(x = `Risk backtest`, y = `Return backtest`), color = "blue")
# gg
# 
# 
# 
# 
# gg_backTest_conv <- plot_frontier_wBacktest(df_frontier_conv, graph_on = T)
# gg_backTest_U <- plot_frontier_wBacktest(df_frontier_U, graph_on = T)
# gg_backTest_conv + gg_backTest_U + plot_layout(ncol = 2)

```



# Reverse engineering the covariance matrix

In principlal components analysis (PCA), a centered dataset $X$, with potentially many variables, is distilled into a smaller dataset $S$ of just a few key variables capturing the main tendencies and structure in the data. This is accomplished by post-multiplying the original data $X$ by a select number of the eigenvectors of its covariance matrix $\tilde{P}$.^[Also referred to as the matrix of right singular vectors.] ...[The number of "key variables" into which the dataset $X$ is distilled is equal to the number of eigenvectors retained from the full set of eigenvectors $P$.]

$$
\begin{equation}
S = X\tilde{P}
\end{equation}
$$

The columns of the distilled matrix $S$ are referred to as the principal components (PC), or the PC scores, or the factor scores. When dealing with noisy time series, as in this paper, the columns of $S$ might just as well be reffered to as the "signals", in the sense that they are signals extracted from noise.

Because of how they are defined in equation \ref{eq:sigDefin}, the signals are uncorrelated with each other, and their variances are the eigenvalues from $K$.[  covariance matrix ($\tilde{\Sigma}$) is just the diagonal matrix of eigenvalues ($U$). ]

\begin{equation}
\Sigma_{SS} = \frac{1}{n-1} S'S = \frac{1}{n-1}P'X'XP \\
=P'KP=P'PU P'P = U
\label{eq:Svariance}
\end{equation}

Concrete meaning can be attributed to the signals in terms of how correlated they are with the observed variables in $X$. The correlation of $X$ with $S$ is derived by first finding the covariance matrix of $X$ with $S$.

$$
\begin{equation}
\Sigma_{XS} = \frac{1}{n-1}X'S = \frac{1}{n-1}X'XP \\
= KP = PUP'P = PU
\end{equation}
$$

The correlation matrix $K_{XS}$ then follows as

$$
\begin{equation}
K_{XS} = D(\mathbf{\sigma}_X)^{-1} \Sigma_{XS} D(\mathbf{\sigma}_S)^{-1} \\
D(\mathbf{\sigma}_X)^{-1} PU D(\mathbf{\sigma}_S)^{-1}
\end{equation}
$$

But $X$ is scaled to unit variance, so $D(\mathbf{\sigma}_X)^{-1}$ reduces to an identity matrix. The standard deviations of the signals, meanwhile, are just the square roots of the eigenvalues (as shown in equation \ref{eq:Svariance}). The correlation matrix of variables with signals thus reduces to

$$
\begin{equation}
K_{XS} = \Sigma_{XS} U^{-{1 \over 2}} \\
= PU U^{-{1 \over 2}} = PU^{1 \over 2}
\end{equation}
$$
To illustrate this point, let $X$ equal the centered, scaled financial data used in the portfolio optimization example above. The first few columns of the correlations of $X$ with $S$ are plotted as barcharts in Figure \ref{fig:XScorrBarchart}. [Interpret signals accordingly]. This becomes particularly evident in Figure \ref{fig:signals_and_hiCorr_items}, where the time series of individual financial assets can be seen to hew closely to the signals with which they are most correlated.

The correlations matrix $K_{SX}$ is sometimes referred to as the "loadings" matrix, in the sense that it indicates how much each variable in $X$ loads onto a given signal (or how much each signal loads onto a given variable).^[Although, be careful, many also call $P$ the "loadings".] Following this convention, and to reduce notational clutter, let the covariance matrix $K_{XS}$ be denoted $L$.

## Selecting signals
- The proportion of variance explained.
There remains the question of how many signals to extract...

\begin{equation}
\frac{u_i}{\sum_i u_i}
\end{equation}

The cumulative variance explained is

\begin{equation}
\frac{\sum_{i=1}^k u_i}{\sum_i u_i}
\end{equation}

Can follow a rule such as "we are interested in signals that explain 90% of variation in the data".
Elbow rule ... perils of such rules of thumb have been documented.

The random matrix approach

According to this method, all the signals should be used. In the larger financial data set, only the leading 6 should be used, explaining x% of the total variance. 

## Deriving the correlation matrix from loadings

The correlation matrix can be approximated by the loadings of the retained signals as follows.

\begin{equation}

\hat{K}_{XX} = \tilde{L}\tilde{L}'

\end{equation}

The difference between the data correlation matrix from the portfolio optimization example and the loadings derived correlation matrix is shown in Figure \ref{fig:compareCorMats}. The loadings derived correlation matrix is approximate in the sense that it approximates the data correlation matrix. However, it should be kept in mind that the _accuracy_ of the loadings derived correlation matrix is not necessarily inferior to that of the data correlation matrix. To the extent that the data are contaminated by noise, the loadings derived correlation matrix may even be more accurate, since it is effectively a noise-purged version of the data correlation matrix. A divergence between the two matrices should not automatically be attributed to inferior accuracy in the loadings derived approach.

## Optimization over signals

MV Analysis is, in many ways, still a work in progress [@Michaud...?]. One of the main issues limiting its usefulness in everyday applications is its sensitivity to noisy data, which often results in efficient frontiers that overstate returns and understate risk (as seen in the backtest in Figure \ref{fig:...}). It stands to reason, then, that replacement of the data covariance matrix with a loadings derived covariance matrix could improve performance. Unfortunately, the loadings derived correlation/covariance matrix is not invertible, and thus cannot be used in MV Analysis. However, the portfolio of assets can be replaced with a portfolio of signals. The expected return to each signal can be calculated as the sum of the expected returns to each asset weighted by their loadings onto the given signal.

$$
\begin{equation}

\mathbf{\mu}_s = \mathbf{\mu}' D(\mathbf{\eta})^{-1} L  \:\:\: ;\:\:  \eta = L\mathbf{1}

\end{equation}
$$



This approach outputs the optimal budget allocation to each signal. The optimal allocation to each asset can then be derived from this in proportion to loadings. [L normalized such that sum of cols of L all = 1. this ok because eigenvectors defined up to scaling anyway]

$$
\begin{equation}

L D(\mathbf{\nu})^{-1} \mathbf{w}_{s}^* = \mathbf{w}_{p}^* \:\:\: ;\:\: \nu = L'\mathbf{1}

\end{equation}
$$
The optimal frontier and budget shares for the signals portfolio are displayed on the left side of Figure \ref{fig:sigPortfolio}. A backtest is displayed on the right side of the Figure. The signals portfolio performs well in the backtest, far exceeding the efficient frontier. The budget shares are mapped to the assets in Figure \ref{fig:...}. Here it becomes clear that the excitement is premature. The required budget allocation requires extreme leveraging to obtain these positions on the frontier. In order to force positive budget shares, the utility approach is applied in... Figure \ref{fig:...}


```{r, fig.show='hold', fig.width=5, fig.height=5, fig.align='center', fig.cap="\\label{fig:sigPortfolio}\\textit{(Left) }Optimal frontier and budget shares for the signals portfolio. \\textit{(Right) } Backtest of the frontier."}

P_train <- eigen(cov(mat_pctDiff_train_mv))$vectors
U_train <- diag(eigen(cov(mat_pctDiff_train_mv))$values)
L_train <- P_train %*% sqrt(U_train)
P_test <- eigen(cov(mat_pctDiff_test_mv))$vectors
U_test <- diag(eigen(cov(mat_pctDiff_test_mv))$values)
L_test <- P_test %*% sqrt(U_test)
row.names(L_train) <- colnames(mat_pctDiff_train_mv)
row.names(L_test) <- colnames(mat_pctDiff_train_mv)
#--------------------------------------------------------------
utility_interpretation <- F
#--------------------------------------------------------------
# Covariance matrix
covmat_train <- U_train
covmat_test <- U_test
#--------------------------------------------------------------
# Expected returns vector
nab_pctRet_train <- apply(10^-2 * mat_pctDiff_train_mv, 2, function(x) prod(1 + x)) - 1
nab_pctRet_test <- apply(10^-2 * mat_pctDiff_test_mv, 2, function(x) prod(1 + x)) - 1

nab_pctRet_sigs_train <- as.numeric(t(nab_pctRet_train) %*% diag(1 / rowSums(L_train)) %*% L_train)
nab_pctRet_sigs_test <- as.numeric(t(nab_pctRet_test) %*% diag(1 / rowSums(L_test)) %*% L_test)
names(nab_pctRet_sigs_train) <- paste("Signal", 1:length(nab_pctRet_sigs_train))
names(nab_pctRet_sigs_test) <- paste("Signal", 1:length(nab_pctRet_sigs_test))
#--------------------------------------------------------------
mat_nab <- cbind(nab_pctRet_sigs_train, nab_C)
n_points_on_frontier <- 50
Rtarg_limits <- c(0.1, 0.3)
backtest_info <- list()
backtest_info[["nab_pctRet_test"]] <- nab_pctRet_sigs_test
backtest_info[["covmat_test"]] <- covmat_test
#--------------------------------------------------------------
fun_env_getOptFront <- list()
fun_env_getOptFront[["n_points_on_frontier"]] <- n_points_on_frontier
fun_env_getOptFront[["Rtarg_limits"]] <- Rtarg_limits
fun_env_getOptFront[["utility_interpretation"]] <- utility_interpretation
fun_env_getOptFront[["backtest_info"]] <- backtest_info
fun_env_getOptFront[["C_targ"]] <- C_targ
#--------------------------------------------------------------
list_out <- get_optimal_frontier(covmat_train, mat_nab,
                                 fun_env = fun_env_getOptFront)
df_wStar <- list_out[[1]]
df_frontier <- list_out[[2]]
#--------------------------------------------------------------
gg_frontier <- plot_frontier(df_frontier, graph_on = F)
#--------------------------------------------------------------
n_items <- ncol(df_wStar) - 1
bag_of_colors <- randomcoloR::distinctColorPalette(k = 5 * n_items)
color_vec_mv_eg <- sample(bag_of_colors, n_items)
gg_budget <- plot_budgetShares(df_wStar, color_vec = color_vec_mv_eg, graph_on = F, list_graph_options = NULL)
#--------------------------------------------------------------
df_plot_train <- df_frontier[, c("Risk (variance)", "Return target")]
df_plot_test <- df_frontier[, c("Risk backtest", "Return backtest")]
df_plot_train$Type = "Optimal solution"
df_plot_test$Type = "Backtest"
colnames(df_plot_train)[1:2] <- c("Risk (variance)", "Return")
colnames(df_plot_test)[1:2] <- c("Risk (variance)", "Return")
df_plot <- rbind(df_plot_train, df_plot_test)

gg <- ggplot(df_plot, aes(x = `Risk (variance)`, y = Return, group = Type, color = Type))
gg <- gg + geom_point()
gg <- gg + scale_color_manual(values = c("blue", "black"))
gg <- gg + theme(axis.title.y = element_blank(),
                 legend.title = element_blank(),
                 legend.position = "bottom")
gg_backtest <- gg
#--------------------------------------------------------------
(gg_frontier + gg_budget + plot_layout(ncol = 1)) | gg_backtest
#--------------------------------------------------------------


```

```{rfig.show='hold', fig.width=3, fig.height=3, fig.align='center', fig.cap="\\label{fig:assetWgts} The asset budget shares derived from the signals."}

# Map the signal weights back to the assets
mat_wStar <- t(as.matrix(df_wStar[, -1]))
mat_wStar_assets <- L_train %*% diag(1 / colSums(L_train)) %*% mat_wStar
df_wStar_assets <- data.frame(df_wStar$`Risk (variance)`, t(mat_wStar_assets))
colnames(df_wStar_assets) <- c("Risk (variance)", row.names(L_train))
n_items <- ncol(df_wStar_assets) - 1
bag_of_colors <- randomcoloR::distinctColorPalette(k = 5 * n_items)
color_vec_mv_eg <- sample(bag_of_colors, n_items)
gg_budget <- plot_budgetShares(df_wStar_assets, color_vec = color_vec_mv_eg, graph_on = T, list_graph_options = NULL)

# rowSums(df_wStar[, -1])
# rowSums(df_wStar_assets[, -1])

```

The portfolio optimization is performed using the utility approach to force positive budget shares in Figure \reg{fig:...}.


```{r}
#--------------------------------------------------------------
utility_interpretation <- T
#--------------------------------------------------------------
Rtarg_limits <- c(0.01, 20)
#--------------------------------------------------------------
fun_env_getOptFront[["Rtarg_limits"]] <- Rtarg_limits
fun_env_getOptFront[["utility_interpretation"]] <- utility_interpretation
#--------------------------------------------------------------
list_out <- get_optimal_frontier(covmat_train, mat_nab,
                                 fun_env = fun_env_getOptFront)
df_wStar <- list_out[[1]]
df_frontier <- list_out[[2]]
#--------------------------------------------------------------
gg_frontier <- plot_frontier(df_frontier, graph_on = F)
#--------------------------------------------------------------
# df_wStar_agg <- df_wStar %>% gather_("Item", "Budget Share", colnames(df_wStar)[-1])
# df_wStar_agg <- merge(df_wStar_agg, df_match_group, by = "Item")
# df_wStar_agg <- df_wStar_agg %>% group_by(`Risk (variance)`, Group) %>% summarise(`Budget Share` = sum(`Budget Share`)) %>% spread(Group, `Budget Share`)
#gg_budget <- plot_budgetShares(df_wStar_agg, group_small = NULL, color_vec = group_colors, graph_on = F, list_graph_options = NULL)
n_items <- ncol(df_wStar) - 1
bag_of_colors <- randomcoloR::distinctColorPalette(k = 5 * n_items)
color_vec_mv_eg <- sample(bag_of_colors, n_items)
gg_budget <- plot_budgetShares(df_wStar, color_vec = color_vec_mv_eg, graph_on = F, list_graph_options = NULL)
#--------------------------------------------------------------
df_plot_train <- df_frontier[, c("Risk (variance)", "Return target")]
df_plot_test <- df_frontier[, c("Risk backtest", "Return backtest")]
df_plot_train$Type = "Optimal solution"
df_plot_test$Type = "Backtest"
colnames(df_plot_train)[1:2] <- c("Risk (variance)", "Return")
colnames(df_plot_test)[1:2] <- c("Risk (variance)", "Return")
df_plot <- rbind(df_plot_train, df_plot_test)

gg <- ggplot(df_plot, aes(x = `Risk (variance)`, y = Return, group = Type, color = Type))
gg <- gg + geom_point()
gg <- gg + scale_color_manual(values = c("blue", "black"))
gg <- gg + theme(axis.title.y = element_blank(),
                 legend.title = element_blank(),
                 legend.position = "bottom")
gg_backtest <- gg
#--------------------------------------------------------------
(gg_frontier + gg_budget + plot_layout(ncol = 1)) | gg_backtest


```










## Rotations
- It is often instructive to apply an orthogonal rotation $R$ to the correlation matrix $L$ in order to further clarify the interpretation of signals. Let this rotated matrix be denoted $L_{\circlearrowleft}$.
$$
\begin{equation}
L_{\circlearrowleft} = LR \:\: ; \:\:\: R'R = I
\end{equation}
$$

In Figure \ref{fig:XSCorrBarchart_varimaxRot}, a certain kind of orthogonal rotation called a varimax rotation is applied to $K_{XS}$. [Refined interpretation...maybe do the expanded example with tons of assets. "To offer some idea of the reach of this method, consider an expanded example,..."]

Reverse engineering the covariance matrix from rotated loadings - The 

Note that orthogonal rotations of the loadings do not alter $\Sigma_{XX}$.

\begin{equation}

L_{\circlearrowleft}L_{\circlearrowleft} = LR'RL' = LL'

\end{equation}







Orthogonal rotations of the loadings do not alter the variables covariance matrix (equation \ref{eq:Sigma_invar}). The 

\begin{equation}
\Sigma = L_{\circlearrowleft} L_{\circlearrowleft}' = L R'R L' = L L' = PUP'
\label{eq:Sigma_invar}
\end{equation}


This is true regardless of whether $R$ is a varimax rotation, or some other kind of rotation. The important thing is that $R$ is orthogonal.


$$
\begin{equation}
\tilde{\Sigma} = L_{\circlearrowleft}' L_{\circlearrowleft}
\end{equation}
$$

Note that $\Sigma$ will be the same regardless of $R$. On the other hand, $\tilde{\Sigma}$ changes with $R$.













When interpreting the signals, an orthogonal rotation is sometimes applied to clarify the loadings.


In the example presented here, a special kind of orthogonal rotation called a varimax rotation is used. Varimax rotations are useful for clarifying the interpretation of the loadings matrix.


$$
\begin{equation}
\Sigma = L_{\circlearrowleft} L_{\circlearrowleft}'
\end{equation}
$$

This is true regardless of whether $R$ is a varimax rotation, or some other kind of rotation. The important thing is that $R$ is orthogonal.


$$
\begin{equation}
\tilde{\Sigma} = L_{\circlearrowleft}' L_{\circlearrowleft}
\end{equation}
$$

Note that $\Sigma$ will be the same regardless of $R$. On the other hand, $\tilde{\Sigma}$ changes with $R$.



In Figure \ref{fig:eigDens}, a density plot of the correlation matrix eigenvalues is compared against one of eigenvalues derived from a random matrix. In this plot, it is evident that most eigenvalues are small and cannot be distinguished from noise, but six of them extend beyond the random matrix eigenvalue density plot. These correspond to the eigenvectors that can be meaningfully distinguished from noise.

```{r, fig.show = 'hold', fig.width=5, fig.height=3, fig.align='center', fig.cap="\\label{fig:eigDens}Eigenvalue density plots for the financial data correlation matrix and a\ncorrelation matrix of a random dataset of the same dimensions.", echo=FALSE}

#=======================================================================
# Resources:
# Correlation matrices:
# http://www.sthda.com/english/wiki/correlation-matrix-a-quick-start-guide-to-analyze-format-and-visualize-a-correlation-matrix-using-r-software
# https://cran.r-project.org/web/packages/corrplot/vignettes/corrplot-intro.html
#=======================================================================
# Define functions
#=======================================================================
  # res <- FactoMineR::PCA(mat_pctDiff_in, scale.unit = F, ncp = ncol(mat_pctDiff_in), graph = F)
  # eigvals <- as.data.frame(res$eig)$eigenvalue
  # eigval_max <- max(eigvals)
  # mat_loads <- res$var$coord
  # mat_eigvecs <- mat_loads %*% diag(1 / sqrt(eigvals))

signals_from_noise <- function(mat_pctDiff_in){
  mat_pctDiff_in <- mat_pctDiff_train_mv
out_svd <- svd(scale(mat_pctDiff_in, scale = F))
#out_svd <- svd(scale(mat_pctDiff_in, scale = F))
sing_values <- out_svd$d
n_obs <- nrow(mat_pctDiff_in)
eig_values <- sing_values^2 / n_obs
mat_P <- out_svd$v
if(mean(mat_P[, 1]) < 0){mat_P <- -mat_P}
#covmat <- cor(mat_pctDiff_in)
#eig_values[1:4] / eigen(covmat)$values[1:4]
#t(mat_P) %*% eigen(covmat)$vectors
mat_L <- (mat_P %*% diag(sing_values)) / sqrt(n_obs)
# res <- FactoMineR::PCA(mat_pctDiff_in, scale.unit = F, ncp = ncol(mat_pctDiff_in), graph = F)
# mat_L_FactoMiner <- res$var$coord
# mat_L / mat_L_FactoMiner
  mat_Lrot <- varimax(mat_L)[[1]]
  mat_Lrot <- matrix(as.numeric(mat_Lrot),
                          attributes(mat_Lrot)$dim,
                          dimnames = attributes(mat_Lrot)$dimnames)
  
  mat_Prot <- varimax(mat_P)[[1]]
  mat_Prot <- matrix(as.numeric(mat_Prot),
                          attributes(mat_Prot)$dim,
                          dimnames = attributes(mat_Prot)$dimnames)

n_signals <- 5
mat_Lrot_sig <- mat_Lrot[, 1:n_signals]
mat_P_sig <- mat_P[, 1:n_signals]
mat_S <- mat_pctDiff_in %*% mat_P_sig
#mat_S <- mat_pctDiff_in %*% mat_Lrot[, 1:n_signals]
mat_Lrot_sig <- mat_Lrot[, 1:n_signals]
mat_L_sig <- mat_L[, 1:n_signals]
#(mat_L_sig %*% t(mat_L_sig)) / covmat

list_out <- list(mat_S, mat_Lrot_sig)
return(list_out)
}


#Plot rotated loadings (interpret_loadings)
  df_plot <- data.frame(id = colnames(mat_pctDiff_in), mat_Lrot)
  signal_id <- paste("Signal", c(1:n_signals))
  colnames(df_plot)[2:(n_signals + 1)] <- signal_id
  gathercols <- as.character(signal_id) 
  df_plot <- gather_(df_plot, "Signal", "Loading", gathercols)
  df_plot <- transform(df_plot,
                       Signal = factor(Signal, levels = gathercols))
    gg <- ggplot(df_plot, aes(x = id, y = Loading))
  gg <- gg + geom_bar(stat = "identity", color = "black", position = "dodge")
  gg <- gg + facet_wrap(~ Signal, nrow = 1)
  gg <- gg + theme(axis.text.y = element_text(size = 9),
                   axis.text.x = element_text(size = 9),
                   axis.title.y = element_blank(),
                   axis.title.x = element_text(size = 9),
                   legend.title = element_blank(),
                   legend.text = element_text(size = 9)
                   #plot.caption = element_text(size = 10, hjust = 0)
                   )
  #gg <- gg + labs(caption = fig_title)
  gg <- gg + coord_equal()
  gg <- gg + coord_flip()



  
  load_threshold <- 0.3
  n_display_max <- 5
  
    date_vec <- row.names(mat_pctDiff_in)
  xAxis_labels <- date_vec[seq(1, nrow(mat_pctDiff_in), length.out = 5)]
  list_gg <- list()
  for(i in 1:n_signals){
    this_loadvec <- mat_P_sig[, i]
    #ind_tracks <- which(abs(this_loadvec) >= load_threshold)
    ind_tracks <- which(abs(this_loadvec) == max(abs(this_loadvec)))
    mat_pctDiff_tracks <- mat_pctDiff_in[, ind_tracks]
    #------------
    # n_display <- length(ind_tracks)
    # if(n_display > n_display_max){
    #   n_to_omit <- n_display - n_display_max
    #   random_omission <- sample(1:n_display, n_to_omit)
    #   mat_pctDiff_tracks <- mat_pctDiff_tracks[, -random_omission]
    # }
    #------------
    df_plot_sig <- data.frame(Date = date_vec, Value = mat_S[, i])
    df_plot_tracks <- data.frame(Date = date_vec, mat_pctDiff_tracks)
    if(ncol(df_plot_tracks) > 2){
      gathercols <- colnames(mat_pctDiff_tracks)
      colnames(df_plot_tracks)[-1] <- gathercols
      df_plot_tracks <- df_plot_tracks %>% gather_("ts", "Value", gathercols)
    }else{
      colnames(df_plot_tracks)[-1] <- "Value"
      df_plot_tracks$ts <- colnames(mat_pctDiff_in)[ind_tracks]
    }
    #-----
    gg <- ggplot()
    gg <- gg + geom_line(data = df_plot_sig, aes(x = Date, y = Value, group = 1), color = "grey", lwd = 1.3)
    gg <- gg + geom_line(data = df_plot_tracks, aes(x = Date, y = Value, group = ts, color = ts))
    gg <- gg + scale_x_discrete(breaks = xAxis_labels)
    if(i == n_signals){
      gg <- gg + labs(title = paste("Signal", i), caption = fig_title)
      gg <- gg + theme(#axis.text.x = element_text(angle = 60, hjust = 1),
        axis.title.x = element_blank(),
        axis.title.y = element_blank(),
        legend.title = element_blank(),
        plot.title = element_text(size = 9)
        #plot.caption = element_text(hjust = 0, size = 10)
      )
    }else{
      gg <- gg + labs(title = paste("Signal", i))
      gg <- gg + theme(axis.text.x = element_blank(),
                       axis.title.x = element_blank(),
                       axis.title.y = element_blank(),
                       legend.title = element_blank(),
                       plot.title = element_text(size = 9))
    }
    
    list_gg[[i]] <- gg
    
  }
  
  gg_all <- wrap_plots(list_gg) + plot_layout(ncol = 1)
  print(gg_all)

  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  

# Plot signals (against average )
  facet_ncol <- 1
  ts_avg <- rowMeans(mat_pctDiff_in)
  date_vec <- row.names(mat_pctDiff_in)
  df_plot1 <- data.frame(Date = date_vec, ts_avg)
  df_plot2 <- data.frame(Date = date_vec, mat_S)
  df_plot1$Date <- factor(format(df_plot1$Date, format = "%y-%m-%d%"), ordered = T)
  df_plot2$Date <- factor(format(df_plot2$Date, format = "%y-%m-%d%"), ordered = T)
  xAxis_labels <- df_plot1$Date[seq(1, nrow(df_plot1), length.out = 5)]
  signal_id <- paste("Signal", c(1:n_signals))
  colnames(df_plot2)[2:(n_signals + 1)] <- signal_id
  gathercols <- signal_id
  df_plot2 <- df_plot2 %>% gather_("Signal", "Value", gathercols)
  gg <- ggplot()
  gg <- gg + geom_line(data = df_plot1, aes(x = Date, y = ts_avg, group = 1), color = "orange", lwd = 2)
  gg <- gg + geom_line(data = df_plot2, aes(x = Date, y = Value, group = 1))
  gg <- gg + scale_x_discrete(breaks = xAxis_labels)
  gg <- gg + facet_wrap(~ Signal, ncol = facet_ncol)
  gg <- gg + theme(axis.title.y = element_blank(),
                   axis.title.x = element_blank(),
                   axis.text.x = element_text(size = 9),
                   strip.text = element_text(size = 9)
                   #plot.caption = element_text(size = 10, hjust = 0)
  )
  #gg <- gg + labs(caption = fig_title)
  print(gg)

  
#=======================================================================
interpret_loadings <- function(mat_Lrot_sig, fun_env = NULL){
  #---------------------------------------------------------
  if(is.null(fun_env)){
    group_info = NULL
    signal_names = NULL
    group_colors = NULL
    fig_title = "Each item's contribution to each signal"
  }else{
    group_info = fun_env[[1]]
    signal_names = fun_env[[2]]
    group_colors = fun_env[[3]]
    fig_title = fun_env[[4]]
  }
  #---------------------------------------------------------
  # Handle case where there's just 1 signal
  # (In such cases, mat_Lrot_sig will be of class "numeric")
  if(class(mat_Lrot_sig) == "numeric"){
    n_items <- length(mat_Lrot_sig)
    n_signals <- 1
    varNames_ordered <- names(mat_Lrot_sig)
  }
  if(class(mat_Lrot_sig) == "matrix"){
    n_items <- nrow(mat_Lrot_sig)
    n_signals <- ncol(mat_Lrot_sig)
    varNames_ordered <- row.names(mat_Lrot_sig)
  }
  #------------------------------------------------------------
  # Plot loadings barcharts
  df_plot <- data.frame(id = colnames(mat_pctDiff_in), mat_Lrot_sig)
  #--------------
  # Name the signals, if names provided
  if(is.null(signal_names)){
    signal_id <- paste("Signal", c(1:n_signals))
  }else{
    signal_id <- signal_names
  }
  #--------------
  colnames(df_plot)[2:(n_signals + 1)] <- signal_id
  gathercols <- as.character(signal_id) 
  df_plot <- gather_(df_plot, "Signal", "Loading", gathercols)
  df_plot <- transform(df_plot,
                       Signal = factor(Signal, levels = gathercols))
  #--------------
  # Group the vars if group info is provided
  if(!is.null(group_info)){
    list_groups <- group_info[[1]]
    group_names <- group_info[[2]]
    group_vec <- rep(NA, n_items)
    for(i in 1:length(list_groups)){
      this_group_vec <- list_groups[[i]]
      this_group_name <- group_names[i]
      group_vec[which(varNames_ordered %in% this_group_vec)] <- this_group_name
    }
    #--------------
    df_plot$Type <- factor(group_vec)
    xx <- df_plot$Type
    df_plot$id <- factor(df_plot$id, levels = unique(df_plot$id[order(xx)]))
    gg <- ggplot(df_plot, aes(x = id, y = Loading, fill = Type))
    gg <- gg + scale_fill_manual(values = group_colors)
  }else{
    gg <- ggplot(df_plot, aes(x = id, y = Loading))
  }
  gg <- gg + geom_bar(stat = "identity", color = "black", position = "dodge")
  gg <- gg + facet_wrap(~ Signal, nrow = 1)
  gg <- gg + theme(axis.text.y = element_text(size = 9),
                   axis.text.x = element_text(size = 9),
                   axis.title.y = element_blank(),
                   axis.title.x = element_text(size = 9),
                   legend.title = element_blank(),
                   legend.text = element_text(size = 9)
                   #plot.caption = element_text(size = 10, hjust = 0)
  )
  #gg <- gg + labs(caption = fig_title)
  gg <- gg + coord_equal()
  gg <- gg + coord_flip()
  print(gg)
  
}
#=======================================================================

mat_S <- signals_from_noise(mat_pctDiff_in)




signals_from_noise <- function(mat_pctDiff_in,
                               fun_env = NULL){
  
  if(is.null(fun_env)){
    eigenvalue_density_plot = T
    pca_var_plot = T
    pca_ind_plot = T
    group_info = NULL
    quietly = F
  }else{
    eigenvalue_density_plot = fun_env[[1]]
    pca_var_plot = fun_env[[2]]
    pca_ind_plot = fun_env[[3]]
    group_info = fun_env[[4]]
    quietly = fun_env[[5]]
    
  }
  #---------------------------------------------------------
  # Separate signals from noise
  #---------------------------------------------------------

  # res <- FactoMineR::PCA(mat_pctDiff_in, scale.unit = F, ncp = ncol(mat_pctDiff_in), graph = F)
  # eigvals <- as.data.frame(res$eig)$eigenvalue
  # eigval_max <- max(eigvals)
  # mat_loads <- res$var$coord
  # mat_Lrot <- varimax(mat_loads)[[1]]
  # mat_Lrot <- matrix(as.numeric(mat_Lrot),
  #                         attributes(mat_Lrot)$dim,
  #                         dimnames = attributes(mat_Lrot)$dimnames)
  
  #---------------------------------------------------------
  # Apply random matrix theory () to determine eigenvalue distribution of a 
  # correlation matrix of random data.
  n_items <- ncol(mat_pctDiff_in)
  Q <- n_obs / n_items
  s_sq <- 1 - eigval_max / n_items
  #s_sq <- 1
  eigvals_rand_max <- s_sq * (1 + 1 / Q + 2 / sqrt(Q))
  eigvals_rand_min <- s_sq * (1 + 1 / Q - 2 / sqrt(Q))
  eigvals_rand <- seq(eigvals_rand_min, eigvals_rand_max, length.out = n_items)
  eigvals_rand_density <- Q / (2 * pi * s_sq) * sqrt((eigvals_rand_max - eigvals_rand) * (eigvals_rand - eigvals_rand_min)) / eigvals_rand
  #---------------------------------------------------------
  # Plot eigenvalue density vs. random matrix eigenvalue density
  df_plot_data <- data.frame(Eigenvalue = eig_values, Type = "Data")
  df_plot_rand <- data.frame(Eigenvalue = eigvals_rand, Type = "Random")
  df_plot <- rbind(df_plot_data, df_plot_rand)
  
  if(eigenvalue_density_plot){
    #if(is.null(fig_title_eigDens)){fig_title_eigDens <- "Eigenvalue density"}
    gg <- ggplot(df_plot, aes(x = Eigenvalue, fill = Type))
    gg <- gg + geom_density(alpha = .3)
    gg <- gg + theme(axis.title.y = element_blank(),
                     axis.text.y = element_blank(),
                     axis.title.x = element_text(size = 9),
                     #plot.caption = element_text(size = 9, hjust = 0),
                     legend.title = element_blank(),
                     legend.text = element_text(size = 9))
    #gg <- gg + labs(caption = fig_title_eigDens)
    print(gg)
    
    # gg <- ggplot()
    # gg <- gg + geom_density(data = df_plot, aes(x = Eigenvalues, color = "Correlation Matrix"), lwd = 1.1)
    # gg <- gg + geom_line(data = data.frame(x = eigvals_rand, y = eigvals_rand_density), aes(x = x, y = y, color = "Random matrix"), lwd = 1.1)
    # gg <- gg + scale_colour_manual(name = "density", 
    #                                values = c(`Correlation Matrix` = "blue", `Random matrix` = "magenta"))
  }
  #---------------------------------------------------------
  # Which data eigenvalues can be meaningfully distinguished from noise?
  ind_deviating_from_noise <- which(eig_values > eigvals_rand_max) # (eigvals_rand_max + 5 * 10^-1))
  #---------------------------------------------------------
  # Extract signal loadings matrix from noise
  mat_loads_sig <- mat_loads[, ind_deviating_from_noise]
  eigvals_sig <- eigvals[ind_deviating_from_noise]
  mat_Lrot_sig <- mat_Lrot[, ind_deviating_from_noise]
  #---------------------------------------------------------
  n_signals <- length(eigvals_sig)
  if(!quietly){print(paste("Number of signals: ", n_signals))}
  #---------------------------------------------------------
  # Get dimensionally reduced version of original input data
  mat_eigvecs_sig <- mat_eigvecs[, ind_deviating_from_noise]
  mat_inData_sig <- mat_pctDiff %*% mat_eigvecs_sig
  if(n_signals == 1){
    mat_inData_sig <- mat_inData_sig / eigvals_sig
  }else{
    mat_inData_sig <- mat_inData_sig %*% diag(1 / eigvals_sig)
  }
  #---------------------------------------------------------
  # Set sign of eigenvectors such that they
  # best conform to the input time series
  inData_avg <- rowMeans(mat_pctDiff)
  # if(n_signals == 1){
  #   mse <- mean((mat_inData_sig - inData_avg)^2)
  #   mse_neg <- mean((-mat_inData_sig - inData_avg)^2)
  #   if(mse_neg < mse){
  #     mat_eigvecs <- -mat_eigvecs
  #     mat_inData_sig <- -mat_inData_sig
  #     mat_Lrot_sig <- -mat_Lrot_sig
  #   }
  # }else{
  #   for(i in 1:n_signals){
  #     mse <- mean((mat_inData_sig[, i] - inData_avg)^2)
  #     mse_neg <- mean((-mat_inData_sig[, i] - inData_avg)^2)
  #     if(mse_neg < mse){
  #       mat_eigvecs_sig[, i] <- -mat_eigvecs_sig[, i]
  #       mat_inData_sig[, i] <- -mat_inData_sig[, i]
  #       mat_Lrot_sig[, i] <- -mat_Lrot_sig[, i]
  #     }
  #   }
  #   
  # }
  
  #---------------------------------------------------------
  # PCA cluster plots to examine natural grouping in the data
  #---------------------------------------------------------
  # By variable
  if(pca_var_plot){
    if(n_signals == 1){
      print("Only 1 signal, can't do PCA plots.")
    }else{
      gg <- factoextra::fviz_pca_var(res, habillage = factor(group_vec))
      print(gg)
    }
    
  }
  #---------------------------------------------------------
  # By individual
  if(pca_ind_plot){
    if(n_signals == 1){
      print("Only 1 signal, can't do PCA plots.")
    }else{
      res <- FactoMineR::PCA(t(mat_pctDiff), graph = F)
      gg <- factoextra::fviz_pca_ind(res, habillage = factor(group_vec), addEllipses = T)
      print(gg)
    }
    
  }
  #---------------------------------------------------------
  # Cluster plot using Mclust()
  # mc <- mclust::Mclust(t(mat_pctDiff))
  # summary(mc)
  # View(mc$classification)
  # factoextra::fviz_cluster(mc, frame.type = "norm", geom = "text")
  #---------------------------------------------------------
  
  
  list_out <- list(mat_loads_sig, mat_Lrot_sig, mat_loads, mat_Lrot, mat_inData_sig, eigvals_sig, mat_eigvecs_sig, eigvals, mat_eigvecs)
  return(list_out)
}
#=======================================================================
plot_signals_against_avg <- function(mat_inData_sig, mat_inData,
                                     fig_title = NULL, facet_ncol = 1){
  # (mat_inData = mat_pctDiff)
  #---------------------------------------------------------
  # Dimensionally reduced plot of data (signal plots)
  #---------------------------------------------------------
  n_signals <- ncol(mat_inData_sig)
  #if(is.null(fig_title)){fig_title <- "Signals"}
  #---------------------------------------------------------
  # Plot signal data against average
  inData_avg <- rowMeans(mat_inData)
  date_vec <- row.names(mat_inData)
  df_plot1 <- data.frame(Date = date_vec, inData_avg)
  df_plot2 <- data.frame(Date = date_vec, mat_inData_sig)
  df_plot1$Date <- factor(format(df_plot1$Date, format = "%y-%m-%d%"), ordered = T)
  df_plot2$Date <- factor(format(df_plot2$Date, format = "%y-%m-%d%"), ordered = T)
  xAxis_labels <- df_plot1$Date[seq(1, nrow(df_plot1), length.out = 5)]
  signal_id <- paste("Signal", c(1:n_signals))
  colnames(df_plot2)[2:(n_signals + 1)] <- signal_id
  gathercols <- signal_id
  df_plot2 <- df_plot2 %>% gather_("Signal", "Value", gathercols)
  gg <- ggplot()
  gg <- gg + geom_line(data = df_plot1, aes(x = Date, y = inData_avg, group = 1), color = "orange", lwd = 2)
  gg <- gg + geom_line(data = df_plot2, aes(x = Date, y = Value, group = 1))
  gg <- gg + scale_x_discrete(breaks = xAxis_labels)
  gg <- gg + facet_wrap(~ Signal, ncol = facet_ncol)
  gg <- gg + theme(axis.title.y = element_blank(),
                   axis.title.x = element_blank(),
                   axis.text.x = element_text(size = 9),
                   strip.text = element_text(size = 9)
                   #plot.caption = element_text(size = 10, hjust = 0)
  )
  #gg <- gg + labs(caption = fig_title)
  print(gg)
  
  
}
#=======================================================================
#=======================================================================
# End function definition
#=======================================================================
#=======================================================================



fun_env = list(eigenvalue_density_plot = T,
               pca_var_plot = F,
               pca_ind_plot = F,
               group_info,
               quietly = T)
# this_fig_title <- "Figure 4: Eigenvalue density plots for the financial data correlation matrix and a\ncorrelation matrix of a random dataset of the same dimensions."
this_fig_title <- NULL
list_out <- signals_from_noise(mat_pctDiff_train, fig_title_eigDens = this_fig_title, fun_env)
# list_out <- list(mat_loads_sig, mat_Lrot_sig, mat_loads, mat_Lrot, mat_inData_sig, eigvals_sig, mat_eigvecs_sig, eigvals, mat_eigvecs)
mat_loads_sig_train <- list_out[[1]]
mat_Lrot_sig_train <- list_out[[2]]
mat_pctDiff_sig_train <- list_out[[5]]
mat_eigvecs_sig_train <- list_out[[7]]

```

Six signals are then constructed from these eigenvectors and plotted in Figure \ref{fig:sigs1}. The data set average ($\bar{x}$) is included in these plots as an orienting reference (thick yellow line).

Note how Signal 1 hews closely to the average, while the other signals capture movements in the data placing upward or downward pressure on the average.

```{r, fig.show = 'hold', fig.width=6, fig.height=9, fig.cap="\\label{fig:sigs1}Signals (black lines) plotted against the average (yellow line), train data.", echo=FALSE}

# this_fig_title <- "Figure 5: Signals (black lines) plotted against the average (yellow line), train data"
this_fig_title <- NULL
plot_signals_against_avg(mat_pctDiff_sig_train, mat_pctDiff_train,
                         fig_title = this_fig_title,
                         facet_ncol = 1)

```


But what do these signals represent in concrete terms? Figure \ref{fig:sigs1} already offers insight into Signal 1, but what about the other signals? Do they track closely to particular market sectors?

Turning to the signal loadings, we see that the first signal is highly correlated with a broad range of items, consistent with what is observed in Figure \ref{fig:sigs1}. The second, third, and fourth signals are highly correlated with specific markets (precious metals, forex trading, and US Treasury bonds, respectively). This begins to offer insight into the concrete character of these signals. However, there remains considerable fuzziness in the picture. There is considerable overlap between signals, with many items loading moderately onto several different signals at once. The interpretation of the fifth and sixth signals is particularly blurry.

\pagebreak

```{r, fig.show='hold', fig.width=10, fig.height=7, fig.align='center', fig.cap="\\label{fig:loads}Signal loadings, train data.", echo=FALSE}

# signal_names <- c("US / Emerg. Markets,\nEnergy, Blockchain", "Precious metals", "FOREX", "US Bonds", "Agriculture", "Industrial metals")
signal_names <- NULL
fun_env <- list(group_info, signal_names, group_colors)
#this_fig_title <- "Figure 6: Signal loadings, train data"
this_fig_title <- NULL
interpret_loadings(mat_loads_sig_train, fig_title = this_fig_title, fun_env)

```


```{r, fig.show='hold', fig.width=10, fig.height=7, fig.align='center', fig.cap="\\label{fig:loadsRot}Rotated signal loadings, train data.", echo=FALSE}

#signal_names <- c("US / Emerg. Markets,\nEnergy, Blockchain", "Precious metals", "FOREX", "US Bonds", "Agriculture", "Industrial metals")
signal_names <- NULL
fun_env <- list(group_info, signal_names, group_colors)
interpret_loadings(mat_Lrot_sig_train, fun_env)

```

To further aid in analysis, I apply an orthogonal varimax rotation to the loadings matrix (Figure \ref{fig:loadsRot}). The rotated loadings ($\tilde{L}_{\circlearrowright}$) greatly clarify the picture, removing the overlap between signals. Even Signals 5 and 6 have a clear interpretation now. Signal 5 is highly correlated with agriculture, while Signal 6 tracks industrial metals---in contradistinction to Signal 2, which tracks precious metals.

These loadings vary depending on the time period and size of the time window. However, a plot of the signals against their respective highest loading items suggests that signal correlations with some items are persistent over time (Figure \ref{fig:sigs2}).

```{r, fig.show='hold', fig.width=10, fig.height=12, fig.align='center', fig.cap="\\label{fig:sigs2}Signals plotted against their respective highest loading items, train data.", echo=FALSE}
#=======================================================================
plot_signals_against_associated_items <- function(mat_Lrot_sig,
                                                  mat_pctDiff,
                                                  mat_pctDiff_sig,
                                                  load_threshold = 0.5,
                                                  n_display_max = 5,
                                                  fig_title = NULL){
  #---------------------------------------------------------
  #if(is.null(fig_title)){fig_title = "Signals plotted against their associated portfolio items"}
  #---------------------------------------------------------
  # Handle case where there's just 1 signal
  # (In such cases, mat_Lrot_sig will be of class "numeric")
  if(class(mat_Lrot_sig) == "numeric"){
    n_signals <- 1
  }
  if(class(mat_Lrot_sig) == "matrix"){
    n_signals <- ncol(mat_Lrot_sig)
  }
  #------------------------------------------------------------
  date_vec <- row.names(mat_pctDiff)
  xAxis_labels <- date_vec[seq(1, nrow(mat_pctDiff), length.out = 5)]
  list_gg <- list()
  for(i in 1:n_signals){
    this_loadvec <- mat_Lrot_sig[, i]
    ind_tracks <- which(this_loadvec >= load_threshold)
    #ind_tracks <- which(this_loadvec >= load_threshold | this_loadvec <= -load_threshold)
    if(length(ind_tracks) == 0){
      ind_tracks <- which(this_loadvec == max(this_loadvec))
    }
    mat_pctDiff_tracks <- mat_pctDiff[, ind_tracks]
    #------------
    n_display <- length(ind_tracks)
    if(n_display > n_display_max){
      n_to_omit <- n_display - n_display_max
      random_omission <- sample(1:n_display, n_to_omit)
      mat_pctDiff_tracks <- mat_pctDiff_tracks[, -random_omission]
    }
    #------------
    df_plot_sig <- data.frame(Date = date_vec, Value = mat_pctDiff_sig[, i])
    df_plot_tracks <- data.frame(Date = date_vec, mat_pctDiff_tracks)
    if(ncol(df_plot_tracks) > 2){
      gathercols <- colnames(mat_pctDiff_tracks)
      colnames(df_plot_tracks)[-1] <- gathercols
      df_plot_tracks <- df_plot_tracks %>% gather_("ts", "Value", gathercols)
    }else{
      colnames(df_plot_tracks)[-1] <- "Value"
      df_plot_tracks$ts <- colnames(mat_pctDiff)[ind_tracks]
    }
    #-----
    gg <- ggplot()
    gg <- gg + geom_line(data = df_plot_sig, aes(x = Date, y = Value, group = 1), color = "grey", lwd = 1.3)
    gg <- gg + geom_line(data = df_plot_tracks, aes(x = Date, y = Value, group = ts, color = ts))
    gg <- gg + scale_x_discrete(breaks = xAxis_labels)
    if(i == n_signals){
      gg <- gg + labs(title = paste("Signal", i), caption = fig_title)
      gg <- gg + theme(#axis.text.x = element_text(angle = 60, hjust = 1),
        axis.title.x = element_blank(),
        axis.title.y = element_blank(),
        legend.title = element_blank(),
        plot.title = element_text(size = 9)
        #plot.caption = element_text(hjust = 0, size = 10)
      )
    }else{
      gg <- gg + labs(title = paste("Signal", i))
      gg <- gg + theme(axis.text.x = element_blank(),
                       axis.title.x = element_blank(),
                       axis.title.y = element_blank(),
                       legend.title = element_blank(),
                       plot.title = element_text(size = 9))
    }
    
    list_gg[[i]] <- gg
    
  }
  
  gg_all <- wrap_plots(list_gg) + plot_layout(ncol = 1)
  print(gg_all)
  
  
}
#=======================================================================
#this_fig_title <- "Figure 8: Signals plotted with their respective highest loading items, train data"
this_fig_title <- NULL
# plot_signals_against_associated_items(mat_Lrot_sig_train,
#                                       mat_pctDiff_train,
#                                       mat_pctDiff_sig_train,
#                                       load_threshold = 0.5,
#                                       n_display_max = 4,
#                                       fig_title = this_fig_title)

plot_signals_against_associated_items(mat_Lrot[, 1:n_signals],
                                      mat_pctDiff_in,
                                      mat_S,
                                      load_threshold = 0.5,
                                      n_display_max = 4,
                                      fig_title = this_fig_title)


```


## Toy example

In practice, AR4D risk adjusted portfolio optimization would come after Step 3, either complementing or replacing the ranking of research proposals in order of priority that is slated to occur in Step 4. By this point in the resource allocation workflow, a list of research proposals has been drawn up, and the expected impact of each proposal has been assessed and quantified. This quantification of net impacts is calculated over the dimensions of interest to the stakeholders. For example, if the ... then the net impact is an aggregation of the net benefits in each of these categories. A hypothetical list of such proposals is presented in Figure \ref{fig:ExpPctRet_Examp}.^{The AR4D proposals are loosely grouped into four categories to facilitate interpretation of the gaphics, but there is no strict rule followed, and clearly some overlap, in the grouping. The "Heat Tolerant Beans" proposal has been placed in the "Smallholder Staples" category, but just as well could just have gone under "Climate Smart Agriculture".}

```{r, fig.show = "hold", fig.width = 8, fig.height = 4, fig.align = "center", fig.cap = "\\label{fig:ExpPctRet_Examp}A set of hypothetical AR4D proposals and their expected returns."}

plot_returns_barchart <- function(df_pctRet, group_colors){
    colnames(df_pctRet)[1] <- "Period Return"
  df_plot <- df_pctRet
  gg <- ggplot(df_plot, aes(x = id, y = `Period Return`, fill = Type))
  #gg <- gg + scale_color_brewer(palette = "Dark2")
  gg <- gg + scale_fill_manual(values = group_colors)
  gg <- gg + geom_bar(stat = "identity", color = "black", position = "dodge")
  gg <- gg + labs(title = "Period Return")
  gg <- gg + theme(axis.text.x = element_text(size = 9, angle = 60, hjust = 1),
                   #axis.text.x = element_blank(),
                   axis.title.x = element_blank(),
                   axis.text.y = element_text(size = 9),
                   #axis.title.y = element_text(size = 9),
                   axis.title.y = element_blank(),
                   legend.position = "bottom",
                   legend.title = element_blank(),
                   legend.text = element_text(size = 9),
                   plot.title = element_text(face = "bold", size = 9)
  )
  #  gg <- gg + coord_equal()
  #  gg <- gg + coord_flip()
  #print(gg)
  gg

}


#---------------------------------------------
prop_CSA <- c("Coffee Agroforestry", "Digital Agriculture", "Low Emission\nSilvo-Pastoril")
econGrowth_CSA <- c(0.2, 0.3, -0.3)
econEquality_CSA <- c(0.4, 0.7, 0.3)
envSust_CSA <- c(0.4, 0.5, 0.8)
#---------------------------------------------
prop_socialCap <- c("Micro Finance\nand Insurance", "Multi-stakeholder\nPlatforms", "Market Access")
econGrowth_socialCap <- c(0.2, 0.3, 0.4)
econEquality_socialCap <- c(0.7, 0.75, 0.8)
envSust_socialCap <- c(0.1, 0.1, 0)
#---------------------------------------------
prop_smallHolder <- c("Heat Tolerant Beans", "Cassava for\nBio-ethanol", "Dual Purpose\nSweet Potato", "Dairy Cooperative")
econGrowth_smallHolder <- c(0.3, 0.4, -0.1, 0.1)
econEquality_smallHolder <- c(0.65, 0.8, 0.63, 0.7)
envSust_smallHolder <- c(0.5, 0.5, 0.4, -0.2)
#---------------------------------------------
prop_highYcommod <- c("Super Maize", "Mega Rice", "Hyper Palm", "Uber Cow")
econGrowth_highYcommod <- c(0.7, 0.8, 0.78, 0.75)
econEquality_highYcommod <- c(0.04, 0.02, 0.03, 0.01)
envSust_highYcommod <- c(-0.6, -0.5, -0.6, -0.7)
#---------------------------------------------
df_CSA <- data.frame(Proposal = prop_CSA, `Economic Growth` = econGrowth_CSA, `Economic Equality` = econEquality_CSA, `Environmental Sustainability` = envSust_CSA, Group = "Climate Smart\nAgriculture")

df_socialCap <- data.frame(Proposal = prop_socialCap, `Economic Growth` = econGrowth_socialCap, `Economic Equality` = econEquality_socialCap, `Environmental Sustainability` = envSust_socialCap, Group = "Social Capital")

df_smallHolder <- data.frame(Proposal = prop_smallHolder, `Economic Growth` = econGrowth_smallHolder, `Economic Equality` = econEquality_smallHolder, `Environmental Sustainability` = envSust_smallHolder, Group = "Smallholder\nStaples")

df_highYcommod <- data.frame(Proposal = prop_highYcommod, `Economic Growth` = econGrowth_highYcommod, `Economic Equality` = econEquality_highYcommod, `Environmental Sustainability` = envSust_highYcommod, Group = "High Value\nYield Enhancement")

list_df <- list(df_highYcommod, df_smallHolder, df_CSA, df_socialCap)

df_Lrot <- as.data.frame(do.call(rbind, list_df))
colnames(df_Lrot)[2:4] <- gsub("\\.", " ", colnames(df_Lrot)[2:4])
group_names <- as.character(unique(df_Lrot$Group))
n_groups <- length(group_names)
bag_of_colors <- randomcoloR::distinctColorPalette(k = 2 * n_groups)
group_colors_arb <- sample(bag_of_colors, n_groups)

# Randomly assign an expected pct. return to each AR4D proposal
n_prop <- nrow(df_Lrot)
n_dim <- ncol(df_Lrot) - 2
mu_pctRet <- exp(rnorm(n_prop))
mu_pctRet <- round(5 * mu_pctRet / sum(mu_pctRet), 2)
df_pctRet <- data.frame(mu_pctRet, id = df_Lrot$Proposal, Type = df_Lrot$Group)
# Plot expected returns for each AR4D proposal

group_colors <- group_colors_arb
plot_returns_barchart(df_pctRet, group_colors)

```

To optimize resource allocation across this portfolio of proposals such that risk is minimized, a proposal covariance matrix is required. In section ... I showed that a covariance matrix can be acquired without data if a rotated loadings can be estimated. Such a matrix can be crowdsourced from stakeholders. The crowdsourced matrix will be as accurate as the stakeholders' collective domain knowledge. The first step in crowdsourcing the rotated loadings is elicitation of the dimensions that best describe the evolution of the research proposals within the problem space that is of interest to the stakeholders. Then, the portfolio items (i.e. the AR4D proposals) are mapped to each dimension.

The key dimensions of the portfolio space can be elicited by asking questions such as "What are our key AR4D objectives? And what are the key metrics by which to measure progress towards those objectives?" This can be accomplished in a variety of ways (eg.: online surveys, in-person events, consultation of strategy documents, some combination thereof, etc.). In the AR4D context, there is often pre-existing consensus on these questions, which can be found laid out in strategy documents. The Sustainable Development Goals may be considered a set of dimensions defining a very broad problem space. The key dimensions of a particular AR4D portfolio space could be selected as a subset of these. The number of portfolio dimensions generally must be a small fraction of the number of portfolio items.

In the hypothetical example presented here, the stakeholders have decided that Economic Growth, Income Equality, and Environmental Sustainability are the dimensions that best register the evolution of the proposals under consideration. The next step in eliciting a rotated loadings matrix is to then invite the stakeholders to rate, from $-100$ to $100$, each research proposal in terms of its contribution towards each of these objectives. A rating of 100 means that the research proposal is exactly correlated with that objective; a rating of 0 means that the proposal makes no contribution toward the given objective; and negative ratings mean that the research proposal works against the goal. The result of this exercise for the hypothetical example considered here is presented in Figure \ref{fig:loadsRotExamp}.

<!-- This would be the case, for example, of a yield enhancing variety that requires increased use of chemical inputs that degrade soils, pollute water sources, and pose health risks. Such a technology might contribute toward the economic growth objective, but works against the environmental sustainability objective. Likewise, it is customary in AR4D communities to assume that a tradeoff exists between the objectives of economic growth and economic equality [@Alston].^{Recent empirical studies have cast doubt on this idea [@;@;@].} Such tradeoffs are inherent in any research proposal. It is critical that stakeholders acknowledge them.-->


```{r, fig.show = "hold", fig.width = 8, fig.height = 5, fig.align = "center", fig.cap = "\\label{fig:loadsRotExamp}A hypothetical example of rotated signal loadings elicited from a stakeholder survey."}
df_x <- df_Lrot
df_x$Group <- NULL
prop_col <- df_x$Proposal
df_x$Proposal <- NULL
mat_Lrot <- as.matrix(df_x)
rownames(mat_Lrot) <- prop_col
list_groups <- list(prop_highYcommod, prop_smallHolder, prop_CSA, prop_socialCap)
group_info <- list(list_groups, group_names)

signal_names <- c("Economic\nGrowth", "Economic\nEquality", "Environmental\nSustainability")

fig_title <- NULL
fun_env <- list(group_info, signal_names, group_colors_arb, fig_title)
mat_Lrot_sig <- mat_Lrot
interpret_loadings(mat_Lrot_sig, fun_env)




```


In the formal language of section ..., these ratings are then interpreted as an orthogonal rotation of the loadings $L_{\circlearrowleft}$. More specifically, each rating is interpreted as the respective proposal's correlation with the given objective. By the lemma proven in the last section (equation \ref{eq:}), the dimension covariance can then be calculated as

$$
\begin{equation}
\Sigma = L_{\circlearrowleft} ' L_{\circlearrowleft}
\end{equation}
$$

An approximation of the implicit research proposal covariance matrix can be calculated as

$$
\begin{equation}
\hat{\Sigma} = L_{\circlearrowleft} L_{\circlearrowleft}'
\end{equation}
$$
This offers insight into the implicit risk of each research proposal---the diagonal elements of $\hat{\Sigma}$---and the implicit covariance between research proposals---the off-diagonal elements---which are otherwise unobservable (Figure \ref{fig:covmatProps}). However, $\hat{\Sigma}$ cannot be used in portfolio optimization, since it is not invertible.


```{r, fig.show = "hold", fig.width = 6, fig.height = 4, fig.align = "center", fig.cap = "\\label{fig:covmatProps}Implicit proposal covariance matrix."}

covmat <- mat_Lrot %*% t(mat_Lrot)
n_col <- ncol(covmat)
df_plot <- covmat %>% tbl_df()
these_levels <- colnames(df_plot)
df_plot$Proposal1 <- colnames(df_plot)
gathercols <- colnames(df_plot)[-ncol(df_plot)]
df_plot <- df_plot %>% gather_("Proposal2", "Value", gathercols)

df_plot$Proposal1 <- factor(df_plot$Proposal1, levels = these_levels)
df_plot$Proposal2 <- factor(df_plot$Proposal2, levels = these_levels)

#midpoint <- min(covmat) + (max(covmat) - min(covmat)) / 2
midpoint <- 0
gg <- ggplot(df_plot, aes(Proposal1, Proposal2))
gg <- gg + geom_tile(aes(fill = Value))#, width = 4, height = 4)
gg <- gg + geom_text(aes(label = round(Value, 2)), size = 3)
gg <- gg + theme(axis.text.x = element_text(angle = 60, hjust = 1),
                 axis.title = element_blank(),
                 legend.title = element_blank())
gg <- gg + scale_fill_gradient2(low = "magenta", mid = "khaki", high = "cyan", midpoint)
gg


```

The dimension covariance and correlation matrices are given in Figure \ref{fig:covmatDim}.

```{r, fig.show = "hold", fig.width = 6, fig.height = 4, fig.align = "center", fig.cap = "\\label{fig:covmatDim}Implicit dimensional covariance and correlation matrices."}

covmat <- t(mat_Lrot) %*% mat_Lrot
sd_vec <- diag(sqrt(covmat))
cormat <- diag(1 / sd_vec) %*% covmat %*% diag(1 / sd_vec)
rownames(cormat) <- rownames(covmat)
colnames(cormat) <- colnames(covmat)


n_col <- ncol(covmat)
df_plot <- covmat %>% tbl_df()
these_levels <- colnames(df_plot)
df_plot$V1 <- colnames(df_plot)
gathercols <- colnames(df_plot)[-ncol(df_plot)]
df_plot <- df_plot %>% gather_("V2", "Value", gathercols)

df_plot$V1 <- factor(df_plot$V1, levels = these_levels)
df_plot$V2 <- factor(df_plot$V2, levels = these_levels)

#midpoint <- min(covmat) + (max(covmat) - min(covmat)) / 2
midpoint <- 0
gg <- ggplot(df_plot, aes(V1, V2))
gg <- gg + geom_tile(aes(fill = Value))#, width = 4, height = 4)
gg <- gg + geom_text(aes(label = round(Value, 2)), size = 3)
gg <- gg + theme(axis.text.x = element_text(angle = 60, hjust = 1),
                 axis.title = element_blank(),
                 legend.title = element_blank(),
                 legend.position = "top",
                 plot.title = element_text(size = 10))
gg <- gg + labs(title = "Covariance Matrix")
gg <- gg + scale_fill_gradient2(low = "magenta", mid = "khaki", high = "cyan", midpoint)
gg_covmat <- gg


df_plot <- cormat %>% tbl_df()
these_levels <- colnames(df_plot)
df_plot$V1 <- colnames(df_plot)
gathercols <- colnames(df_plot)[-ncol(df_plot)]
df_plot <- df_plot %>% gather_("V2", "Value", gathercols)
df_plot$V1 <- factor(df_plot$V1, levels = these_levels)
df_plot$V2 <- factor(df_plot$V2, levels = these_levels)

#midpoint <- min(covmat) + (max(covmat) - min(covmat)) / 2
midpoint <- 0
gg <- ggplot(df_plot, aes(V1, V2))
gg <- gg + geom_tile(aes(fill = Value))#, width = 4, height = 4)
gg <- gg + geom_text(aes(label = round(Value, 2)), size = 3)
gg <- gg + theme(axis.text.x = element_text(angle = 60, hjust = 1),
                 axis.title = element_blank(),
                 axis.text.y = element_blank(),
                 legend.title = element_blank(),
                 legend.position = "top",
                 plot.title = element_text(size = 10))
gg <- gg + labs(title = "Correlation Matrix")
gg <- gg + scale_fill_gradient2(low = "magenta", mid = "khaki", high = "cyan", midpoint)
gg_cormat <- gg


gg_covmat | gg_cormat

#kappa(covmat)
#eigen(covmat)$values
#eigen(covmat)$vectors

# R <- eigen(covmat)$vectors
# mat_L <- mat_Lrot %*% t(R)
# 
# mat_Lrot <- varimax(mat_L)[[1]]
# mat_Lrot <- matrix(as.numeric(mat_Lrot), attributes(mat_Lrot)$dim, dimnames=attributes(mat_Lrot)$dimnames)
# 
# interpret_loadings(mat_Lrot, fun_env)


```

When approached in this way, the portfolio optimization problem is dimensionally reduced from fourteen proposals to just three crosscutting signals. Each signal may itself be viewed as a _thematic_ AR4D portfolio, i.e. as a set of investments that is weighted towards a particular development theme (in this case, either Economic Growth, Economic Equality, or Environmental Sustainability).

If the return to proposal $i$ in dimension $j$ can be expressed
$$
\begin{equation}

\mu_{ij} = \frac{\ell_{ij}}{\sum_j \ell_{ij}} \mu_i \:\:;\:\:\:\mu_i = \sum_j \mu_{ij}
\end{equation}

$$
Then the expected return to each signal can be calculated

$$
\begin{equation}

\mu_j = \frac{1}{n} \sum_i \mu_{ij}

\end{equation}
$$
where $n$ is the number of proposals in the portfolio.


```{r, fig.show='hold', fig.width=5, fig.height=4, fig.align='center', fig.cap="\\label{fig:AR4DFrontier}Hypothetical example of the optimal frontier and budget shares in the AR4D context.", echo = FALSE}

sum_Lrows <- as.numeric(mat_Lrot %*% as.matrix(rep(1, n_dim)))
mat_Lrot_normdByRows <- diag(1 / sum_Lrows) %*% mat_Lrot
#rowSums(mat_Lrot_normdByRows)
rownames(mat_Lrot_normdByRows) <- rownames(mat_Lrot)
mat_mu <- diag(mu_pctRet) %*% mat_Lrot_normdByRows
#mat_mu %*% as.matrix(rep(1, n_dim)) - as.matrix(mu_pctRet)
#mu_pctRet_dims <- colSums(mat_mu)
mu_pctRet_dims <- colMeans(mat_mu)
#-------------------------------------------------------------
nab_pctRet <- mu_pctRet_dims
nab_C <- rep(1, n_dim)
mat_nab <- cbind(nab_pctRet, nab_C)

n_points_on_frontier <- 50
fun_env <- list(n_points_on_frontier,
                utility_interpretation = T,
                backtest_info = NULL,
                frontier_and_budget_plot = T,
                group_info = NULL,
                group_colors = NULL,
                group_small = NULL,
                C_targ = 1,
                fig_title = NULL)
AR4D_eg_rTarg_limits <- c(0.08, .6)
list_out <- get_optimal_frontier(covmat, mat_nab,
                                 Rtarg_limits = AR4D_eg_rTarg_limits,
                                 fun_env)

df_wStar <- list_out[[1]]
df_frontier <- list_out[[2]]
outlist_gg <- list_out[[4]]
gg_frontier <- outlist_gg[1]
gg_weights <- outlist_gg[2]

gg_frontier[[1]] + gg_weights[[1]] + plot_layout(ncol = 1)

#(eigen(covmat)$vectors)^2 %*% as.matrix(mu_pctRet_dims)

```

Note how the optimal solution changes with risk tolerance. At lower risk tolerances, the optimal investment focuses on economic growth, followed by environmental sustainability and economic equality. However, as risk tolerance increases, the optimal investment dictates that investments in environmental sustainability be displaced by investments in economic growth and, to a lesser extent, economic equality.

The resource allocation to the individual AR4D proposals within each of these dimensions is calculated as follows. First, consider that the budget share allocated to the $i^{th}$ proposal in the $j^{th}$ dimension, for a given risk tolerance, can be expressed

$$
\begin{equation}

w_{ij} = \frac{\ell_{ij}}{\sum_i \ell_{ij}} w_j \:\:;\:\:\:w_i = \sum_j w_{ij}

\end{equation}
$$
The optimal budget allocation to the $i^{th}$ proposal then follows as

$$
\begin{equation}

w_i = \sum_j w_{ij}

\end{equation}
$$

```{r, fig.show='hold', fig.width=7, fig.height=5, fig.align='center', fig.cap="\\label{fig:propWgts}dfgg.", echo = FALSE}

fun_env <- list(n_points_on_frontier,
                utility_interpretation = F,
                backtest_info = NULL,
                frontier_and_budget_plot = T,
                group_info = NULL,
                group_colors = NULL,
                group_small = NULL,
                C_targ = 1,
                fig_title = NULL)

list_out <- get_optimal_frontier(covmat, mat_nab,
                                 Rtarg_limits = AR4D_eg_rTarg_limits,
                                 fun_env)

df_wStar <- list_out[[1]]
#rowSums(df_wStar[, -1])

mat_wStar <- as.matrix(df_wStar)
#mat_Lrot_normdByCols <- mat_Lrot %*% diag(1 / colSums(mat_Lrot))
#colSums(mat_Lrot_normdByCols)
mat_wStar_prop <- mat_Lrot %*% t(mat_wStar[, -1])
#colSums(mat_wStar_prop)
mat_wStar_prop <- mat_wStar_prop %*% diag(1 / colSums(mat_wStar_prop))
mat_wStar_prop <- exp(mat_wStar_prop)
mat_wStar_prop <- mat_wStar_prop %*% diag(1 / colSums(mat_wStar_prop))
# mat_wStar_prop <- mat_wStar_prop %*% diag(1 / colSums(mat_wStar_prop))
#colSums(mat_wStar_prop)
mat_wStar_prop <- cbind(df_wStar$`Risk (variance)`, t(mat_wStar_prop))
colnames(mat_wStar_prop) <- c(colnames(df_wStar)[1], rownames(mat_Lrot_normd))
df_wStar_prop <- as.data.frame(mat_wStar_prop)

gg_wgts <- plot_budgetShares(df_wStar_prop, n_points_on_frontier, graph_on = T, group_small = NULL, group_info = group_info, group_colors = group_colors_arb, legend_position = "bottom")

list_df <- list(df_highYcommod, df_CSA, df_socialCap, df_smallHolder)
list_gg <- list()
last_i <- length(list_df)
axis_titles <- "off"
Xaxis_numbers_off <- T
for(i in 1:last_i){
  df_this <- list_df[[i]]
  this_group <- unique(df_this$Group)
  ind_these <- c(1, which(colnames(df_wStar_prop) %in% df_this$Proposal))
  df_wStar_prop_thisGroup <- df_wStar_prop[, ind_these]
  if(i == last_i){
    axis_titles <- "x only"
    Xaxis_numbers_off <- F
    }
  gg_wgts_thisGroup <- plot_budgetShares(df_wStar_prop_thisGroup,
                                     n_points_on_frontier, graph_on = F,
                                     group_small = NULL, group_info = NULL,
                                     group_colors = NULL,
                                     legend_position = "right",
                                     fig_title = this_group,
                                     axis_titles,
                                     Xaxis_numbers_off)
  list_gg[[i]] <- gg_wgts_thisGroup
}

# ind_soCap <- c(1, which(colnames(df_wStar_prop) %in% df_socialCap$Proposal))
# df_wStar_prop_soCap <- df_wStar_prop[, ind_soCap]
# gg_wgts_soCap <- plot_budgetShares(df_wStar_prop_soCap, n_points_on_frontier, graph_on = T, group_small = NULL, group_info = NULL, group_colors = NULL, legend_position = "right")

gg_wgts | (list_gg[[1]] / list_gg[[2]] / list_gg[[3]] / list_gg[[4]]) | plot_layout(widths = c(2, 1))

```
# Discussion

* Benefit of reverse engineering: the empirical covariance matrix has a lot of noise in it, and very ill-conditioned for large datasets. Backtest. In the example in Figure ..., the frontier looks good, offering portfolio returns of 20% or more for relatively small risk. However, a backtest of the optimal weights (right side of Figure \ref{fig:mvConv}) indicates that the real return on investment in these optimal portfolios would have been much less, and the risk much higher, than the efficient fontier suggests. [ The dataset is large enough to allow for backtesting of the optimal portfolio. The portfolio is optimized using the first two thirds of the data (the "train" data), and then backtested against the remaining third. ]  [Note also that the frontier here looks much less appealing than the unmodified frontier above. However, a backtest (right side of Figure \ref{fig:mvUtility}) suggests that it is more accurate than the unmodified approach.]

* May not be exactly "accurate", but generates an optimal solution that is consistent with what we think we know about the domain. Moreover, it should kept in mind that data are not infallible sources of information. Data of the sort considered here, in particular, are inherently noisy. Under such circumstances, arguably, it may be preferible to build up the covariance matrix from crowdsourced domain knowledge, in the manner shown above, rather than calculate it from data.

* Can get into interpretation of $\Lambda$ eigenvalues deduced from $L_{\circlearrowleft}$

* Dimensionally reduced from potentially several dozen to just a handful of signals.




























































































Since much of the inaccuracy of the MV frontier is attributable to noise in the data, it stands to reason that the optimal frontier of a portfolio containing only signals---i.e. the significant information in the data, purged of noise---will be more accurate. But is this true? The frontier and budget shares of the signals portfolio are displayed on the left side of in Figure \ref{fig:mvUSig}. The optimal frontier for such a portfolio is unappealing compared to the conventional frontier, but the backtest (right side of Figure \ref{fig:mvUSig}) reveals that the solutions yield a much better performance, at least over this particular dataset. More experimentation is required, of course, before this can be confirmed or rejected as a general rule. Note that a signals portfolio also has the benefit of reducing transaction costs. Were banks or other financial institutions to create products (along the lines of an ETF, for example) that track such signals, investors could then invest in just a handful of signals instead of the potentially several dozen securities that the signals are correlated with.


```{r, fig.show='hold', fig.width=10, fig.height=4, fig.align='center', fig.cap="\\label{fig:mvUSig}\\textit{(Left)} Optimal frontier and budget shares, signals approach (using utility functions). The signals are named in accordance with their respective highest loading items, and numbered in descending order of their respective eigenvalues. \\textit{(Right)} Backtest of the optimal frontier.", echo=F}

#=======================================================================
# Signals correlation matrix risk-reward frontier
n_signals <- ncol(mat_loads_sig_train)
nab_C <- rep(1, n_signals)
#------------------------------------
mat_pctDiff_sig_test <- mat_pctDiff_test %*% mat_eigvecs_sig_train
#------------------------------------
# Correlation matrix
cormat <- cor(mat_pctDiff_sig_train)
cormat_test <- round(cor(mat_pctDiff_sig_test), 7)
# covmat <- cov(mat_pctDiff_sig_train)
# covmat_test <- round(cov(mat_pctDiff_sig_test), 7)
# cormat <- Hmisc::rcorr(mat_pctDiff_sig_train)$r
# cormat_test <- round(Hmisc::rcorr(mat_pctDiff_sig_test)$r, 7)
#------------------------------------
# Expected returns vector
#nab_pctRet_train <- apply(mat_pctDiff_sig_train[ind_equal_test, ], 2, function(x) prod(1 + x)) - 1
nab_pctRet_train <- apply(mat_pctDiff_sig_train, 2, function(x) prod(1 + x)) - 1
nab_pctRet_test <- apply(mat_pctDiff_sig_test, 2, function(x) prod(1 + x)) - 1
#------------------------------------
mat_nab <- cbind(nab_pctRet_train, nab_C)
signal_names <- c("Sig. 1: US / Emerg. Markets,\nEnergy, Blockchain", "Sig. 2: Precious metals", "Sig. 3: FOREX", "Sig. 4: US Bonds", "Sig. 5: Agriculture", "Sig. 6: Industrial metals")
row.names(mat_nab) <- signal_names
#------------------------------------
fun_env <- list(n_points_on_frontier,
                utility_interpretation = T,
                backtest_info = list(nab_pctRet_test, cormat_test),
                frontier_and_budget_plot = T,
                group_info = NULL,
                group_colors = NULL,
                group_small = NULL)

list_out <- get_optimal_frontier(cormat, mat_nab,
                                 Rtarg_limits = c(0.1, 0.5),
                                 fig_title = NULL,
                                 fun_env)

df_frontier <- list_out[[2]]
df_backtest <- list_out[[3]]
outlist_gg <- list_out[[4]]
gg_frontier <- outlist_gg[1]
gg_weights <- outlist_gg[2]
#------------------------------------
gg_backtest <- plot_frontier_wBacktest(df_frontier,
                                       df_backtest,
                                       fig_title = NULL)
#------------------------------------
(gg_frontier[[1]] / gg_weights[[1]] / plot_layout(ncol = 1)) | gg_backtest
#https://gotellilab.github.io/GotelliLabMeetingHacks/NickGotelli/ggplotPatchwork.html
#------------------------------------

```





To build intuition, I first walk through an application of MV Analysis in its native financial context using daily price data downloaded from yahoo finance, including a backtest of the optimal solutions frontier. Then, I move to an application in the AR4D context using yearly farmgate price data for staple crops downloaded from the Food and Agriculture Organization (FAO).
<!-- The FAO price series are not long enough to conduct a backtest of the optimal frontier. [However, I extend the dataset into the future using the International Model for Policy Analysis of Agricultural Commodities and Trade (IMPACT) and conduct "future backtests" over this extension. These backtests are conducted under a variety of assumptions regarding future demographic and economic growth, as well as future emissions scenarios.] -->


## Separating signals from noise, identifying main assets contributing to market movements

MV analysis is, in many ways, still a work in progress, with unresolved methodological issues severely limiting its usefulness even in its native financial context. In particular, covariance matrices estimated from price data tend to contain a lot of noise; and the optimal solutions then tend to diverge considerably from reality as a result of this [@michaud1989markowitz]. In my adaptation below, I propose a new approach to this problem by first dimensionally reducing the portfolio from several dozen assets to just a handful of "signals" constructed from the eigenvectors of the correlation matrix that can be meaningfully distinguished from noise.

The meaningful eigenvectors are separated out from the noisy eigenvectors using an old, but little-utilized technique developed in the study of physical systems. Once signals have been isolated, their meaning is interpreted based on how much the different portfolio items load onto them. A varimax rotation of the loadings matrix turns out to be of great assistance in this task. The interpretation of the signals is then further confirmed and illustrated by constructing the signal time series and then plotting them against their respective highest loading price series. The highest loading price series are seen to track their respective signals closely.

## Reverse engineering the covariance matrix




<!-- [Scraps: -->
<!-- The Consultative Group on International Agricultural Research (CGIAR) is said to have a "long history of good intentions but limited success in developing appropriate approaches for priority setting" [@Birner2016]. An historic effort begun in 2011 to restructure CGIAR financing around Core Research Programs (CRPs) was ad hoc, thereby reinforcing the very institutional inertia it was designed to disrupt [@Birner2016]. ReDonor attempts to establish criteria and methods for a merit-based allocation of ever-scarcer research funds are seen as invasive characterized as "development at the expense of research" [@Birner2016] or even the "Balkanization" of research [@Petsko2011]. "One of the geniuses" of CG centers is said to be their ability to consistently attract "aid funding of long-term research" while keeping "aid professionals from setting research agendas" [@mccalla2014cgiar]. Scientists respond with ultimatums of their own, demanding a return to the days of "more stable funding" and "budget autonomy" [@leeuwis2018reforming]. -->
<!-- “The failure of reforms is attributed to the unwillingness of donors, and the World Bank leadership of the CGIAR, to take on entrenched center interests” [@mccalla2014cgiar]. Interpretation of "price" is different from the financial context. This could reflect 1) higher kcals per unit weight, 2) higher quality kcals, 3) higher demand, or 4) some combination of all of the above. -->

<!-- ] -->

# Method

## Isolating and interpreting signals

Given enough observations, a large matrix of time series data $X$ can be condensed into just a handful of "signals" that capture the majority of the movement in the data. Formally, these signals are defined

\begin{equation}
S = X \tilde{P}
\label{eq:signalEq}
\end{equation}

where $P$ is the matrix of eigenvectors of $X'X$, and $\tilde{P}$ is this same matrix truncated to retain only the eigenvectors containing significant information---i.e. information that can meaningfully be distinguished from noise. In principle components (PC) analysis, these signals are sometimes referred to as the "PC scores".

A number of different rules of thumb are often followed to determing which vectors of $P$ contain information (as opposed to noise) and should be retained. "Keep only the signals that describe 90% of the variation", for example, or "the scree test", or "keep only those with eigenvalues greater than 1". The perils of using these arbitrary cutoff rules has been well documented [@russell2002search]. Here I follow Laloux, Cizeau, Bouchaud, and Potters [-@laloux1999noise] who apply a more rigorous technique developed by physicists in the 1960s [@dehesa1983mathematical; @mehta2004random]. The technique is based on a key theorem of Random Matrix Theory that says that the distribution of the eigenvalues of any random correlation matrix is a function of the dimensions of the underlying matrix. Physicists have used this theorem to identify the important components ("collective modes") in complexly interacting systems by comparing the eigenvalue density plot of their data correlation matrix against that of a random matrix. The eigenvalues of the data correlation matrix that extend beyond the random matrix eigenvalue density can be identified as corresponding to components of the system that can be meaningfully distinguished from noise.

Signal separation effectively reduces the problem of making sense of several dozen complexly interacting indicators to the more mangeable problem of making sense of just a few cross cutting trends. The next step is to interpret these cross cutting trends in concrete terms. This can be achieved by looking at a varimax rotation of the loadings corresponding to each signal. The loadings indicate how correlated each portfolio item is with each signal. Signals can then be characterized in concrete terms based on which items are most strongly associated with their movement.

The signal loadings are defined

\begin{equation}
\tilde{L} = \tilde{P}\tilde{\Lambda}^{1/2}
\end{equation}

where $\Lambda$ is the diagonal matrix of eigenvalues of the correlation matrix of $X$, and where, as before, the squiggly lines on top indicate that the respective matrix is truncated to contain only the columns that can be meaningfully separated from noise.

A varimax rotation is then applied to $\tilde{L}$ to throw any existing structure in data into stark relief.

\begin{equation}
\tilde{L}_{\circlearrowright} = V\tilde{L}
\end{equation}

where $\tilde{L}_{\circlearrowright}$ is the matrix of rotated loadings and $V$ is an orthogonal matrix (the varimax rotation). We are then interested in only the retained rotated loadings $\tilde{L}_{\circlearrowright}$.

The components of the retained rotated loadings can then be interpreted as the magnitude and direction of the influence of each portfolio item over the average movement of the dataset. Loadings with opposite signs indicate a tradeoff, while loadings with the same sign are indicative of synergy. When there is structure in the data, the rotated loadings tend to be thematically organized, such that each loading vector corresponds to a particular aspect of the overall evolution of the system.^[Gopikrishnan, Rosenow, Plerou, and Stanley [-@gopikrishnan2001quantifying] pursued a similar line of inquiry when they looked at the components of the eigenvectors of a financial data correlation matrix. But they did not first apply a rigorous signal separation technique as I have done here. Nor did they apply a varimax rotation to clarify the interpretation.]



\pagebreak

# An example in the AR4D context

In this section, the workflow presented above is applied to the AR4D context using FAO yearly farmgate price^["Value of Agricultural Production" divided by "Production"] data for staple crops in Sub-Saharan Africa for the years 1991-2016. Historical daily returns and period return are presented in Figure \ref{fig:hReturnsFAO}.

There are, of course, many differences between the financial and AR4D contexts that must be kept in mind. First and foremost of these differences is the motivation for investment. By investing in AR4D of key staple crops, the donor aims to increase their quality and production, thereby lowering their price and making them more accessible to consumers and producers. A financial investor, on the other hand, typically has no expectation that their investment will move the price in any way, but would hope that the price goes up. Another difference is in time horizons. AR4D investment time horizons are on the order of 20-40 years, whereas the time horizon of the typical investor is on the order of 5-10 years. Data environments also differ between the two contexts. Financial data are available on a daily, even intradaily, time scale. Completeness becomes an issue when conducting analyses extending back to before 2005 or so, but given the shorter time horizons this is hardly necessary in practice. Agricultural commmodity farmgate price series, on the other hand, go back considerably further, but are typically available only on a yearly scale. Also, a whole new dimension enters into play when moving from finance to AR4D: _geography_. Where does the donor want to see impacts?



<!-- , and a correlation matrix is presented in Figure \ref{fig:cormatFAO}. -->

<!-- There is not enough data to conduct a backtest. However, the historical data is extended into two hypothetical futures using the IMPACT model. Both futures run out to 2050, but under different sets of assumptions regarding future demographic, economic, climatic, and greenhouse gas mitigation trends.  referred to as Shared Socioeconomic Pathways (SSPs). Each future is also run under . Backtests are then conducted over the future projection. -->

<!-- The efficient frontier and budget shares are given in Figure \ref{fig:mvUtilityFAO}. The scenarios are explained in Table ...(SSP2/RCP4.5, SSP3/RCP8.5). Future backtests of the optimal portfolio under three different IMPACT projections (Figure ...) suggests that the frontier is accurate/understates under scenario x but is inaccurate/overstates under scenarios y and z. -->

```{r, echo=F}

#=======================================================================
area_vec <- c("World","Low Income Food Deficit Countries", "Net Food Importing Developing Countries",
              "Least Developed Countries", "Eastern Africa", "Western Africa", "South America",
              "Southern Asia", "Southern Africa", "Middle Africa", "Asia",
              "Sub-Saharan Africa")
SSA_vec <- c("South Africa", "Africa", "Eastern Africa", "Middle Africa", "Southern Africa", "Western Africa")
#------------------
# Have to rename some crops because names in the production and value of production datasets differ somewhat from those in the trade dataset used above.
cereal_vec <- c("Maize", "Wheat", "Sorghum", "Rice, paddy", "Millet")
legumes_oilcrops_vec <- c("Beans, dry", "Cow peas, dry", "Chick peas", "Lentils", "Soybeans", "Groundnuts, with shell")
RnT_vec <- c("Cassava", "Yams", "Potatoes", "Sweet potatoes")
item_vec <- c(cereal_vec, legumes_oilcrops_vec, RnT_vec)
#------------------------------------
list_groups <- list(cereal_vec, legumes_oilcrops_vec, RnT_vec)
group_names <- c("Cereals", "Legumes", "Roots & Tubers")
group_info <- list(list_groups, group_names)
#------------------------------------
# Get total value of ag production data
#rm(df_expVal, df_expVal_raw)
#gc()
df_vap_raw <- read.csv("Value_of_Production_E_All_Data.csv", stringsAsFactors = F)
#colnames(df_vap_raw)
df_vap_raw$Area.Code <- NULL
df_vap_raw$Item.Code <- NULL
df_vap_raw$Element.Code <-NULL
u <- colnames(df_vap_raw)
#colnames(df_vap_raw)
df_vap_raw <- df_vap_raw[, -grep("F", u)]
colnames(df_vap_raw)[5:ncol(df_vap_raw)] <- as.character(c(1961:(1961 + ncol(df_vap_raw) - 5)))
df_vap_raw <- gather(df_vap_raw,Year,Value,`1961`:`2016`)
#------------------
#unique(df_vap_raw$Area)[grep("africa", unique(df_vap_raw$Area), ignore.case = T)]
#unique(df_vap_raw$Item)[grep("potato", unique(df_vap_raw$Item), ignore.case = T)]
#------------------
df_vap <- subset(df_vap_raw, Area %in% area_vec)
df_vap <- subset(df_vap, Item %in% item_vec)
#unique(df_vap_raw$Element)
element_vec <- c("Gross Production Value (current million US$)")
df_vap <- subset(df_vap, Element %in% element_vec)
df_vap <- subset(df_vap, Year > 1990)
df_vap$Unit <- NULL
df_vap$Element <- NULL
colnames(df_vap)[4] <- "Gross Production Value\n(current million USD)"
#------------------------------
# Get production data
df_prod_raw <- read.csv("Production_Crops_E_All_Data.csv", stringsAsFactors = F)
df_prod_raw <- subset(df_prod_raw, Item.Code != 2928)
df_prod_raw$Area.Code <- NULL
df_prod_raw$Item.Code <- NULL
df_prod_raw$Element.Code <-NULL
df_prod_raw$Unit <- NULL
u <- colnames(df_prod_raw)
df_prod_raw <- df_prod_raw[, -grep("F", u)]
last_yr <- (1961 + ncol(df_prod_raw) - 4)
colnames(df_prod_raw)[4:ncol(df_prod_raw)] <- as.character(c(1961:last_yr))
gathercols <- colnames(df_prod_raw)[4:ncol(df_prod_raw)]
df_prod_raw <- gather_(df_prod_raw, "Year", "Value", gathercols)
#------------------------------
#unique(df_prod_raw$Item)[grep("beans", unique(df_prod_raw$Item), ignore.case = T)]
df_prod <- subset(df_prod_raw, Item %in% item_vec)
df_prod <- subset(df_prod, Area %in% area_vec)
df_prod <- subset(df_prod, Element == "Production")
df_prod <- subset(df_prod, Year > 1990)
df_prod$Element <- NULL
colnames(df_prod)[4] <- "Production"
#-----------------------------
# df_prod$Group <- NA
# u <- df_prod$Item
# df_prod$Group[which(u %in% cereal_vec)] <- "Cereals"
# df_prod$Group[which(u %in% legumes_oilcrops_vec)] <- "Legumes"
# df_prod$Group[which(u %in% RnT_vec)] <- "Roots & Tubers"
# df_prod$Group <- factor(df_prod$Group)
#-----------------------------
df <- merge(df_vap, df_prod, by = c("Area", "Year", "Item"))
#-----------------------------
# Get SSA region
df_x <- subset(df, Area %in% SSA_vec)
df_x <- as.data.frame(df_x %>% group_by(Year, Item) %>% summarise_at(c("Gross Production Value\n(current million USD)", "Production"), sum, na.rm = T))
df_x$Area <- "Sub-Saharan Africa"
df_x <- df_x[, colnames(df)]
df <- as.data.frame(rbind(df, df_x))
df$`Gross Prod. Value / MT\n(current USD)` <- 10^6 * df$`Gross Production Value\n(current million USD)` / df$Production
#-----------------------------
# Get grouping info
df$Group <- NA
u <- df$Item
df$Group[which(u %in% cereal_vec)] <- "Cereals"
df$Group[which(u %in% legumes_oilcrops_vec)] <- "Legumes"
df$Group[which(u %in% RnT_vec)] <- "Roots & Tubers"
df$Group <- factor(df$Group)
#
# Create grouping info for plots (required input into functions)
list_groups <- list(cereal_vec, legumes_oilcrops_vec, RnT_vec)
group_names <- c("Cereals", "Legumes", "Roots & Tubers")
group_info <- list(list_groups, group_names)
n_groups <- length(group_names)
#-------------------------------------------

#-----------------------------
# kcalMT_cereals_vec <- c(4.14, )
# df_kcal <- data.frame(Item = item_vec, kcal_per_MT = kcalMT_vec)
#-----------------------------
# VAP Plots
check_on_data <- F
if(check_on_data){
  df_plot <- subset(df, Year == 2016)
  df_plot <- subset(df_plot, Area == "World")
  xx <- df_plot$Group
  df_plot$Item <- factor(df_plot$Item, levels = unique(df_plot$Item[order(xx)]))
  df_plot$Production <- NULL
  df_plot <- df_plot %>% gather_("Element", "Value", colnames(df_plot[5:6]))
  gg <- ggplot(df_plot, aes(x = Item, y = Value, fill = Group))
  gg <- gg + geom_bar(stat = "identity", position = "dodge")
  gg <- gg + facet_wrap(~Element, scales = "free_y")
  gg <- gg + theme(axis.title.x = element_blank(),
                   axis.text.x = element_text(angle = 60, hjust = 1),
                   axis.title.y = element_blank(),
                   legend.title = element_blank())
  #gg <- gg + coord_flip()
  gg
  #-----------------------------
  df_plot <- subset(df, Year == 2016)
  df_plot$Area[grep("Least Developed Countries", df_plot$Area)] <- "Least Developed\nCountries"
  df_plot$Area[grep("Low Income Food Deficit Countries", df_plot$Area)] <- "Low Income\nFood Deficit Countries"
  xx <- df_plot$Group
  df_plot$Item <- factor(df_plot$Item, levels = unique(df_plot$Item[order(xx)]))
  gg <- ggplot(df_plot, aes(x = Item, y = `Gross Prod. Value / MT\n(current USD)`, fill = Group))
  gg <- gg + geom_bar(stat = "identity", position = "dodge")
  gg <- gg + facet_wrap(~Area)
  gg <- gg + theme(axis.title.y = element_blank(),
                   axis.text.y = element_text(size = 7))
  gg <- gg + coord_flip()
  gg
  
  
  df_plot <- subset(df_plot, Area == "World")
  gg <- ggplot(df_plot, aes(x = Item, y = `Gross Prod. Value / MT\n(current USD)`, fill = Group))
  gg <- gg + geom_bar(stat = "identity", position = "dodge")
  #gg <- gg + facet_wrap(~Area)
  gg <- gg + theme(axis.title.y = element_blank(),
                   axis.text.y = element_text(size = 7))
  #gg <- gg + coord_flip()
  gg
  #-----------------------------
  df_plot <- df
  df_plot$Year <- as.integer(df_plot$Year)
  xx <- df_plot$Group
  df_plot$Item <- factor(df_plot$Item, levels = unique(df_plot$Item[order(xx)]))
  gg <- ggplot(df_plot, aes(x = Year, y = `Gross Prod. Value / MT (current USD)`, fill = Item))
  gg <- gg + geom_area(position = "stack")
  gg <- gg + facet_wrap(~Area)
  gg
  df_plot <- as.data.frame(df_plot %>% group_by(Area, Year) %>% mutate(sum_vap = sum(`Gross Prod. Value / MT (current USD)`)))
  df_plot$`Gross Prod. Value / MT (share)` <- df_plot$`Gross Prod. Value / MT (current USD)` / df_plot$sum_vap
  gg <- ggplot(df_plot, aes(x = Year, y = `Gross Prod. Value / MT (share)`, fill = Item))
  gg <- gg + geom_area(position = "stack")
  gg <- gg + facet_wrap(~Area)
  gg
  #-----------------------------
  # Same on a per kcal basis
  #.....
}

```



```{r, fig.show='hold', fig.align='center', fig.width=4, fig.height=5, fig.cap="\\label{fig:hReturnsFAO}Historical daily returns and period return, Sub-Saharan Africa farmgate prices", echo=F}
#=======================================================================
# Examine historical returns, correlation matrix
#-----------------------------
# group_colors <- wesanderson::wes_palette("Moonrise1", n = n_groups, type = "continuous")
group_colors <- RColorBrewer::brewer.pal(n = n_groups, "Dark2")
#-----------------------------
#"Low Income Food Deficit Countries"
#"Net Food Importing Developing Countries"
#"Sub-Saharan Africa"
#"South America"
#"Asia"
#"World"
this_area <- "Sub-Saharan Africa"
period <- c(1991, 2016)
fraction_train <- 1
#-----------------------------
df_in <- subset(df, Area == this_area)
if(this_area == "South America"){df_in <- subset(df_in, !(Item %in% c("Cow peas, dry", "Millet")))}
df_in <- df_in[, c("Item", "Year", "Gross Prod. Value / MT\n(current USD)")]
df_in <- df_in %>% spread(Item, `Gross Prod. Value / MT\n(current USD)`)
if(!is.null(period)){
  ind_ret_sinceYr <- which(df_in$Year == period[1])
  ind_ret_toYr <- which(df_in$Year == period[2])
  df_in <- df_in[ind_ret_sinceYr:ind_ret_toYr, ]
}
#-------------------------------------------
mat <- as.matrix(df_in[, -1])
#-------------------------------------------
# Replace NA with interplation
mat <- na.approx(mat)
#-------------------------------------------
# Get percentage difference
mat_pctDiff <- diff(mat) / mat[-nrow(mat), ]
year_vec <- df_in$Year[-1]
# mat_pctDiff <- scale(mat)
#   year_vec <- df_in$Year
#-------------------------------------------
row.names(mat_pctDiff) <- year_vec
#-------------------------------------------
n_items <- ncol(mat_pctDiff)
#-------------------------------------------
# Separate train from test data
ind_train <- 1:round(nrow(mat_pctDiff) * fraction_train)
ind_test <- setdiff(1:nrow(mat_pctDiff), ind_train)
mat_pctDiff_train <- mat_pctDiff[ind_train, ]
mat_pctDiff_test <- mat_pctDiff[ind_test, ]
if(length(ind_test) == 0){
  mat_pctDiff_test <- NULL
}


historical_returns_and_boxplot(mat_pctDiff_train,
                               mat_pctDiff_test,
                               group_info,
                               group_colors = group_colors)


```


```{r, fig.show='hold', fig.align='center', fig.cap="\\label{fig:cormatFAO}Correlation matrix, Sub-Saharan Africa farmgate prices", echo=F}
#include=FALSE

outlist <- group_fn(mat_pctDiff, group_info, group_colors, reverse_order = F)
cols_ordered_by_group <- outlist[[1]]
group_color_vec <- outlist[[2]]
corr_colorRamp <- colorRampPalette(c("darkred", "white", "blue4"))(50)
#cormat <- Hmisc::rcorr(mat_pctDiff_train[, cols_ordered_by_group])$r
cormat <- cor(mat_pctDiff_train[, cols_ordered_by_group])
#cormat <- cov(mat_pctDiff_train[, cols_ordered_by_group])
# corrplot::corrplot(cormat, is.corr = T, type = "lower", tl.col = group_color_vec, tl.srt = 30,
#                    col = corr_colorRamp)


# gridGraphics::grid.echo()
# p_corplot <- grid::grid.grab()

#grid::grid.draw(gg_corplot)
# cormat <- cor(mat_pctDiff_test[, cols_ordered_by_group])
# #cormat <- cov(mat_pctDiff[, cols_ordered_by_group])
# corrplot::corrplot(cormat, is.corr = T, type = "lower", tl.col = group_color_vec, tl.srt = 30,
#                    col = corr_colorRamp)
# gridGraphics::grid.echo()
# p_corplot_test <- grid::grid.grab()

```

```{r, fig.show="hold", fig.cap="\\label{fig:corrMats}Correlation matrices, train and test data.", fig.width=7, fig.height=9, fig.align='center'}
#gridExtra::grid.arrange(p_corplot, p_corplot_test)

```

In Figure \ref{fig:eigDensFAO}, a density plot of the correlation matrix eigenvalues is compared against one of eigenvalues derived from a random matrix. In this case, only two of them extend beyond the random matrix eigenvalue density plot. Just two of the eigenvectors, then, can be meaningfully distinguished from noise.

The signals are constructed and plotted in Figure \ref{fig:sigs1FAO}. The first signal again hews closely to the average, but not quite as tightly as in the financial context. Turning to the rotated signal loadings in Figure \ref{fig:loadsRotFAO} for more clarity, it appears that the first signal is primarily correlated with movements in the prices of sweet potato, cassava, soybean, and cowpea. Interestingly, the cereals appear to contribute little to Signal 1. Signal 2 appears mostly to reflect movements in lentil and chick pea prices. These observations are then further corroborated by a plot of the signals against their respective highest loading prices (Figure \ref{fig:sigs2FAO}).

```{r, fig.show='hold', fig.width=5, fig.height=3, fig.align='center', fig.cap="\\label{fig:eigDensFAO}Eigenvalue density plots for the farmgate price data correlation matrix and a\ncorrelation matrix of a random dataset of the same dimensions.", echo=F}

# mat_z <- scale(mat)
#   year_vec <- df_in$Year
#   #-------------------------------------------
#   row.names(mat_z) <- year_vec
#   #-------------------------------------------
#   n_items <- ncol(mat)
#   #-------------------------------------------
# # Separate train from test data
#   ind_train <- 1:round(nrow(mat_z) * fraction_train)
#     ind_test <- setdiff(1:nrow(mat_z), ind_train)
#     mat_z_train <- mat_z[ind_train, ]
#     mat_z_test <- mat_z[ind_test, ]
# 
#===========================================
# Extract signals from noise
#-------------------------------------------
fun_env = list(eigenvalue_density_plot = T,
               pca_var_plot = F,
               pca_ind_plot = F,
               group_info,
               quietly = T)
# this_fig_title <- "Figure 17: Eigenvalue density plots for the farmgate price data correlation matrix and a\ncorrelation matrix of a random dataset of the same dimensions."
this_fig_title <- NULL
list_out <- signals_from_noise(mat_pctDiff_train, fig_title_eigDens = this_fig_title, fun_env)
# list_out <- list(mat_loads_sig, mat_Lrot_sig, mat_loads, mat_Lrot, mat_inData_sig, eigvals_sig, mat_eigvecs_sig, eigvals, mat_eigvecs)
mat_loads_sig_train <- list_out[[1]]
mat_Lrot_sig_train <- list_out[[2]]
mat_pctDiff_sig_train <- list_out[[5]]
mat_eigvecs_sig_train <- list_out[[7]]

covmat <- cor(mat_pctDiff_train)
covmat_L <- mat_Lrot_sig_train %*% t(mat_Lrot_sig_train)
View(covmat - covmat_L)
mean((covmat - covmat_L)^2)




```


```{r, fig.show = 'hold', fig.width=4, fig.height=3, fig.align='center', fig.cap="\\label{fig:sigs1FAO}Signals (black lines) plotted against the average (yellow line), Sub-Saharan Africa farmgate prices.", echo=FALSE}

this_fig_title <- NULL
plot_signals_against_avg(mat_pctDiff_sig_train, mat_pctDiff_train,
                         fig_title = this_fig_title,
                         facet_ncol = 1)

```


```{r, fig.show='hold', fig.width=6, fig.height=4, fig.align='center', fig.cap="\\label{fig:loadsRotFAO}Rotated signal loadings, Sub-Saharan Africa farmgate prices."}
signal_names <- NULL
fun_env <- list(group_info, signal_names, group_colors)
#this_fig_title <- "Figure 18: Signal loadings, farmgate price data"
this_fig_title <- NULL
interpret_loadings(mat_Lrot_sig_train, fig_title = this_fig_title, fun_env)
```



```{r, fig.show='hold', fig.width=4, fig.height=3, fig.align='center', fig.cap="\\label{fig:sigs2FAO}Signals, represented by the thicker gray line, plotted against their respective highest loading items, Sub-Saharan Africa farmgate prices.", echo=FALSE}

#this_fig_title <- "Figure 8: Signals plotted with their respective highest loading items, train data"
this_fig_title <- NULL
plot_signals_against_associated_items(mat_Lrot_sig_train,
                                      mat_pctDiff_train,
                                      mat_pctDiff_sig_train,
                                      load_threshold = 0.5,
                                      n_display_max = 4,
                                      fig_title = this_fig_title)

```

\pagebreak

The efficient frontier and budget allocation is displayed in Figure \ref{fig:mvUSigsFAO}. In this analysis, cost and return are replaced by the utility of cost and return, as explained in the methodology section. In this graphic, the solution suggests that, for a wide range of risk tolerances, investments should focus primarily on lentils and sweet potatoes, followed by millet, yams, and beans. Note also that the optimal budget allocation for lentils and sweet potatoes increases with risk tolerance, whereas the portions going to millet, yams, and beans increase as the investor is more risk averse.

The signals portfolio efficient frontier and budget allocation is displayed in Figure  \ref{fig:mvUSigFAO}

```{r, fig.show='hold', fig.width=9, fig.height=5, fig.align='center', fig.cap="\\label{fig:mvUSigsFAO}\textit{(Left)} Optimal frontier and budget shares, Sub-Saharan farmgate price of staple crops. \textit{(Right)} Optimal frontier and budget shares, signals. The signals are named in accordance with their respective highest loading items, and numbered in descending order of their respective eigenvalues.", echo = FALSE}
#=======================================================================
# Conventional risk-reward frontier
n_items <- ncol(mat_pctDiff)
C_targ <- 1
nab_C <- rep(1, n_items)
#------------------------------------
# Correlation matrix
cormat <- cor(mat_pctDiff_train)
#cormat <- round(mat_Lrot_sig %*% t(mat_Lrot_sig), 7)
#cormat <- round(mat_loads_sig %*% t(mat_loads_sig), 7)
# mse <- mean((cor(mat_pctDiff) - cormat)^2)
# mse
#------------------------------------
# Expected returns vector
nab_pctDiff_train <- apply(mat_pctDiff_train, 2, function(x) prod(1 + x)) - 1
#------------------------------------
mat_nab <- cbind(nab_pctDiff_train, nab_C)
n_points_on_frontier <- 50
#------------------------------------
if(!is.null(mat_pctDiff_test)){
  cormat_test <- cor(mat_pctDiff_test)
  nab_pctDiff_test <- apply(mat_pctDiff_test, 2, function(x) prod(1 + x)) - 1
  backtest_info <- list(nab_pctRet_test, cormat_test)
  
}else{
  backtest_info <- NULL
}

#------------------------------------
if(this_area == "South America"){these_Rtarg_limits <- c(1, 12)}
if(this_area == "Sub-Saharan Africa"){these_Rtarg_limits <- c(10, 35)} #c(15, 30)
if(this_area == "Asia"){these_Rtarg_limits <- c(12, 60)}
if(this_area == "Low Income Food Deficit Countries"){these_Rtarg_limits <- c(1, 20)}
if(this_area == "Net Food Importing Developing Countries"){these_Rtarg_limits <- c(1, 20)}
if(this_area == "World"){these_Rtarg_limits <- c(1, 15)}
this_fig_title <- paste("Optimal Portfolio Frontier,", this_area)
fun_env <- list(n_points_on_frontier,
                utility_interpretation = T,
                backtest_info,
                frontier_and_budget_plot = T,
                group_info = NULL,
                group_colors = NULL,
                group_small = T)

#this_fig_title <- paste("Figure 19: Optimal portfolio frontier, utility weights,", this_area)
this_fig_title <- NULL
list_out <- get_optimal_frontier(cormat, mat_nab,
                                 Rtarg_limits = these_Rtarg_limits,
                                 fig_title = this_fig_title,
                                 fun_env)

# df_frontier <- list_out[[2]]
# df_backtest <- list_out[[3]]
outlist_gg <- list_out[[4]]
gg_frontier <- outlist_gg[1]
gg_weights <- outlist_gg[2]
gg_mvUFAO <- gg_frontier[[1]] + gg_weights[[1]] + plot_layout(ncol = 1)
#------------------------------------
# gg_backtest <- plot_frontier_wBacktest(df_frontier,
#                                        df_backtest,
#                                        fig_title = NULL)
# #------------------------------------
# (gg_frontier[[1]] / gg_weights[[1]] / plot_layout(ncol = 1)) | gg_backtest
# #https://gotellilab.github.io/GotelliLabMeetingHacks/NickGotelli/ggplotPatchwork.html
#=======================================================================
# Signals correlation matrix risk-reward frontier
n_signals <- ncol(mat_loads_sig_train)
nab_C <- rep(1, n_signals)
#------------------------------------
#mat_pctDiff_sig_test <- mat_pctDiff_test %*% mat_eigvecs_sig_train
#------------------------------------
# Correlation matrix
cormat <- cor(mat_pctDiff_sig_train)
#cormat_test <- round(cor(mat_pctDiff_sig_test), 7)
# covmat <- cov(mat_pctDiff_sig_train)
# covmat_test <- round(cov(mat_pctDiff_sig_test), 7)
# cormat <- Hmisc::rcorr(mat_pctDiff_sig_train)$r
# cormat_test <- round(Hmisc::rcorr(mat_pctDiff_sig_test)$r, 7)
#------------------------------------
# Expected returns vector
#nab_pctRet_train <- apply(mat_pctDiff_sig_train[ind_equal_test, ], 2, function(x) prod(1 + x)) - 1
nab_pctRet_train <- apply(mat_pctDiff_sig_train, 2, function(x) prod(1 + x)) - 1
#nab_pctRet_test <- apply(mat_pctDiff_sig_test, 2, function(x) prod(1 + x)) - 1
#------------------------------------
mat_nab <- cbind(nab_pctRet_train, nab_C)
signal_names <- c("Sig. 1: Sweet Potatoes,\nCassava, Cowpeas", "Sig. 2: Lentils,\n Chickpeas")
row.names(mat_nab) <- signal_names
#------------------------------------
fun_env <- list(n_points_on_frontier,
                utility_interpretation = T,
                backtest_info = NULL,
                frontier_and_budget_plot = T,
                group_info = NULL,
                group_colors = NULL,
                group_small = NULL)

list_out <- get_optimal_frontier(cormat, mat_nab,
                                 Rtarg_limits = c(0.8, 3),
                                 fig_title = NULL,
                                 fun_env)

df_frontier <- list_out[[2]]
df_backtest <- list_out[[3]]
outlist_gg <- list_out[[4]]
gg_frontier <- outlist_gg[1]
gg_weights <- outlist_gg[2]
gg_mvUSigsFAO <- gg_frontier[[1]] + gg_weights[[1]] + plot_layout(ncol = 1)
#=======================================================================
gg_mvUFAO | gg_mvUSigsFAO
#=======================================================================

```

```{r, fig.show='hold', fig.width=5, fig.height=5, fig.align='center', fig.cap="\\label{fig:mvUSigFAO}\\textit{(Left)}  \\textit{(Right)} Backtest of the optimal frontier.", echo=F}


```


<!-- The frontier of a dimensionally reduced portfolio, containing just the two signals (Figures \ref{fig:sigs1FAO} and \ref{fig:sigs2FAO}), on the other hand, is much more accurate/understates. -->

\pagebreak

# Discussion and conclusion

Discussion

-AR4D resource allocation has been called for for a long time .
-There is a cultural impasses between donors and research institutes . 
-Much of the cultural impasse is premised upon a highly qualitative resource allocation procedure that is easily influenced by stakeholder pressure. basic lack of precise impartial tools at the portfolio level. 
-It stands to reason, then, that the elaboration of such tools could go a long way towards ameliorating the toxic environment. 
-[BUT] Adaptation of MV analysis to the AR4D context requires clearing some non-trivial methodological hurdles, which have also been of concern in the financial context: addressing the negative budget allocations and noise reduction. Here I have explored the possibility of redressing these issues in a novel ways (...) .
-Here I am mainly concerned with introduction and adaptation of these methods to the AR4D context. and pointing towards their potential. In the financial example, there is enough data to conduct a backtest. The results of the backtest are encouraging, but do not necessarily prove that the methods presented here will always generate portfolio frontiers that are more accurate than the conventional frontier. More empirical tests are necessary for that.

When applied to the AR4D context, the modified portfolio optimization workflow yields analagous insights. The crops loading highest onto the signals are those which are most correlated with the signals (Figures \ref{fig:loadsRotFAO} and \ref{fig:sigs2FAO}). The optimal budget allocation increases with investor risk tolerance for some crops, while decreasing for others (Figure \ref{fig:mvUtilityFAO}).

-COPARE The noise reduced [signal] portfolio vs. the conventional portfolio.

However, the AR4D data environment is much sparser, yielding just two signals that can be meaningfully distinguished from noise, and not enough observations to conduct a backtest.

Suggested future research

As FAO resolution (number of observations) increases, the components of the system will become more clearly rendered. In the meantime, the FAO dataset could be extended into the future using IMPACT projections. "Future backtests" of the optimal portfolio could then be conducted for different Representative Concentration Pathway and Shared Socioeconomic Pathway scenario combinations [@IMPACT citation].

This exercise has merely scratched the surface of the potential for MV analysis applications in the AR4D context. An obvious next step would be to repeat the exercise for different geographies and country groupings ("Net Food Importing Developing Countries", etc.).^[My own cursory explorations suggest that efficient frontiers and budget shares vary substantially from one geography to another.] It would also be interesting to repeat the analysis on a value per kilocalorie basis rather than the value per unit weight basis used above. Finally, in the AR4D context the portfolio approach need not be based on crops or price data at all. The portfolio could, rather, contain research programs; and the returns vector could reflect their respective expected impacts, measured using ex-ante impact assessment tools. This would be more in line with the AR4D resource allocation workflow articulated by Mills, referred to at the start of this paper. However, this more direct approach would require assessment not only of expected impacts, but also of the risks associated with each program. It would also require a precise assessment of synergies and tradeoffs between programs, such that a program impact covariance matrix could be estimated. Any effort to apply MV analysis to a portfolio of programs, initiatives, or projects must first adequately address these issues. [In addition to redressing the issue of negative budget weights and noise, which I have done in this paper, diminishing returns and risk assessment] Millsian Workflow diagram?

[end]

MV Analysis is still a work in progress even in the financial context where it originated. In my adaptation to the AR4D context, I have attempted to redress the issue of negative budget shares by replacing the revenue and cost functions with utility functions. To redress the issue of noise-induced inaccuracy, I have attempted to purge the noise from the portfolio by dimensionally reducing the portfolio to a handful of signals constructed from those eigenvectors of the data correlation matrix that can be meaningfully distinguished from noise. To isolate these signals, I have applied a criterion developed in the study of phsyical systems, more rigorous than the usual rules of thumb. A backtest of these modifications compares favorably to conventional MV Analysis.

<!-- An important implication of the theorem is that signal extraction is as much a function of data quality (i.e. whether there really is structure in the observed system, and how well this is captured in the data) as it is of data quantity. If none or very few of the eigenvalues of a given data correlation matrix can be distinguished from noise, this can be either because there is no structure to be found in the observed system, or because there are not enough observations to fully flesh out the structure. Just as a low resolution image of a person is difficult to distinguish from an image of randomly shaded pixels, underlying structure in the FAO data may be difficult to discern due to the low number of observations currently available. -->

<!-- * Efficient frontiers and budget shares vary substantially with geographic focus. The accuracy of those frontiers in future backtests, moreover, varies substantially with assumptions about future climate, demographic, and economic growth projections, as well as assumptions about government commitment to mitigating. -->

<!-- * Other things to try: other geographical areas or categories ("Net Food Importing Developing Countries"), future scenarios, vap per kcal -->

<!-- [Removed from abstract:  There is not enough FAO data for an historical backtest, but I introduce the notion of a "future backtest" based on hypothetical extensions of the FAO data into the future using the IMPACT sector equilibrium model. Future backtest results are compared under a range of different assumptions regarding future socioeconomic and emissions trajectories.] -->

   <!-- dimensionally reduce the portfolio from the number of items it contains to the number of signals that can be meaningfully distinguished from noise. I then conduct MV Analysis over the signals portfolio. In a backtest over the present financial dataset, this method generates a frontier that offers more accurate, better peforming portfolios than the conventional approach. This may be of interest to financial analysts, as it has not yet appeared in the financial literature. whereby the noisy dimensions are purged from the data and only the "signally" dimensions retained. -->

\pagebreak

# References
