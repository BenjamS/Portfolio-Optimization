---
title: "A new approach to risk-adjusted optimization of agricultural research for development portfolios"
author:
  - name: Ben Schiek
    email: b.schiek@cgiar.org
    affiliation: CIAT
address:
  - code: CIAT
    address: International Center for Tropical Agriculture (CIAT), Km 17 Recta Cali-Palmira, Valle del Cauca, Colombia
abstract: |
  Conventional agricultural research for development priority setting exercises generate valuable insight into the strengths and weaknesses of individual agricultural research proposals, but they stop short of providing tools that can translate this insight into optimal resource allocation shares across a portfolio containing several such proposals. They also lack tools for risk accounting. Here I explore the possibility of redressing both of these methodological lacunae in one stroke by adapting Mean-Variance Analysis, a risk-adjusted portfolio optimization technique developed in financial contexts, to the agricultural research portfolio optimization problem. Along the way, I present new approaches to two old problems faced in Mean-Variance Analysis: negative budget weights and inaccurate mean-variance frontiers. These innovations compare favorably to the baseline Mean-Variance model in a backtest using financial data. I then apply the adapted Mean-Variance model to the agricultural research context using FAO farmgate price data for the major staple crops. There is not enough FAO data for an historical backtest, but I introduce the notion of a "future backtest" based on hypothetical extensions of the FAO data into the future using the IMPACT sector equilibrium model. Future backtest results are compared for different geographical regions under a range of different assumptions regarding future socioeconomic and emissions trajectories.
keywords: Risk-adjusted portfolio optmization, Mean-Variance Analysis, AR4D, ex-ante
  impact assessment, foresight
journal: European Journal of Operational Research
date: "`r Sys.Date()`"
bibliography: AR4D Portfolio Optimization.bib
biblio-style: apalike
#linenumbers: true
numbersections: true
output:
  rticles::elsevier_article:
    latex_engine: xelatex
mainfont: Garamond
---
<!-- https://tug.org/FontCatalogue/ -->
<!-- Source Serif Pro Light -->
<!-- rmarkdown::draft("Test.Rmd", template = "elsevier_article", package = "rticles") -->

# Introduction

## Mills' missing fifth step

Long ago, Bradford Mills articulated the four steps of priority setting in agricultural research for development (AR4D) as follows [-@Mills1998]:

>Step one is the development of an information base. Step two is the establishment of research alternatives. Step three is the evaluation of the potential impact of research alternatives. Step four, often considered the final output of a priority-setting exercise, is the ranking of alternatives into research priorities.

However, he went on to observe that these steps were insufficient. An as of yet non-existent

>fifth step, the development of guidelines for translating priorities into planning and resource allocation decisions, is necessary to establish direct links with planning and resource allocation activities [@Mills1998].

Mills identified this lacuna at a time when AR4D centers were just beginning to come under pressure from public and private donors to "do a lot more with a lot less" [@Alston1995]. Methodological work in response to this pressure focused on Steps 1-4. The emergence and refinement of numerous ex-ante impact assessment models for the evaluation of individual research alternatives, in particular, is remarkable (see, for example, @Alston1995; @Antle1999; @Antle2015; Mills, 1998; and @nelson2014modeling). However, work has still not begun on Mills' missing fifth step.

In hindsight, Mills' concern was well founded. Budgetary pressure on AR4D centers to "prove their relevance" [@Braunschweig2000], "show value for money" [@yet2016bayesian], and otherwise demonstrate "more efficient spending of resources" [@petsakos2018comparing] has only increased since the 1990s. Between Step 4 and the final allocation of funds, stakeholder politics, institutional inertia, ad hoc procedures and criteria, and other subjective forces have repeatedly undercut any careful, objective rigor occurring in Steps 1-4 [@Birner2016; @mccalla2014cgiar]. This  inconsistency, opacity, and subjectivity in resource allocation decisions, has, in turn, aggravated scientists' already longstanding distrust of budget allocation mechanisms. Relations between donors and the research programs they fund have thus reached historic levels of toxicity [@Birner2016; @leeuwis2018reforming; @mccalla2014cgiar].

The problem at once suggests its solution. A rigorous tool _at the portfolio level_, along the lines suggested by Mills over twenty years ago, could go a long ways in guaranteeing the consistency, transparency, and objectivity that has been lacking from the resource allocation process. The AR4D community must replace its pervasive emphasis on the "priority setting exercise" with a pervasive emphasis on the "resource allocation exercise". A list of research alternatives ranked in order of priority is ultimately useful, after all, only insofar as it can be translated into precise budget shares. It would also be nice if such a method took explicit account of the risk associated with each research alternative, and were quick and inexpensive to implement relative to consensus building mechanisms such as the Analytical Hierarchy Process [Braunschweig2000]. In this paper, I explore the possibility of building such a tool based on risk-adjusted portfolio optimization techniques developed in the financial context.


## AR4D mean-variance analysis

In particular, I explore the possibility of adapting Mean-Variance (MV) Analysis, first conceptualized by Markowitz [@markowitz1952portfolio] and later formalized and refined by Merton [@merton1972analytic], to the AR4D context. In stock market investment contexts, MV Analysis is used to optimize the investor's return on investment in a portfolio of $n$ risky assets, given the investor's level of risk tolerance. On input, MV Analysis takes the expected returns and risk of each asset in the portfolio, and outputs the precise budget shares that must be invested in each of the portfolio assets in order to achieve the risk adjusted maximum return. An "efficient frontier" can also be constructed indicating the maximized return associated with any level of risk tolerance.

To build intuition, I first walk through an application of MV Analysis in its native financial context using daily price data downloaded from yahoo finance. Solutions are backtested on a subsequent dataset. Then, I move to an application in the AR4D context using yearly country-level farmgate price data for staple crops downloaded from the Food and Agriculture Organization (FAO), aggregated up to various geographical regions. The FAO price series are not long enough to conduct a backtest of the optimal solution. [However, I extend the dataset into the future using the International Model for Policy Analysis of Agricultural Commodities and Trade (IMPACT) and conduct "future backtests" over this extension. These backtests are conducted under a variety of assumptions regarding future demographic and economic growth, as well as future emissions scenarios.]

There are, of course, many differences between the financial and AR4D contexts that must be kept in mind. First and foremost of these differences is the motivation for investment. By investing in AR4D of key staple crops, the donor aims to increase their quality and production, thereby lowering their price and making them more accessible to consumers and producers. A financial investor, on the other hand, typically has no expectation that their investment will move the price in any way, but would hope that the price goes up. Another difference is in time horizons. AR4D investment time horizons are on the order of 20-40 years, whereas the time horizon of the typical investor is on the order of 5-10 years. The data environments also differ between the two contexts. Financial data are available on a daily, even intradaily, time scale. Completeness becomes an issue when conducting an analysis that extends back to before 2004 or so, but for portfolio optimization purposes this is hardly necessary. Agricultural commmodity farmgate price series, on the other hand, go back considerably further, but are typically available only on a yearly scale. Also, a whole new dimension enters into play when moving from finance to AR4D: geography. Where does the donor want to see impacts?

These differences are important to keep in mind when preparing data for input, or when interpreting outputs; but they do not require any alteration to the MV Analysis apparatus as received from the financial context. However, there is another major difference between the two contexts that does require adjustment when adapting MV Analysis. In particular, MV Analysis allows for negative budget shares. A negative sign on a budget share indicates that the investor should invest in the inverse of the corresponding portfolio item. In the financial context, this is possible through short selling, or through investment in financial products that track the inverse of the given asset price.^[However, even in the financial context, negative weights are viewed by many as a methodological nuissance. See, for example, Boyle [-@boyle2014positive]. One problem that arises is that a portfolio with both negative and positive weights implies that the investor must borrow beyond their budget.] In the AR4D context, there is no analogue to either one of these conventions. To force positive budget shares, I replace the linear returns and cost functions with log-linear returns and cost functions, meant to represent their respective utilities. These utility functions are derived from the time honored assumption of marginally diminishing returns to increments in the quantity of any particular good. This forces the MV Analysis apparatus to output a vector of optimal logged budget shares, which in turn guarantees that the budget shares themselves are positive. This modification violates the budget constraint, but the budget constraint can then be arbitrarily restored by virtue of the fact that utility functions are defined only up to an affine transformation. This adaptation is novel, and so may be of interest to financial analysts.

In a more advanced iteration of the method explored here, the expected returns and risk associated with each of the research alternatives in the AR4D portfolio would be evaluated using an ex-ante impact assessment model, thus adhering to the budget allocation workflow defined by Mills. Before that can happen, however, modelers must first develop more sophisticated risk assessment tools at the project or program level. Alston and Norton acknowledged in 1995 that the treatment of risk in impact assessment models was "rudimentary and in need of further refinement" [-@Alston1995]. This is still true today.


## Separating signals from noise, identifying main assets contributing to market movements

MV analysis is, in many ways, still a work in progress, with unresolved methodological issues severely limiting its usefulness in everyday financial practice. In a nutshell, price data is inherently noisy, and so one is optimizing the portfolio over a great deal of noise; and the optimal solutions then tend to diverge considerably from reality as a result of this [@michaud1989markowitz]. In my adaptation below, I propose a new approach to this problem by first dimensionally reducing the portfolio from several dozen assets to just a handful of "signals" constructed from the eigenvectors of the correlation matrix that can be meaningfully distinguished from noise.

The meaningful eigenvectors are separated out from the noisy eigenvectors using an old, but little-utilized technique developed in the study of physical systems. Once signals have been isolated, their meaning is interpreted based on how much the different portfolio items load onto them. A varimax rotation of the loadings matrix turns out to be of great assistance in this task. The interpretation of the signals is then further confirmed and illustrated by constructing the signal time series and then plotting them against their respective highest loading price series. The highest loading price series are seen to track their respective signals closely.




[Scraps:
 The Consultative Group on International Agricultural Research (CGIAR) is said to have a "long history of good intentions but limited success in developing appropriate approaches for priority setting" [@Birner2016]. An historic effort begun in 2011 to restructure CGIAR financing around Core Research Programs (CRPs) was ad hoc, thereby reinforcing the very institutional inertia it was designed to disrupt [@Birner2016]. ReDonor attempts to establish criteria and methods for a merit-based allocation of ever-scarcer research funds are seen as invasive characterized as "development at the expense of research" [@Birner2016] or even the "Balkanization" of research [@Petsko2011]. "One of the geniuses" of CG centers is said to be their ability to consistently attract "aid funding of long-term research" while keeping "aid professionals from setting research agendas" [@mccalla2014cgiar]. Scientists respond with ultimatums of their own, demanding a return to the days of "more stable funding" and "budget autonomy" [@leeuwis2018reforming].
“The failure of reforms is attributed to the unwillingness of donors, and the World Bank leadership of the CGIAR, to take on entrenched center interests” [@mccalla2014cgiar]. Interpretation of "price" is different from the financial context. This could reflect 1) higher kcals per unit weight, 2) higher quality kcals, 3) higher demand, or 4) some combination of all of the above.

]

# Methods

## Isolating and interpreting signals

Given enough observations, a large matrix of time series data $X$ can be condensed into just a handful of "signals" that capture the majority of the movement in the data. Formally, these signals are defined

\begin{equation}
S = X \tilde{P}
\end{equation}

where $P$ is the matrix of eigenvectors of $X'X$, and $\tilde{P}$ is this same matrix truncated to retain only the eigenvectors containing significant information---i.e. information that can meaningfully be distinguished from noise. A number of different rules of thumb are often followed to determing how many vectors of $P$ should be retained. "Keep only the signals that describe 90% of the variation", for example, or "the scree test", or "keep only those with eigenvalues greater than 1". The perils of using these arbitrary cutoff rules has been well documented [@russell2002search]. Here I follow Laloux, Cizeau, Bouchaud, and Potters [-@laloux1999noise] who apply a more rigorous technique developed by physicists in the 1960s [@dehesa1983mathematical; @mehta2004random]. The technique is based on a key theorem of Random Matrix Theory that says that the distribution of the eigenvalues of any random correlation matrix is a function of the dimensions of the underlying matrix. Physicists have used this theorem to identify the important components ("collective modes") in complexly interacting systems by comparing the eigenvalue density plot of their data correlation matrix against that of a random matrix. The eigenvalues of the data correlation matrix that extend beyond the random matrix eigenvalue density can be identified as corresponding to components of the system that can be meaningfully distinguished from noise.

Signal separation effectively reduces the problem of making sense of several dozen complexly interacting indicators to the more mangeable problem of making sense of just a few cross cutting trends. The next step is to interpret these cross cutting trends in concrete terms. This can be achieved by looking at a varimax rotation of the loadings corresponding to each signal. The loadings indicate how correlated each portfolio item is with each signal. Signals can then be characterized in concrete terms based on which items are most strongly associated with their movement.

The signal loadings are defined

\begin{equation}
\tilde{L} = \tilde{P}\tilde{\Lambda}^{1/2}
\end{equation}

where $\Lambda$ is the diagonal matrix of eigenvalues of $X'X$, and where, as before, the squiggly lines on top indicate that the respective matrix is truncated to contain only the columns that can be meaningfully separated from noise.

A varimax rotation is then applied to $\tilde{L}$ to throw any existing structure in data into stark relief.

\begin{equation}
L_{\circlearrowright} = VL
\end{equation}

where $L_{\circlearrowright}$ is the matrix of rotated loadings and $V$ is an orthogonal matrix (the varimax rotation). We are then interested in only the retained rotated loadings $\tilde{L}_{\circlearrowright}$.

The components of the retained rotated loadings can then be interpreted as the magnitude and direction of the influence of each portfolio item over the average movement of the dataset. Loadings with opposite signs indicate a tradeoff, while loadings with the same sign are indicative of synergy. When there is structure in the data, the rotated loadings tend to be thematically organized, such that each loading vector corresponds to a particular aspect of the overall evolution of the system.^[Gopikrishnan, Rosenow, Plerou, and Stanley [-@gopikrishnan2001quantifying] pursued a similar line of inquiry when they looked at the components of the eigenvectors of a financial data correlation matrix. But they did not first apply a rigorous signal separation technique as I have done here. Nor did they apply a varimax rotation to clarify the interpretation.]


## Risk adjusted portfolio optimization

The risk-adjusted optimization problem for a portfolio of $n$ risky assets is formulated as follows. 

\begin{equation}
\max_{\mathbf{w}}\:R \:\:\:\:s.t. \:\:\: C=\bar{C} \:, \:\:\: V = \bar{V}
\end{equation}

where $\mathbf{w}$ is the vector if budget weights, $R$ is the expected portfolio return, defined $R = \mathbf{w} \cdot \boldsymbol{\mu}$, where $\boldsymbol{\mu}$ is the vector of expected returns of each asset; $C$ is the cost, defined $C = \mathbf{w} \cdot \mathbf{1}$, constrained to sum to the budget $\bar{C}$; and $V$ is the portfolio risk, defined $V = \mathbf{w} \cdot K \cdot \mathbf{w}$, where $K$ is the asset correlation matrix, and $V$ is constrained to equal the investor's risk tolerance $\bar{V}$.

(In the financial context, the problem is usually formulated as a risk minimization problem subject to a budget constraint and return target. Both approaches yield the same outputs. In the financial context, the problem includes the option to invest in one risk free asset. There is no risk free investment in the AR4D context, so the set up here focuses on optimization of the risky portfolio only.)

The Lagrangian is then

\begin{equation}
\mathcal{L} = R - \lambda_C(C - \bar{C}) - \lambda_{V}(V - \bar{V})
\end{equation}

with first order conditions

\begin{equation}
\nabla \mathcal{L} = \boldsymbol{\mu} - \lambda_C \mathbf{1} - 2 \lambda_{V} K \cdot \mathbf{w} = \mathbf{0}
\end{equation}

and second order conditions

\begin{equation}
\mathbf{w} \cdot \nabla^2 \mathcal{L} \cdot \mathbf{w} = - 2 \lambda_{V} V < 0
\end{equation}

(where $\nabla^2 \mathcal{L}$ is the Hessian matrix of $\mathcal{L}$.) Hence, a maximum is guaranteed so long as $\lambda_{V}$ is positive.

Dotting the first order conditions through by $\mathbf{w}$ gives the equation for the efficient frontier.

\begin{equation}
R^* = \lambda_C \bar{C} + 2 \lambda_V \bar{V}
\label{eq:rStar}
\end{equation}

where the asterisk on $R$ indicates that this is the maximum portfolio return given budget constraint $\bar{C}$ and risk tolerance $\bar{V}$.

Note, in passing, that equation \ref{eq:rStar} implies that the risk shadow price is proportional to the expected reward to risk ratio.

\begin{equation}
\lambda_V = \frac{NR}{2\bar{V}} \: ; \:\:\: \psi = R^* - \lambda_C \bar{C}
\end{equation}

where $\psi = R^* - \lambda_C \bar{C}$ is the expected reward, i.e., the expected net revenue, adjusted by the budget shadow price $\lambda_C$.

Now, dotting the first order conditions through by $K^{-1}$ and rearranging gives an equation for the optimal budget weights.

\begin{equation}
\mathbf{w}^* = \frac{1}{2 \lambda_V} K^{-1} \cdot \nabla \psi \: ; \:\:\: \nabla \psi = \boldsymbol{\mu} - \lambda_C \mathbf{1}
\label{eq:budgetShares}
\end{equation}

Note, in passing, that dotting this through by $\nabla \psi$ gives another instructive equation for $\lambda_V$.

\begin{equation}
\lambda_V = \frac{d_m^2}{2 \psi} \: ; \:\:\:\:d_m^2 = \nabla \psi \cdot K^{-1} \cdot \nabla \psi
\end{equation}

This says that the risk shadow price is inversely proportional to the ratio of the squared Mahalanobis distance ($d_m^2$) of the portfolio net reward gradient to the net reward. In this setting, the Mahalanobis distance reflects how improbable a given portfolio is. [Note that $d_m = 0$ only when $\nabla \psi = \mathbf{0}$, which occurs only when every component in the vector of asset returns equals] Moreover, combining this with the previous expression for $\lambda_V$ gives

\begin{equation}
\psi = d_m \sqrt{\bar{V}}
\end{equation}

The expected portfolio reward is thus equal to the the square root of the risk tolerance scaled by the improbability of the portfolio.

Returning to the task at hand, in order to evaluate the optimal budget weights or the frontier in equations \ref{eq:rStar} and \ref{eq:budgetShares}, we must first solve for the cost and risk shadow prices $\lambda_C$ and $\lambda_V$.

To do this, first note that the budget weights equation can be rewritten as follows:

\begin{equation}
\mathbf{w}^* = \frac{1}{2 \lambda_V} K^{-1} \cdot [\boldsymbol{\mu}, \: \mathbf{1}] \left[\begin{matrix} 1 \\ -\lambda_C \\ \end{matrix} \right]
\end{equation}

Now, dotting through by $[\boldsymbol{\mu}, \mathbf{1}]$ gives

\begin{equation}
\left[\begin{matrix}
R \\
\bar{C} \\
\end{matrix} \right]  = \frac{1}{2 \lambda_{V}} M\left[\begin{matrix}
1 \\
-\lambda_C \\
\end{matrix} \right]
\end{equation}

where $M$ has been introduced to stand for the matrix

\begin{equation}
M = [\boldsymbol{\mu}, \: \mathbf{1}]' \cdot K^{-1} \cdot [\boldsymbol{\mu}, \: \mathbf{1}]
\end{equation}

Let $M$ be called the "Merton matrix", after the man in whose footsteps we are now following [@merton1972analytic]. Pre-multiplying both sides of the previous equation by the inverse Merton matrix and rearranging gives the following expression: 

\begin{equation}
2 M^{-1}\left[\begin{matrix}
R \\
\bar{C} \\
\end{matrix} \right]  = \left[\begin{matrix}
1/\lambda_V \\
-\lambda_C / \lambda_V \\
\end{matrix} \right]
\end{equation}

For any given return target $R$ and budget $\bar{C}$, then, the cost and risk shadow prices are given by this equation. With values for $\lambda_C$ and $\lambda_V$ in hand, the budget weights and risk associated with the chosen return target can be evaluated.


## Dealing with negative weights

In equation \ref{eq:budgetShares} it is clear that budget shares can be negative. As discussed in the introduction, this is unacceptable in the AR4D context. In order to force optimal budget weights, I replace the revenue $R$ and $C$ functions with the utility functions $U_R$ and $U_C$, where

\begin{equation}
U_R = \boldsymbol{\mu} \cdot \ln{\mathbf{w}}
\end{equation}

and 

\begin{equation}
U_C = \mathbf{1} \cdot \ln{\mathbf{w}}
\end{equation}

Moreover, the budget $\bar{C}$ is replaced with the utility budget $\bar{U}_C$.

This is tantamount to substituting $\ln \mathbf{w}$ wherever $\mathbf{w}$ appears in the equations of the previous section.

The utility functions $U_R$ and $U_C$ can be derived from the observation, going back perhaps as far as Aristotle [@kauder1953genesis], and certainly as far back as Bernoulli [-@Bernoulli1954], that

>"in the absence of the unusual, the utility resulting from any small increase in wealth will be inversely proportionate to the quantity of goods previously possessed."

This time honored observation can be formalized as follows:

\begin{equation}
\frac{\partial U}{\partial w_i} \sim \frac{d \ln w_i}{dw_i}
\label{eq:uDefine}
\end{equation}

Note that multiplying through by $U / w_i$ gives an interesting alternative expression.

\begin{equation}
\frac{\partial \ln U}{\partial \ln w_i} \sim \frac{d \ln U}{dU}
\end{equation}

Equation \ref{eq:uDefine} can be used to arrive at an expression for $U$ as follows:

\begin{equation}
U(\mathbf{w}; \boldsymbol{\alpha}, k) = \int \nabla_{\mathbf{w}} U \cdot d\mathbf{w} = \boldsymbol{\alpha} \cdot \ln{\mathbf{w}} + k
\end{equation}

Since utility is defined purely on the basis of the marginal relation articulated by Bernoulli and formalized in equation \ref{eq:uDefine}, then, generally speaking, it is defined only up to the arbitrary constants $\boldsymbol{\alpha}$ and $k$. (In mathspeak, it is said that utility is defined up to an affine transformation.) However, arbitrary constants are often fixed by the specific context of the problem. In the present setting, the constants $\boldsymbol{\alpha}$ correspond to the vectors $\boldsymbol{\mu}$ and $\mathbf{1}$ in the $U_R$ and $U_C$ functions, respectively, leaving just $k$ unfixed. $U_R$ and $U_C$ are thereby defined up to translation. This means that 

\begin{equation}
U_R(\mathbf{w}; \boldsymbol{\mu}, k_1) = U_R(\mathbf{w}; \boldsymbol{\mu}, k_2)
\label{eq:uTrans}
\end{equation}

for two different constants $k_1$ and $k_2$. This is very important because it allows for arbitrary enforcement of the budget constraint. The substitution $\ln \mathbf{w}$ for $\mathbf{w}$ generally causes the budget shares to no longer add up to the constraint $\bar{C}$. But equation \ref{eq:uTrans} says that the weights can be divided by the constant $\bar{C} / \mathbf{1} \cdot \mathbf{w}$ so as to restore the condition $\mathbf{1} \cdot \mathbf{w} = \bar{C}$, and that $U_R$ is invariant under this operation. It is tantamount to setting $k_1 = 0$ and $k_2 = \ln \bar{C} - \ln (\mathbf{1} \cdot \mathbf{w})$.

\begin{equation}
U_R(\mathbf{w}; \boldsymbol{\mu}) = U_R(\mathbf{w}; \boldsymbol{\mu}, \ln \bar{C} - \ln (\mathbf{1} \cdot \mathbf{w}))
\end{equation}

## Dealing with noise-induced inaccuracy in MV Analysis

As mentioned in the introduction, MV Analysis is of limited practical use in real life due to the difficulty involved in accurately estimating asset returns and correlations (for more details, see ()). While adapting MV Analysis to the AR4D context, I tried to address this problem in several different ways, all but one of which were ineffective. The one approach that showed some promise---at least with respect to the financial data used in this study---may be characterized as a dimensional reduction of the original problem.
As mentioned in the introduction, the real world effectiveness of MV Analysis is severely limited by its   Real portfolio returns tend to be lower, and risk higher, than the MV frontier would suggest. To address this problem, I dimensionally reduce the portfolio from the number of items it contains to the number of signals that can be meaningfully distinguished from noise. I then conduct MV Analysis over the signals portfolio. In a backtest on financial data, this method generates a frontier that offers more accurate, better peforming portfolios than the conventional approach. This may be of interest to financial analysts, as it has not yet appeared in the financial literature. whereby the noisy dimensions are purged from the data and only the "signally" dimensions retained.

# An example from the financial context

Before exploring the possibility of applying these tools to the AR4D context, it will be instructive to first apply them in the financial context where they have been developed. Below, daily financial data for 46 securities over the period 20 June 2018 to 8 August 2019 is downloaded directly into R from yahoo finance using the tidyquant package. The 46 securities have been chosen so as to be representative of the radar screen of a modern, broad minded investor (Table 1). They are mostly exchange traded funds tracking broad categories of stocks, bonds, markets, and commodities, plus the major currency pair exchange rates traded on the FOREX market.

[Table here detailing the ETF names, what they track, and my category name]

The dataset is large enough to allow for backtesting of the optimal portfolio. The portfolio is optimized using the first two thirds of the data (the "train" data), and then backtested against the remaining third. Historical returns and correlation matrices are presented for these two datasets in Figures .... 

```{r, fig.width=14, fig.height=14, fig.align='center', echo = FALSE}

#setwd("D:/OneDrive - CGIAR/Documents")
#options(warn = -1); options(scipen = 999)
#-------------------------------------------------------------
#devtools::install_github("thomasp85/patchwork")
library(plyr)
# library(tidyverse)
# library(ggplot2)
library(zoo)
# library(FactoMineR)
# library(factoextra)
# library(Hmisc)
# library(corrplot)
library(tidyquant)
library(patchwork)
#=======================================================================
# Resources:
# Correlation matrices:
# http://www.sthda.com/english/wiki/correlation-matrix-a-quick-start-guide-to-analyze-format-and-visualize-a-correlation-matrix-using-r-software
# https://cran.r-project.org/web/packages/corrplot/vignettes/corrplot-intro.html
#=======================================================================
# Define functions
#=======================================================================
signals_from_noise <- function(mat_pctDiff,
                               fig_title_eigDens = NULL,
                               fun_env = NULL){
  
  if(is.null(fun_env)){
    eigenvalue_density_plot = T
    pca_var_plot = T
    pca_ind_plot = T
    group_info = NULL
    quietly = F
  }else{
    eigenvalue_density_plot = fun_env[[1]]
    pca_var_plot = fun_env[[2]]
    pca_ind_plot = fun_env[[3]]
    group_info = fun_env[[4]]
    quietly = fun_env[[5]]
    
  }
  #---------------------------------------------------------
  # Separate signals from noise
  #---------------------------------------------------------
  res <- FactoMineR::PCA(mat_pctDiff, ncp = ncol(mat_pctDiff), graph = F)
  eigvals <- as.data.frame(res$eig)$eigenvalue
  eigval_max <- max(eigvals)
  mat_loads <- res$var$coord
  mat_loads_rot <- varimax(mat_loads)[[1]]
  mat_eigvecs <- mat_loads %*% diag(1 / sqrt(eigvals))
  #---------------------------------------------------------
  # Apply random matrix theory () to determine eigenvalue distribution of a 
  # correlation matrix of random data.
  n_obs <- nrow(mat_pctDiff)
  n_items <- ncol(mat_pctDiff)
  Q <- n_obs / n_items
  s_sq <- 1 - eigval_max / n_items
  #s_sq <- 1
  eigvals_rand_max <- s_sq * (1 + 1 / Q + 2 / sqrt(Q))
  eigvals_rand_min <- s_sq * (1 + 1 / Q - 2 / sqrt(Q))
  eigvals_rand <- seq(eigvals_rand_min, eigvals_rand_max, length.out = n_items)
  eigvals_rand_density <- Q / (2 * pi * s_sq) * sqrt((eigvals_rand_max - eigvals_rand) * (eigvals_rand - eigvals_rand_min)) / eigvals_rand
  #---------------------------------------------------------
  # Plot eigenvalue density vs. random matrix eigenvalue density
  df_plot_data <- data.frame(Eigenvalue = eigvals, Type = "Data")
  df_plot_rand <- data.frame(Eigenvalue = eigvals_rand, Type = "Random")
  df_plot <- rbind(df_plot_data, df_plot_rand)
  
  if(eigenvalue_density_plot){
    if(is.null(fig_title_eigDens)){fig_title_eigDens <- "Eigenvalue density"}
    gg <- ggplot(df_plot, aes(x = Eigenvalue, fill = Type))
    gg <- gg + geom_density(alpha = .3)
    gg <- gg + theme(axis.title.y = element_blank(),
                     axis.text.y = element_blank(),
                     axis.title.x = element_text(size = 10),
                     plot.caption = element_text(size = 10, hjust = 0),
                     legend.title = element_blank())
    gg <- gg + labs(caption = fig_title_eigDens)
    print(gg)
    
    # gg <- ggplot()
    # gg <- gg + geom_density(data = df_plot, aes(x = Eigenvalues, color = "Correlation Matrix"), lwd = 1.1)
    # gg <- gg + geom_line(data = data.frame(x = eigvals_rand, y = eigvals_rand_density), aes(x = x, y = y, color = "Random matrix"), lwd = 1.1)
    # gg <- gg + scale_colour_manual(name = "density", 
    #                                values = c(`Correlation Matrix` = "blue", `Random matrix` = "magenta"))
  }
  #---------------------------------------------------------
  # Which data eigenvalues can be meaningfully distinguished from noise?
  ind_deviating_from_noise <- which(eigvals > eigvals_rand_max) # (eigvals_rand_max + 5 * 10^-1))
  #---------------------------------------------------------
  # Extract signal loadings matrix from noise
  mat_loads_sig <- mat_loads[, ind_deviating_from_noise]
  eigvals_sig <- eigvals[ind_deviating_from_noise]
  mat_loads_rot_sig <- mat_loads_rot[, ind_deviating_from_noise]
  #---------------------------------------------------------
  n_signals <- length(eigvals_sig)
  if(!quietly){print(paste("Number of signals: ", n_signals))}
  #---------------------------------------------------------
  # Get dimensionally reduced version of original input data
  mat_eigvecs_sig <- mat_eigvecs[, ind_deviating_from_noise]
  mat_inData_sig <- mat_pctDiff %*% mat_eigvecs_sig
  if(n_signals == 1){
    mat_inData_sig <- mat_inData_sig / eigvals_sig
  }else{
    mat_inData_sig <- mat_inData_sig %*% diag(1 / eigvals_sig)
  }
  #---------------------------------------------------------
  # Set sign of eigenvectors such that they
  # best conform to the input time series
  inData_avg <- rowMeans(mat_pctDiff)
  # if(n_signals == 1){
  #   mse <- mean((mat_inData_sig - inData_avg)^2)
  #   mse_neg <- mean((-mat_inData_sig - inData_avg)^2)
  #   if(mse_neg < mse){
  #     mat_eigvecs <- -mat_eigvecs
  #     mat_inData_sig <- -mat_inData_sig
  #     mat_loads_rot_sig <- -mat_loads_rot_sig
  #   }
  # }else{
  #   for(i in 1:n_signals){
  #     mse <- mean((mat_inData_sig[, i] - inData_avg)^2)
  #     mse_neg <- mean((-mat_inData_sig[, i] - inData_avg)^2)
  #     if(mse_neg < mse){
  #       mat_eigvecs_sig[, i] <- -mat_eigvecs_sig[, i]
  #       mat_inData_sig[, i] <- -mat_inData_sig[, i]
  #       mat_loads_rot_sig[, i] <- -mat_loads_rot_sig[, i]
  #     }
  #   }
  #   
  # }
  
  #---------------------------------------------------------
  # PCA cluster plots to examine natural grouping in the data
  #---------------------------------------------------------
  # By variable
  if(pca_var_plot){
    if(n_signals == 1){
      print("Only 1 signal, can't do PCA plots.")
    }else{
      gg <- factoextra::fviz_pca_var(res, habillage = factor(group_vec))
      print(gg)
    }
    
  }
  #---------------------------------------------------------
  # By individual
  if(pca_ind_plot){
    if(n_signals == 1){
      print("Only 1 signal, can't do PCA plots.")
    }else{
      res <- FactoMineR::PCA(t(mat_pctDiff), graph = F)
      gg <- factoextra::fviz_pca_ind(res, habillage = factor(group_vec), addEllipses = T)
      print(gg)
    }
    
  }
  #---------------------------------------------------------
  # Cluster plot using Mclust()
  # mc <- mclust::Mclust(t(mat_pctDiff))
  # summary(mc)
  # View(mc$classification)
  # factoextra::fviz_cluster(mc, frame.type = "norm", geom = "text")
  #---------------------------------------------------------
  
  
  list_out <- list(mat_loads_sig, mat_loads_rot_sig, mat_loads, mat_loads_rot, mat_inData_sig, eigvals_sig, mat_eigvecs_sig, eigvals, mat_eigvecs)
  return(list_out)
}
#=======================================================================
plot_signals_against_avg <- function(mat_inData_sig, mat_inData,
                                     fig_title = NULL, facet_ncol = 1){
  # (mat_inData = mat_pctDiff)
  #---------------------------------------------------------
  # Dimensionally reduced plot of data (signal plots)
  #---------------------------------------------------------
  n_signals <- ncol(mat_inData_sig)
  if(is.null(fig_title)){fig_title <- "Signals"}
  #---------------------------------------------------------
  # Plot signal data against average
  inData_avg <- rowMeans(mat_inData)
  date_vec <- row.names(mat_inData)
  df_plot1 <- data.frame(Date = date_vec, inData_avg)
  df_plot2 <- data.frame(Date = date_vec, mat_inData_sig)
  df_plot1$Date <- factor(format(df_plot1$Date, format = "%y-%m-%d%"), ordered = T)
  df_plot2$Date <- factor(format(df_plot2$Date, format = "%y-%m-%d%"), ordered = T)
  xAxis_labels <- df_plot1$Date[seq(1, nrow(df_plot1), length.out = 5)]
  signal_id <- paste("Signal", c(1:n_signals))
  colnames(df_plot2)[2:(n_signals + 1)] <- signal_id
  gathercols <- signal_id
  df_plot2 <- df_plot2 %>% gather_("Signal", "Value", gathercols)
  gg <- ggplot()
  gg <- gg + geom_line(data = df_plot1, aes(x = Date, y = inData_avg, group = 1), color = "orange", lwd = 2)
  gg <- gg + geom_line(data = df_plot2, aes(x = Date, y = Value, group = 1))
  gg <- gg + scale_x_discrete(breaks = xAxis_labels)
  gg <- gg + facet_wrap(~ Signal, ncol = facet_ncol)
  gg <- gg + theme(axis.title.y = element_blank(),
                   #axis.text.x = element_text(angle = 60, hjust = 1),
                   plot.caption = element_text(size = 10, hjust = 0)
  )
  gg <- gg + labs(caption = fig_title)
  print(gg)
  
  
}
#=======================================================================
interpret_loadings <- function(mat_loads_rot_sig, fig_title = NULL, fun_env = NULL){
  #---------------------------------------------------------
  if(is.null(fig_title)){fig_title = "Each item's contribution to each signal"}
  if(is.null(fun_env)){
    group_info = NULL
    signal_names = NULL
    group_colors = NULL
  }else{
    group_info = fun_env[[1]]
    signal_names = fun_env[[2]]
    group_colors = fun_env[[3]]
  }
  #---------------------------------------------------------
  # Handle case where there's just 1 signal
  # (In such cases, mat_loads_rot_sig will be of class "numeric")
  if(class(mat_loads_rot_sig) == "numeric"){
    n_items <- length(mat_loads_rot_sig)
    n_signals <- 1
    varNames_ordered <- names(mat_loads_rot_sig)
  }
  if(class(mat_loads_rot_sig) == "matrix"){
    n_items <- nrow(mat_loads_rot_sig)
    n_signals <- ncol(mat_loads_rot_sig)
    varNames_ordered <- row.names(mat_loads_rot_sig)
  }
  #------------------------------------------------------------
  # Plot loadings barcharts
  df_plot <- data.frame(id = varNames_ordered, mat_loads_rot_sig)
  #--------------
  # Name the signals, if names provided
  if(is.null(signal_names)){
    signal_id <- paste("Signal", c(1:n_signals))
  }else{
    signal_id <- signal_names
  }
  #--------------
  colnames(df_plot)[2:(n_signals + 1)] <- signal_id
  gathercols <- as.character(signal_id) 
  df_plot <- gather_(df_plot, "Signal", "Loading", gathercols)
  df_plot <- transform(df_plot,
                       Signal = factor(Signal, levels = gathercols))
  #--------------
  # Group the vars if group info is provided
  if(!is.null(group_info)){
    list_groups <- group_info[[1]]
    group_names <- group_info[[2]]
    group_vec <- rep(NA, n_items)
    for(i in 1:length(list_groups)){
      this_group_vec <- list_groups[[i]]
      this_group_name <- group_names[i]
      group_vec[which(varNames_ordered %in% this_group_vec)] <- this_group_name
    }
    #--------------
    df_plot$Type <- factor(group_vec)
    xx <- df_plot$Type
    df_plot$id <- factor(df_plot$id, levels = unique(df_plot$id[order(xx)]))
    gg <- ggplot(df_plot, aes(x = id, y = Loading, fill = Type))
    gg <- gg + scale_fill_manual(values = group_colors)
  }else{
    gg <- ggplot(df_plot, aes(x = id, y = Loading))
  }
  gg <- gg + geom_bar(stat = "identity", color = "black", position = "dodge")
  gg <- gg + facet_wrap(~ Signal, nrow = 1)
  gg <- gg + theme(axis.text.y = element_text(face = "bold", size = 10),
                   axis.text.x = element_text(face = "bold", size = 10),
                   axis.title.y = element_blank(),
                   axis.title.x = element_text(face = "bold", size = 10),
                   plot.caption = element_text(size = 10, hjust = 0))
  gg <- gg + labs(caption = fig_title)
  gg <- gg + coord_equal()
  gg <- gg + coord_flip()
  print(gg)
  
}
#=======================================================================

historical_returns_and_corr_plot <- function(mat_pctDiff,
                                             mat_pctDiff_test = NULL,
                                             group_info = NULL,
                                             returns_plot = F,
                                             corr_plot = F,
                                             group_colors = NULL,
                                             fig_title_returns = NULL,
                                             fig_title_corrplot = NULL,
                                             fig_title_corrplot_test = NULL,
                                             returns_plot_range = NULL,
                                             corrplot_options = list(
                                               plot_with_pvals = F,
                                               plot_with_corrCoefs = F,
                                               corr_coef_size = 0.75)
){
  #------------------------------------------------------------
  if(is.null(fig_title_returns)){fig_title_returns <- "Historical Returns"}
  #------------------------------------------------------------
  # Group the vars if group info is provided
  if(!is.null(group_info)){
    if(is.null(group_colors)){
      group_colors <- randomcoloR::distinctColorPalette(k = length(group_names))
      #group_colors <- viridis::viridis_pal(option = "D")(length(group_names))
    }
    n_groups <- length(list_groups)
    n_items <- ncol(mat_pctDiff)
    varNames_ordered <- colnames(mat_pctDiff)
    list_groups <- group_info[[1]]
    group_names <- group_info[[2]]
    group_vec <- rep(NA, n_items)
    group_color_vec <- rep(NA, n_items)
    for(i in 1:n_groups){
      this_group_vec <- list_groups[[i]]
      this_group_name <- group_names[i]
      this_group_color <- group_colors[i]
      group_vec[which(varNames_ordered %in% this_group_vec)] <- this_group_name
      group_color_vec[which(varNames_ordered %in% this_group_vec)] <- this_group_color
    }
    xx <- factor(group_vec)
    cols_ordered_by_group <- as.character(colnames(mat_pctDiff)[order(xx)])
    group_color_vec <- group_color_vec[order(xx)]
  }
  
  
  # for(i in 1:length(list_groups)){
  #   this_group_vec <- list_groups[[i]]
  #   this_group_color <- group_colors[i]
  #   label_colors[which(cols_ordered_by_group %in% this_group_vec)] <- this_group_color
  # }
  
  
  #------------------------------------------------------------
  from_date <- row.names(mat_pctDiff)[1]
  to_date <- row.names(mat_pctDiff)[nrow(mat_pctDiff)]
  from_date <- gsub("-", "/", from_date)
  to_date <- gsub("-", "/", to_date)
  date_interval <- paste(from_date, to_date, sep = " - ")
  fig_subtitle_returns <- date_interval
  nab_pctRet <- apply(mat_pctDiff, 2, function(x) prod(1 + x)) - 1
  #------------------------------------------------------------
  df_plot <- data.frame(Returns = nab_pctRet)
  df_plot$id <- row.names(df_plot)
  if(!is.null(group_info)){
    df_plot$Type <- factor(group_vec)
    xx <- df_plot$Type
    df_plot$id <- factor(df_plot$id, levels = cols_ordered_by_group)
  }
  #------------------------------------------------------------
  if(!is.null(mat_pctDiff_test)){
    #------
    data_type_train <- paste("Train data ", date_interval)
    df_plot$Data <- data_type_train
    #------
    from_date <- row.names(mat_pctDiff_test)[1]
    to_date <- row.names(mat_pctDiff_test)[nrow(mat_pctDiff_test)]
    from_date <- gsub("-", "/", from_date)
    to_date <- gsub("-", "/", to_date)
    date_interval <- paste(from_date, to_date, sep = " - ")
    data_type_test <- paste("Backtest data ", date_interval)
    nab_pctRet_test <- apply(mat_pctDiff_test, 2, function(x) prod(1 + x)) - 1
    #------------------------------------------------------------
    df_plot_test <- data.frame(Returns = nab_pctRet_test, Data = data_type_test)
    df_plot_test$id <- row.names(df_plot_test)
    if(!is.null(group_info)){
      df_plot_test$Type <- factor(group_vec)
      xx <- df_plot_test$Type
      df_plot_test$id <- factor(df_plot_test$id, levels = cols_ordered_by_group)
    }
    df_plot <- as.data.frame(rbind(df_plot, df_plot_test))
    
    df_plot$Data <- factor(df_plot$Data, levels = c(data_type_train, data_type_test))
    
  }
  
  #------------------------------------------------------------
  # Historical returns plot
  if(returns_plot){
    
    if(!is.null(group_info)){
      gg <- ggplot(df_plot, aes(x = id, y = Returns, fill = Type))
      #gg <- gg + scale_color_brewer(palette = "Dark2")
      gg <- gg + scale_fill_manual(values = group_colors)
    }else{
      gg <- ggplot(df_plot, aes(x = id, y = Returns))
    }
    gg <- gg + geom_bar(stat = "identity", color = "black", position = "dodge")
    if(!is.null(mat_pctDiff_test)){
      gg <- gg + facet_wrap(~Data, ncol = 1)
      gg <- gg + labs(title = fig_title_returns)
    }else{
      gg <- gg + labs(title = fig_title_returns, subtitle = fig_subtitle_returns)
    }
    gg <- gg + theme(axis.text.x = element_text(face = "bold", size = 10, angle = 60, hjust = 1),
                     axis.text.y = element_text(face = "bold", size = 10),
                     axis.title.x = element_blank(),
                     axis.title.y = element_text(face = "bold", size = 10),
                     plot.title = element_text(size = 11))
    if(!is.null(returns_plot_range)){
      gg <- gg + coord_cartesian(ylim = returns_plot_range)
    }
    #  gg <- gg + coord_equal()
    #  gg <- gg + coord_flip()
    print(gg)
    
  }
  #------------------------------------------------------------
  # Correlation matrix plot
  if(corr_plot){
    if(is.null(fig_title_corrplot)){fig_title_corrplot <- "Correlation matrix"}
    plot_with_pvals <- corrplot_options[["plot_with_pvals"]]
    plot_with_corrCoefs <- corrplot_options[["plot_with_corrCoefs"]]
    corr_coef_size <- corrplot_options[["corr_coef_size"]]
    if(!is.null(group_info)){
      mat_pctDiff_corrplot <- mat_pctDiff[, cols_ordered_by_group]
      if(!is.null(mat_pctDiff_test)){
        mat_pctDiff_corrplot_test <- mat_pctDiff_test[, cols_ordered_by_group]
      }else{
        mat_pctDiff_corrplot_test <- NULL
      }
      
    }else{
      mat_pctDiff_corrplot <- mat_pctDiff
      if(!is.null(mat_pctDiff_test)){
        mat_pctDiff_corrplot_test <- mat_pctDiff_test
      }else{
        mat_pctDiff_corrplot_test <- NULL
      }
      
      group_color_vec <- "black"
    }
    #-------------------
    corr_colorRamp <- colorRampPalette(c("orange", "white", "deepskyblue"))(50)
    #-------------------
    xx <- Hmisc::rcorr(mat_pctDiff_corrplot)
    cormat <- xx$r
    if(plot_with_pvals){
      pvals <- xx$P
    }else{
      pvals <- NULL
    }
    
    if(plot_with_corrCoefs){
      corrplot::corrplot.mixed(cormat, tl.pos = "lt", tl.col = group_color_vec,
                               tl.srt = 45, p.mat = pvals, number.cex = corr_coef_size,
                               title = fig_title_corrplot,
                               col = corr_colorRamp)
    }else{
      corrplot::corrplot(cormat, type = "lower", tl.col = group_color_vec, tl.srt = 45,
                         p.mat = pvals, title = fig_title_corrplot,
                         col = corr_colorRamp)
    }
    #-------------------
    if(!is.null(mat_pctDiff_corrplot_test)){
      if(is.null(fig_title_corrplot_test)){
        fig_title_corrplot_test <- "Correlation matrix, test data"
      }
      xx <- Hmisc::rcorr(mat_pctDiff_corrplot_test)
      cormat <- xx$r
      if(plot_with_pvals){
        pvals <- xx$P
      }else{
        pvals <- NULL
      }
      if(plot_with_corrCoefs){
        corrplot::corrplot.mixed(cormat, tl.pos = "lt", tl.col = group_color_vec,
                                 tl.srt = 45, p.mat = pvals, number.cex = corr_coef_size,
                                 title = fig_title_corrplot_test,
                                 col = corr_colorRamp)
      }else{
        corrplot::corrplot(cormat, type = "lower", tl.col = group_color_vec, tl.srt = 45,
                           p.mat = pvals, title = fig_title_corrplot_test,
                           col = corr_colorRamp)
      }
      
    }
    
  }
  #------------------------------------------------------------
  
}
#=======================================================================
# End function definition
#=======================================================================
#=======================================================================

spy_sector_symbs <- c("XLF", "XLC", "XLY", "XLP", "XLV", "XLK", "RWR",
                      "XLU", "XLI", "XBI", "IYT") #"TTEK"
spy_sector_detail <- c("Financials", "Communications", "Luxury goods", "Staple goods",
                       "Healthcare", "Technology", "Real estate", "Utilities", "Industrial",
                       "Biotechnology", "Transportation") #"Gov. foreign aid"
minerals_symbs <- c("GLD", "SLV", "PPLT", "JJC", "JJM") #"XME"
minerals_detail <- c("Gold", "Silver", "Platinum", "Copper", "Industrial metals") #"US metals and mining"
agriculture_symbs <- c("JJG", "BAL", "SOYB", "SGG", "JO", "NIB", "DBA")
agriculture_detail <- c("Grains", "Cotton", "Soybean", "Sugar", "Coffee", "Cacao", "General agriculture")
energy_symbs <- c("WTI", "FUE", "WOOD", "ICLN", "KOL", "UNG")
energy_detail <- c("Oil (W&T Offshore Inc.)", "Biofuels", "Timber", "Clean energy", "Coal", "US natural gas")
#currency_symbs <- c("EMLC", "UUP", "FXE", "FXY", "FXF", "FXC", "FXB", "FXA")
#currency_detail <- c("Emerging mkt currencies", "USD", "EUR", "JPY", "CHF", "CND", "GBP", "AUD")
currency_symbs <- c("EURUSD=X", "JPY=X", "CHF=X", "CAD=X",
                    "GBPUSD=X", "AUDUSD=X", "INR=X")
currency_detail <- c("EUR/USD", "USD/JPY",
                     "USD/CHF", "USD/CAD", "GBP/USD", "AUD/USD", "USD/INR")
emerg_mkt_symbs <- c("EMLC", "ELD", "BKF", "VWOB")
emerg_mkt_detail <- c("Emerg mkts debt", "Emerg mkts gov. bonds", "BRIC countries", "Emerging mkt currencies")
crypto_symbs <- c("BLOK", "LEGR", "BCNA")
crypto_detail <- c("Blockchain tech.", "Blockchain companies", "Blockchain in China")
Tbond_symbs <- c("IEI", "IEF", "TLT")#, "BIL"
Tbond_detail <- c("T-bond 3-7 yrs", "T-bond 7-10 yrs", "T-bond 20+ yrs") #"T-bond 1-3 months"

ts_symb_vec <- c(spy_sector_symbs, minerals_symbs, agriculture_symbs, energy_symbs,
                 currency_symbs, emerg_mkt_symbs, crypto_symbs, Tbond_symbs)
ts_detail_vec <- c(spy_sector_detail, minerals_detail, agriculture_detail, energy_detail,
                   currency_detail, emerg_mkt_detail, crypto_detail, Tbond_detail)
df_symb_detail <- data.frame(symbol = ts_symb_vec, detail = ts_detail_vec)
#--
#length(ts_symb_vec)
#--
# list_groups <- list(spy_sector_symbs, minerals_symbs, agriculture_symbs, energy_symbs,
#                      currency_symbs, emerg_mkt_symbs, crypto_symbs, Tbond_symbs)
#-------------------------------------------
# Create grouping info for plots (required input into functions)
list_groups <- list(spy_sector_detail, minerals_detail, agriculture_detail, energy_detail,
                    currency_detail, emerg_mkt_detail, crypto_detail, Tbond_detail)
group_names <- c("US Sectors", "Minerals", "Agriculture", "Energy", "Major Currency Pairs",
                 "Emerging Markets", "Cryptocurrencies/Blockchain", "T-Bonds")
group_info <- list(list_groups, group_names)
n_groups <- length(group_names)
#-------------------------------------------
#per_ema <- 13
#fromdate = Sys.Date() - 360
fromdate = "2018-06-20"
todate = "2019-08-09"
tbl_ohlcv <- tq_get(ts_symb_vec, get = "stock.prices", from = fromdate, to = todate)
df_ohlcv <- as.data.frame(tbl_ohlcv)
#--
# Crypto ts can have duplicate date entries sometimes (BTC-USD)
# Get rid of these duplicates, if any
df_ohlcv <- as.data.frame(df_ohlcv %>% group_by(symbol) %>% mutate(dup = duplicated(date)))
df_ohlcv <- subset(df_ohlcv, dup == F)
df_ohlcv$dup <- NULL
#--

#df_ohlcv$p <- rowSums(df_ohlcv[, c(4:6)]) / 3
#--
#df <- df_ohlcv[, c("date", "symbol", "p")]
# df <- df_ohlcv %>% group_by(symbol) %>% tq_transmute(select = adjusted, 
#                      mutate_fun = periodReturn, 
#                      period     = "monthly")
#--
df <- df_ohlcv[, c("date", "symbol", "adjusted")]
df <- df %>% spread(symbol, adjusted)
ind_shift <- which(colnames(df) %in% currency_symbs)
df[, ind_shift] <- rbind(rep(NA, length(ind_shift)), df[-nrow(df), ind_shift])
df <- df[-c(1, nrow(df)), ]
date_vec <- df$date
mat_ts_dy <- na.approx(df[, -1])

#o <- apply(mat_ts_dy, 2, function(x) length(which(is.na(x))))
#table(o)
#which(o==1)
#xts_ts_dy <- xts(mat_ts_dy, date_vec)
#date_vec <- index(xts_ts_dy)
#-----
mat_pctDiff_dy <- diff(mat_ts_dy) / mat_ts_dy[-nrow(mat_ts_dy), ]
#mat_pctDiff_dy <- diff(log(mat_ts_dy))
row.names(mat_pctDiff_dy) <- as.character(date_vec[-1])
ts_avg_dy <- rowMeans(mat_pctDiff_dy)
mu_ret_dy <- colMeans(mat_pctDiff_dy)
sd_ret_dy <- apply(mat_pctDiff_dy, 2, sd)

mat_ts_wk <- to.weekly(xts(mat_ts_dy, date_vec), OHLC = F)
mat_pctDiff_wk <- diff(mat_ts_wk) / mat_ts_wk[-nrow(mat_ts_wk), ]
mat_pctDiff_wk <- as.matrix(mat_pctDiff_wk[-1, ])
ts_avg_wk <- rowMeans(mat_pctDiff_wk)
mu_ret_wk <- colMeans(mat_pctDiff_wk)
sd_ret_wk <- apply(mat_pctDiff_wk, 2, sd)

mat_ts_mo <- to.monthly(xts(mat_ts_dy, date_vec), OHLC = F)
mat_pctDiff_mo <- diff(mat_ts_mo) / mat_ts_mo[-nrow(mat_ts_mo), ]
mat_pctDiff_mo <- as.matrix(mat_pctDiff_mo[-1, ])
ts_avg_mo <- rowMeans(mat_pctDiff_mo)
mu_ret_mo <- colMeans(mat_pctDiff_mo)
sd_ret_mo <- apply(mat_pctDiff_mo, 2, sd)
#----------------------------------------------
# Compare histograms of daily, weekly, and monthly returns
# df_plot <- data.frame(ts_id = names(mu_ret_dy), mu_ret_dy, mu_ret_wk, mu_ret_mo)
# colnames(df_plot)[-1] <- c("Mean Daily Return", "Mean Weekly Return", "Mean Monthly Return")
# gathercols <- colnames(df_plot)[-1]
# df_plot <- df_plot %>% gather_("Return Type", "Value", gathercols)
# #gg <- ggplot(df_plot, aes(Value, fill = `Return Type`))
# gg <- ggplot(df_plot, aes(Value))
# gg <- gg + geom_density(alpha = 0.4)
# gg <- gg + facet_wrap(~`Return Type`, nrow = 1, scales = "free")
# #gg <- gg + coord_cartesian(xlim = c(-0.02, 0.02))
# gg
#----------------------------------------------
#ind_rm_ema <- 1:(per_ema - 1)
# mat_pctDiff <- apply(mat_pctDiff, 2, function(x) x - EMA(x, per_ema))
# mat_pctDiff <- mat_pctDiff[-ind_rm_ema, ]
# date_vec <- df$date[-c(ind_rm_ema, ind_rm_na)]
#----------------------------------------------
mat_ts_in <- mat_ts_dy
ts_avg_in <- ts_avg_dy
# nab_pctRet_in <- mu_ret_dy
# sd_ret_in <- sd_ret_dy
#----------------------------------------------
mat_pctDiff <- mat_pctDiff_dy
date_vec <- row.names(mat_pctDiff)
#length(date_vec)
row.names(mat_pctDiff) <- as.character(date_vec)
colnames(mat_pctDiff) <- as.character(df_symb_detail$detail[order(df_symb_detail$symbol)])
#----------------------------------------------
check_on_data <- F
if(check_on_data){
  df_plot <- as.data.frame(mat_pctDiff)
  gathercols <- colnames(df_plot)
  df_plot$Date <- date_vec
  df_plot <- df_plot %>% gather_("Security", "Weekly change (%)", gathercols)
  df_plot$Type <- NA
  df_plot$Type[which(df_plot$Security %in% spy_sector_detail)] <- "US Sectors"
  df_plot$Type[which(df_plot$Security %in% minerals_detail)] <- "Minerals"
  df_plot$Type[which(df_plot$Security %in% agriculture_detail)] <- "Agriculture"
  df_plot$Type[which(df_plot$Security %in% energy_detail)] <- "Energy"
  df_plot$Type[which(df_plot$Security %in% currency_detail)] <- "Major Currency Pairs"
  df_plot$Type[which(df_plot$Security %in% emerg_mkt_detail)] <- "Emerging Markets"
  df_plot$Type[which(df_plot$Security %in% crypto_detail)] <- "Cryptocurrencies/Blockchain"
  df_plot$Type[which(df_plot$Security %in% Tbond_detail)] <- "T-bonds"
  #------------------------------------------------------------
  # gg <- ggplot(df_plot, aes(x = Year, y = Value, group = Security, color = Security))
  # gg <- gg + geom_line()
  # gg <- gg + facet_wrap(~ Type, nrow = 3, scales = "free")
  # gg <- gg + theme(legend.position = "none")
  # gg
  #------------------------------------------------------------
  this_type <- "US Sectors"
  df_plot2 <- subset(df_plot, Type == this_type)
  gg <- ggplot(df_plot2, aes(x = Date, y = `Weekly change (%)`, group = Security, color = Security))
  gg <- gg + geom_line(lwd = 1.1)
  gg <- gg + geom_point(size = 2)
  gg <- gg + labs(title = this_type)
  gg
  this_type <- "Minerals"
  df_plot2 <- subset(df_plot, Type == this_type)
  gg <- ggplot(df_plot2, aes(x = Date, y = `Weekly change (%)`, group = Security, color = Security))
  gg <- gg + geom_line(lwd = 1.1)
  gg <- gg + geom_point(size = 2)
  gg <- gg + labs(title = this_type)
  gg
  this_type <- "Agriculture"
  df_plot2 <- subset(df_plot, Type == this_type)
  gg <- ggplot(df_plot2, aes(x = Date, y = `Weekly change (%)`, group = Security, color = Security))
  gg <- gg + geom_line(lwd = 1.1)
  gg <- gg + geom_point(size = 2)
  gg <- gg + labs(title = this_type)
  gg
  this_type <- "Energy"
  df_plot2 <- subset(df_plot, Type == this_type)
  gg <- ggplot(df_plot2, aes(x = Date, y = `Weekly change (%)`, group = Security, color = Security))
  gg <- gg + geom_line(lwd = 1.1)
  gg <- gg + geom_point(size = 2)
  gg <- gg + labs(title = this_type)
  gg
  this_type <- "Major Currency Pairs"
  df_plot2 <- subset(df_plot, Type == this_type)
  gg <- ggplot(df_plot2, aes(x = Date, y = `Weekly change (%)`, group = Security, color = Security))
  gg <- gg + geom_line(lwd = 1.1)
  gg <- gg + geom_point(size = 2)
  gg <- gg + labs(title = this_type)
  gg
  this_type <- "Emerging Markets"
  df_plot2 <- subset(df_plot, Type == this_type)
  gg <- ggplot(df_plot2, aes(x = Date, y = `Weekly change (%)`, group = Security, color = Security))
  gg <- gg + geom_line(lwd = 1.1)
  gg <- gg + geom_point(size = 2)
  gg <- gg + labs(title = this_type)
  gg
  this_type <- "Cryptocurrencies/Blockchain"
  df_plot2 <- subset(df_plot, Type == this_type)
  gg <- ggplot(df_plot2, aes(x = Date, y = `Weekly change (%)`, group = Security, color = Security))
  gg <- gg + geom_line(lwd = 1.1)
  gg <- gg + geom_point(size = 2)
  gg <- gg + labs(title = this_type)
  gg
  this_type <- "T-bonds"
  df_plot2 <- subset(df_plot, Type == this_type)
  gg <- ggplot(df_plot2, aes(x = Date, y = `Weekly change (%)`, group = Security, color = Security))
  gg <- gg + geom_line(lwd = 1.1)
  gg <- gg + geom_point(size = 2)
  gg <- gg + labs(title = this_type)
  gg
}

```



```{r, fig.cap="", fig.width=11, fig.height=6, fig.align='center', echo=FALSE}
ind_train <- 1:round(nrow(mat_pctDiff) * 2 / 3)
ind_test <- setdiff(1:nrow(mat_pctDiff), ind_train)
mat_pctDiff_train <- mat_pctDiff[ind_train, ]
mat_pctDiff_test <- mat_pctDiff[ind_test, ]
# ts_avg_test <- ts_avg_in[ind_test]
# date_vec_test <- date_vec[ind_test]
#group_colors <- RColorBrewer::brewer.pal(n = n_groups, "Dark2")
# "Darjeeling"
group_colors <- wesanderson::wes_palette("Darjeeling1", n = n_groups, type = "continuous")
this_fig_title <- "Figure 1: Historical returns"
historical_returns_and_corr_plot(mat_pctDiff_train, mat_pctDiff_test, group_info,
                                 returns_plot = T,
                                 corr_plot = F,
                                 group_colors = group_colors,
                                 fig_title_returns = this_fig_title)


```

```{r, fig.width=12, fig.height=12, fig.align='center', echo=FALSE}

# (mat_pctDiff,
#                                              mat_pctDiff_test = NULL,
#                                              group_info = NULL,
#                                              returns_plot = F,
#                                              corr_plot = F,
#                                              fig_title_returns = NULL,
#                                              fig_title_corrplot = NULL,
#                                              fig_title_corrplot_test = NULL,
#                                              returns_plot_range = NULL,
#                                              corrplot_options = list(
#                                                plot_with_pvals = F,
#                                                plot_with_corrCoefs = F,
#                                                corr_coef_size = 0.75)
# )

corrplot_options = list(plot_with_pvals = F, plot_with_corrCoefs = F, corr_coef_size = 0.75)
fig_title_corrplot <- "Figure 2: Correlation matrix, train data"
fig_title_corrplot_test <- "Figure 3: Correlation matrix, test data"
historical_returns_and_corr_plot(mat_pctDiff_train, mat_pctDiff_test, group_info,
                                 returns_plot = F, 
                                 corr_plot = T, 
                                 group_colors = group_colors, 
                                 fig_title_corrplot = fig_title_corrplot,
                                 fig_title_corrplot_test = fig_title_corrplot_test,
                                 corrplot_options = corrplot_options)



```





# References
